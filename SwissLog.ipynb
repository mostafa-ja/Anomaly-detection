{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNbE98Ap+woVRiN2re8mPha",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mostafa-ja/Anomaly-detection/blob/main/SwissLog.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget 'https://raw.githubusercontent.com/IntelligentDDS/SwissLog/main/log_parser/EngCorpus.pkl'\n",
        "!wget 'https://raw.githubusercontent.com/IntelligentDDS/SwissLog/main/log_parser/offline_logparser/layers/tokenize_group_layer.py'\n",
        "!wget 'https://raw.githubusercontent.com/IntelligentDDS/SwissLog/main/log_parser/offline_logparser/layers/mask_layer.py'\n",
        "!wget 'https://raw.githubusercontent.com/IntelligentDDS/SwissLog/main/log_parser/offline_logparser/layers/knowledge_layer.py'\n",
        "!wget 'https://raw.githubusercontent.com/IntelligentDDS/SwissLog/main/log_parser/offline_logparser/layers/file_output_layer.py'\n",
        "!wget 'https://raw.githubusercontent.com/IntelligentDDS/SwissLog/main/log_parser/offline_logparser/layers/dict_group_layer.py'\n",
        "!wget 'https://raw.githubusercontent.com/IntelligentDDS/SwissLog/main/log_parser/offline_logparser/evaluator/evaluator.py'\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2S6Ps4ORioit",
        "outputId": "1b081e99-cc8f-437b-a006-cd4652bd5e84"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-08-02 14:09:47--  https://raw.githubusercontent.com/IntelligentDDS/SwissLog/main/log_parser/EngCorpus.pkl\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 412351 (403K) [text/plain]\n",
            "Saving to: ‘EngCorpus.pkl’\n",
            "\n",
            "\rEngCorpus.pkl         0%[                    ]       0  --.-KB/s               \rEngCorpus.pkl       100%[===================>] 402.69K  --.-KB/s    in 0.04s   \n",
            "\n",
            "2023-08-02 14:09:47 (11.2 MB/s) - ‘EngCorpus.pkl’ saved [412351/412351]\n",
            "\n",
            "--2023-08-02 14:09:47--  https://raw.githubusercontent.com/IntelligentDDS/SwissLog/main/log_parser/offline_logparser/layers/tokenize_group_layer.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.110.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1559 (1.5K) [text/plain]\n",
            "Saving to: ‘tokenize_group_layer.py’\n",
            "\n",
            "tokenize_group_laye 100%[===================>]   1.52K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-08-02 14:09:47 (18.0 MB/s) - ‘tokenize_group_layer.py’ saved [1559/1559]\n",
            "\n",
            "--2023-08-02 14:09:47--  https://raw.githubusercontent.com/IntelligentDDS/SwissLog/main/log_parser/offline_logparser/layers/mask_layer.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5279 (5.2K) [text/plain]\n",
            "Saving to: ‘mask_layer.py’\n",
            "\n",
            "mask_layer.py       100%[===================>]   5.16K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-08-02 14:09:47 (48.6 MB/s) - ‘mask_layer.py’ saved [5279/5279]\n",
            "\n",
            "--2023-08-02 14:09:47--  https://raw.githubusercontent.com/IntelligentDDS/SwissLog/main/log_parser/offline_logparser/layers/knowledge_layer.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1234 (1.2K) [text/plain]\n",
            "Saving to: ‘knowledge_layer.py’\n",
            "\n",
            "knowledge_layer.py  100%[===================>]   1.21K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-08-02 14:09:47 (107 MB/s) - ‘knowledge_layer.py’ saved [1234/1234]\n",
            "\n",
            "--2023-08-02 14:09:47--  https://raw.githubusercontent.com/IntelligentDDS/SwissLog/main/log_parser/offline_logparser/layers/file_output_layer.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.109.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1734 (1.7K) [text/plain]\n",
            "Saving to: ‘file_output_layer.py’\n",
            "\n",
            "file_output_layer.p 100%[===================>]   1.69K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-08-02 14:09:47 (32.0 MB/s) - ‘file_output_layer.py’ saved [1734/1734]\n",
            "\n",
            "--2023-08-02 14:09:47--  https://raw.githubusercontent.com/IntelligentDDS/SwissLog/main/log_parser/offline_logparser/layers/dict_group_layer.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2190 (2.1K) [text/plain]\n",
            "Saving to: ‘dict_group_layer.py’\n",
            "\n",
            "dict_group_layer.py 100%[===================>]   2.14K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-08-02 14:09:47 (17.7 MB/s) - ‘dict_group_layer.py’ saved [2190/2190]\n",
            "\n",
            "--2023-08-02 14:09:47--  https://raw.githubusercontent.com/IntelligentDDS/SwissLog/main/log_parser/offline_logparser/evaluator/evaluator.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.110.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3660 (3.6K) [text/plain]\n",
            "Saving to: ‘evaluator.py’\n",
            "\n",
            "evaluator.py        100%[===================>]   3.57K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-08-02 14:09:47 (53.3 MB/s) - ‘evaluator.py’ saved [3660/3660]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append('/content/') #to be able for importing libarary from this address"
      ],
      "metadata": {
        "id": "5Nefj_fskx7G"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from layers.file_output_layer import FileOutputLayer\n",
        "from layers.knowledge_layer import KnowledgeGroupLayer\n",
        "from layers.mask_layer import MaskLayer\n",
        "from layers.tokenize_group_layer import TokenizeGroupLayer\n",
        "from layers.dict_group_layer import DictGroupLayer\n",
        "\n",
        "\n",
        "import sys\n",
        "from evaluator import evaluator\n",
        "import os\n",
        "import re\n",
        "import string\n",
        "import hashlib\n",
        "from datetime import datetime\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "import argparse"
      ],
      "metadata": {
        "id": "0CaF5ko3iS7m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_dir = '../logs/' # The input directory of log file\n",
        "output_dir = 'LogParserResult/' # The output directory of parsing results\n",
        "\n",
        "\n",
        "def load_logs(log_file, regex, headers):\n",
        "    \"\"\" Function to transform log file to dataframe\n",
        "    \"\"\"\n",
        "    log_messages = dict()\n",
        "    linecount = 0\n",
        "    with open(log_file, 'r') as fin:\n",
        "        for line in tqdm(fin.readlines(), desc='load data'):\n",
        "            try:\n",
        "                linecount += 1\n",
        "                match = regex.search(line.strip())\n",
        "                message = dict()\n",
        "                for header in headers:\n",
        "                    message[header] = match.group(header)\n",
        "                message['LineId'] = linecount\n",
        "                log_messages[linecount] = message\n",
        "            except Exception as e:\n",
        "                pass\n",
        "    return log_messages"
      ],
      "metadata": {
        "id": "_liVNau4ic6i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_logformat_regex(logformat):\n",
        "    \"\"\" Function to generate regular expression to split log messages\n",
        "    \"\"\"\n",
        "    headers = []\n",
        "    splitters = re.split(r'(<[^<>]+>)', logformat)\n",
        "    regex = ''\n",
        "    for k in range(len(splitters)):\n",
        "        if k % 2 == 0:\n",
        "            splitter = re.sub(' +', '\\\\\\s+', splitters[k])\n",
        "            regex += splitter\n",
        "        else:\n",
        "            header = splitters[k].strip('<').strip('>')\n",
        "            regex += '(?P<%s>.*?)' % header\n",
        "            headers.append(header)\n",
        "    regex = re.compile('^' + regex + '$')\n",
        "    return headers, regex\n",
        "\n",
        "benchmark_settings = {\n",
        "    'HDFS': {\n",
        "        'log_file': 'HDFS/HDFS_2k.log',\n",
        "        'log_format': '<Date> <Time> <Pid> <Level> <Component>: <Content>',\n",
        "        'regex': [r'blk_-?\\d+', r'(\\d+\\.){3}\\d+(:\\d+)?']\n",
        "        }\n",
        "}"
      ],
      "metadata": {
        "id": "aeEMmQpHihDL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "05eUICg0iHeq"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('--dictionary', default='../EngCorpus.pkl', type=str)\n",
        "    args = parser.parse_args()\n",
        "    corpus = args.dictionary\n",
        "\n",
        "    benchmark_result = []\n",
        "    for dataset, setting in benchmark_settings.items():\n",
        "        print('\\n=== Evaluation on %s ==='%dataset)\n",
        "        indir = os.path.join(input_dir, os.path.dirname(setting['log_file']))\n",
        "        outdir = os.path.join(output_dir, os.path.dirname(setting['log_file']))\n",
        "        log_file = os.path.basename(setting['log_file'])\n",
        "\n",
        "        filepath = os.path.join(indir, log_file)\n",
        "        print('Parsing file: ' + filepath)\n",
        "        starttime = datetime.now()\n",
        "        headers, regex = generate_logformat_regex(setting['log_format'])\n",
        "        log_messages = load_logs(filepath, regex, headers)\n",
        "        # preprocess layer\n",
        "        log_messages = KnowledgeGroupLayer(log_messages).run()\n",
        "        # tokenize layer\n",
        "        log_messages = TokenizeGroupLayer(log_messages, rex=setting['regex']).run()\n",
        "        # dictionarize layer and cluster by wordset\n",
        "        dict_group_result = DictGroupLayer(log_messages, corpus).run()\n",
        "        # apply LCS and prefix tree\n",
        "        results, templates = MaskLayer(dict_group_result).run()\n",
        "        output_file = os.path.join(outdir, log_file)\n",
        "        # output parsing results\n",
        "        FileOutputLayer(log_messages, output_file, templates, ['LineId'] + headers).run()\n",
        "        print('Parsing done. [Time taken: {!s}]'.format(datetime.now() - starttime))\n",
        "        F1_measure, accuracy = evaluator.evaluate(\n",
        "                            groundtruth=os.path.join(indir, log_file + '_structured.csv'),\n",
        "                            parsedresult=os.path.join(outdir, log_file + '_structured.csv')\n",
        "                            )\n",
        "        benchmark_result.append([dataset, F1_measure, accuracy])\n",
        "\n",
        "    print('\\n=== Overall evaluation results ===')\n",
        "    avg_accr = 0\n",
        "    for i in range(len(benchmark_result)):\n",
        "        avg_accr += benchmark_result[i][2]\n",
        "    avg_accr /= len(benchmark_result)\n",
        "    pd_result = pd.DataFrame(benchmark_result, columns={'dataset', 'F1_measure', 'Accuracy'})\n",
        "    print(pd_result)\n",
        "    print('avarage accuracy is {}'.format(avg_accr))\n",
        "    pd_result.to_csv('benchmark_result.csv', index=False)\n"
      ]
    }
  ]
}