{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mostafa-ja/Anomaly-detection/blob/main/autoencoder6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gIppU8PXpm-J",
        "outputId": "153c368b-a65b-4069-ab0c-a17ad2dbea67"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# we upload, in case that we cant parse logs(a lot of time and ram consumption)\n",
        "\n",
        "# Mount Google Drive to save datasets\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#copy log file\n",
        "!cp '/content/drive/MyDrive/logs_train' '/content/'\n",
        "!cp '/content/drive/MyDrive/logs_ntest' '/content/'\n",
        "!cp '/content/drive/MyDrive/logs_atest' '/content/'\n",
        "!cp '/content/drive/MyDrive/log2index' '/content/'\n",
        "!cp '/content/drive/MyDrive/reduced_embeddings' '/content/'\n"
      ],
      "metadata": {
        "id": "jIINrPiNqE_a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "jd9WP1rSrLSh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KVDWEwbx0J7p",
        "outputId": "6f96f5cf-acbb-479d-a286-16c537d6f5e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/reduced_embeddings', 'r') as json_file:\n",
        "    embeddings = json.load(json_file)"
      ],
      "metadata": {
        "id": "gG19wcyUyWyv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(embeddings[0])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ySn0vNy33IL_",
        "outputId": "c51bb7a2-f68f-426b-d099-0f7180f654e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "name = 'logs_train'\n",
        "window_size = 10\n",
        "num_sessions = 0\n",
        "inputs = []\n",
        "#outputs = []\n",
        "\n",
        "with open('/content/' + name, 'r') as f:\n",
        "        for row in f:\n",
        "            num_sessions += 1\n",
        "            line = [ embeddings[int(i)] for i in row.strip().split()]\n",
        "            for i in range(len(line) - window_size):\n",
        "                inputs.append(line[i:i + window_size])\n",
        "                #outputs.append(line[i + window_size])\n",
        "\n",
        "print('Number of sessions({}): {}'.format(name, num_sessions))\n",
        "print('Number of seqs({}): {}'.format(name, len(inputs)))\n",
        "#dataset = TensorDataset(torch.tensor(inputs, dtype=torch.float), torch.tensor(outputs))\n",
        "dataset = TensorDataset(torch.tensor(inputs, dtype=torch.float))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZIwHnSfe0f3k",
        "outputId": "23d5ad51-c6b8-4764-ec9f-36f1aee29c6e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of sessions(logs_train): 446578\n",
            "Number of seqs(logs_train): 4704000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the LSTM Autoencoder model with dropout\n",
        "class LSTMAutoencoder(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, num_layers, dropout_prob):\n",
        "        super(LSTMAutoencoder, self).__init__()\n",
        "        self.encoder = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True, bidirectional=True, dropout=dropout_prob)\n",
        "        self.decoder = nn.LSTM(hidden_dim * 2, input_dim, num_layers, batch_first=True, dropout=dropout_prob)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        encoded, _ = self.encoder(x)\n",
        "        #print(encoded.shape)\n",
        "        encoded = encoded[:,-1:,:] # output of last cell\n",
        "        #print(encoded.shape)\n",
        "        input_decode = torch.tile(encoded, (1, 10, 1))\n",
        "        decoded, _ = self.decoder(input_decode)\n",
        "        return decoded"
      ],
      "metadata": {
        "id": "BMnD_BY82ZVd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = LSTMAutoencoder(10, 32, 2, 0.2).to(device)\n",
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PPQ1gUe22tIP",
        "outputId": "ea8c2a27-b8c3-4d3c-913e-47ec62825dc1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LSTMAutoencoder(\n",
              "  (encoder): LSTM(10, 32, num_layers=2, batch_first=True, dropout=0.2, bidirectional=True)\n",
              "  (decoder): LSTM(64, 10, num_layers=2, batch_first=True, dropout=0.2)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = torch.randn(1,10, 10)\n",
        "model(data.to(device)).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dloxW2iT3DWn",
        "outputId": "b2645b90-9ec9-4aeb-b6dd-3b43c77b3048"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 10, 10])"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the number of parameters\n",
        "num_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"Number of Parameters: {num_params}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mIEBmDYf29xi",
        "outputId": "f660915e-ab63-4684-fbd3-f7c440c8f8aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Parameters: 40272\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataloader = DataLoader(dataset, batch_size=256)\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.MSELoss()"
      ],
      "metadata": {
        "id": "SyLp6yyx4PGb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(dataloader) # regard to batch size"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P7LwOnm84llY",
        "outputId": "61701fe2-1e40-4252-b377-e1dcfe06209b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "18375"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for step, (seq) in enumerate(dataloader):\n",
        "  print(seq[0].shape)\n",
        "  break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JOy2RaCB6AWx",
        "outputId": "c466d5ec-ede0-488c-f0e5-990a206e79ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([256, 10, 10])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def adjust_learning_rate(optimizer, epoch, lr_step=(15,25,35,45), lr_decay_ratio=0.5):\n",
        "    \"\"\"Adjust the learning rate based on the epoch number.\"\"\"\n",
        "    if epoch == 0:\n",
        "        optimizer.param_groups[0]['lr'] /= 16\n",
        "    elif epoch in [1, 2, 3, 4]:  # in step five , we finish warm up ,and start normal learning rate\n",
        "        optimizer.param_groups[0]['lr'] *= 2\n",
        "    if epoch in lr_step: # in these steps , we are geting close to optimal point so we need to have shorter step\n",
        "        optimizer.param_groups[0]['lr'] *= lr_decay_ratio\n",
        "    return optimizer\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001, betas=(0.9, 0.999))"
      ],
      "metadata": {
        "id": "HHyNWe1DfYPq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 50\n",
        "input_size = 10 # embedding size\n",
        "\n",
        "# Train the model\n",
        "start_time = time.time()\n",
        "total_step = len(dataloader)\n",
        "for epoch in range(num_epochs):  # Loop over the dataset multiple times\n",
        "    optimizer = adjust_learning_rate(optimizer, epoch)\n",
        "    train_loss = 0\n",
        "    for step, (seq) in enumerate(dataloader):\n",
        "        # Forward pass\n",
        "        seq = seq[0].clone().detach().view(-1, window_size, input_size).to(device)\n",
        "        output = model(seq)\n",
        "        loss = criterion(output, seq.to(device))\n",
        "\n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        train_loss += loss.item()\n",
        "        optimizer.step()\n",
        "    print('Epoch [{}/{}], train_loss: {:.4f}'.format(epoch + 1, num_epochs, train_loss / total_step))\n",
        "elapsed_time = time.time() - start_time\n",
        "print('elapsed_time: {:.3f}s'.format(elapsed_time))\n",
        "print('Finished Training')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "vQ_t5bz35SYq",
        "outputId": "3f9a41df-ac0b-473d-d3e3-808223699033"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/50], train_loss: 0.0152\n",
            "Epoch [2/50], train_loss: 0.0081\n",
            "Epoch [3/50], train_loss: 0.0051\n",
            "Epoch [4/50], train_loss: 0.0037\n",
            "Epoch [5/50], train_loss: 0.0029\n",
            "Epoch [6/50], train_loss: 0.0026\n",
            "Epoch [7/50], train_loss: 0.0023\n",
            "Epoch [8/50], train_loss: 0.0020\n",
            "Epoch [9/50], train_loss: 0.0018\n",
            "Epoch [10/50], train_loss: 0.0017\n",
            "Epoch [11/50], train_loss: 0.0016\n",
            "Epoch [12/50], train_loss: 0.0016\n",
            "Epoch [13/50], train_loss: 0.0015\n",
            "Epoch [14/50], train_loss: 0.0015\n",
            "Epoch [15/50], train_loss: 0.0015\n",
            "Epoch [16/50], train_loss: 0.0015\n",
            "Epoch [17/50], train_loss: 0.0015\n",
            "Epoch [18/50], train_loss: 0.0015\n",
            "Epoch [19/50], train_loss: 0.0014\n",
            "Epoch [20/50], train_loss: 0.0014\n",
            "Epoch [21/50], train_loss: 0.0014\n",
            "Epoch [22/50], train_loss: 0.0014\n",
            "Epoch [23/50], train_loss: 0.0014\n",
            "Epoch [24/50], train_loss: 0.0014\n",
            "Epoch [25/50], train_loss: 0.0014\n",
            "Epoch [26/50], train_loss: 0.0015\n",
            "Epoch [27/50], train_loss: 0.0014\n",
            "Epoch [28/50], train_loss: 0.0014\n",
            "Epoch [29/50], train_loss: 0.0014\n",
            "Epoch [30/50], train_loss: 0.0014\n",
            "Epoch [31/50], train_loss: 0.0014\n",
            "Epoch [32/50], train_loss: 0.0014\n",
            "Epoch [33/50], train_loss: 0.0014\n",
            "Epoch [34/50], train_loss: 0.0014\n",
            "Epoch [35/50], train_loss: 0.0014\n",
            "Epoch [36/50], train_loss: 0.0015\n",
            "Epoch [37/50], train_loss: 0.0014\n",
            "Epoch [38/50], train_loss: 0.0014\n",
            "Epoch [39/50], train_loss: 0.0014\n",
            "Epoch [40/50], train_loss: 0.0014\n",
            "Epoch [41/50], train_loss: 0.0014\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-59-82d2d1f26ac1>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;31m# Backward and optimize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    485\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             )\n\u001b[0;32m--> 487\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    488\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    201\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), '/content/drive/MyDrive/Autoencoder3_parameters.pth')"
      ],
      "metadata": {
        "id": "G_xbylKoiwAW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(embeddings)"
      ],
      "metadata": {
        "id": "wm0bhoHL5yjo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83d02ae4-5352-4dd7-c0c1-524216b5781d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "53"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings.append([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0])"
      ],
      "metadata": {
        "id": "qI8mKRtCH2MB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate(name):\n",
        "    window_size = 10\n",
        "    hdfs = {} #store the unique sequences and their counts.\n",
        "    length = 0\n",
        "    with open('/content/' + name, 'r') as f:\n",
        "        for row in f:\n",
        "            line = [int(i) for i in row.strip().split()]\n",
        "            line = line + [52] * (window_size + 1 - len(line)) #if the length of the line is less than windows size, it covers by 30 a template with zeros vector\n",
        "            hdfs[tuple(line)] = hdfs.get(tuple(line), 0) + 1   #If the tuple is not present in the dictionary, hdfs.get(tuple(ln), 0) returns 0, and the code initializes the count to 1.\n",
        "            length += 1\n",
        "            # hdfs.append(tuple(line))\n",
        "    print('Number of sessions({}): {}'.format(name, len(hdfs)))\n",
        "    return hdfs, length"
      ],
      "metadata": {
        "id": "zeXyQLVK5zAO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_normal_loader, test_normal_length = generate('logs_ntest')\n",
        "test_abnormal_loader, test_abnormal_length = generate('logs_atest')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pUyqMSfg7Ba-",
        "outputId": "9d924a58-c59c-4453-9b11-d33093e6a058"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of sessions(logs_ntest): 1091\n",
            "Number of sessions(logs_atest): 4126\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluation(threshold):\n",
        "  # Test the model\n",
        "  model.eval()\n",
        "\n",
        "  TP = 0\n",
        "  FP = 0\n",
        "\n",
        "  start_time = time.time()\n",
        "  with torch.no_grad():\n",
        "      for line in test_normal_loader.keys():\n",
        "          for i in range(len(line) - window_size):\n",
        "              session = line[i:i + window_size]\n",
        "              seq = [embeddings[temp] for temp in session]\n",
        "              seq = torch.tensor(seq, dtype=torch.float).view(-1, window_size, input_size).to(device)\n",
        "              output = model(seq)\n",
        "\n",
        "              loss = criterion(output, seq)\n",
        "\n",
        "              if (loss.cpu().detach().numpy()>threshold):\n",
        "                FP += test_normal_loader[line] # numbers of that set we have\n",
        "                break\n",
        "  with torch.no_grad():\n",
        "      for line in test_abnormal_loader.keys():\n",
        "          for i in range(len(line) - window_size):\n",
        "              session = line[i:i + window_size]\n",
        "              seq = [embeddings[temp] for temp in session]\n",
        "              seq = torch.tensor(seq, dtype=torch.float).view(-1, window_size, input_size).to(device)\n",
        "              output = model(seq)\n",
        "\n",
        "              loss = criterion(output, seq)\n",
        "\n",
        "              if (loss.cpu().detach().numpy()>threshold):\n",
        "                TP += test_abnormal_loader[line]\n",
        "                break\n",
        "  elapsed_time = time.time() - start_time\n",
        "  print('elapsed_time: {:.3f}s'.format(elapsed_time))\n",
        "  # Compute precision, recall and F1-measure\n",
        "  FN = test_abnormal_length - TP\n",
        "  P = 100 * TP / (TP + FP)\n",
        "  R = 100 * TP / (TP + FN)\n",
        "  F1 = 2 * P * R / (P + R)\n",
        "  print('false positive (FP): {}, false negative (FN): {}, Precision: {:.3f}%, Recall: {:.3f}%, F1-measure: {:.3f}%'.format(FP, FN, P, R, F1))\n",
        "  print('Finished Predicting')"
      ],
      "metadata": {
        "id": "xtAPzH7h9cZK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "threshold = [0.0022,0.0025,0.0028,0.0030,0.0033]\n",
        "for i in threshold:\n",
        "  print('-------------------------------------------------------------------------')\n",
        "  print('threshold = ', i)\n",
        "  evaluation(i)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1_m-pTgN9pGf",
        "outputId": "a5fec881-1dce-4d21-d39c-e98ab3d2c224"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------------------------------------------------------------------\n",
            "threshold =  0.0022\n",
            "elapsed_time: 22.977s\n",
            "false positive (FP): 3753, false negative (FN): 173, Precision: 81.619%, Recall: 98.973%, F1-measure: 89.462%\n",
            "Finished Predicting\n",
            "-------------------------------------------------------------------------\n",
            "threshold =  0.0025\n",
            "elapsed_time: 23.257s\n",
            "false positive (FP): 2979, false negative (FN): 179, Precision: 84.830%, Recall: 98.937%, F1-measure: 91.342%\n",
            "Finished Predicting\n",
            "-------------------------------------------------------------------------\n",
            "threshold =  0.0028\n",
            "elapsed_time: 23.966s\n",
            "false positive (FP): 2480, false negative (FN): 218, Precision: 87.016%, Recall: 98.705%, F1-measure: 92.493%\n",
            "Finished Predicting\n",
            "-------------------------------------------------------------------------\n",
            "threshold =  0.003\n",
            "elapsed_time: 22.943s\n",
            "false positive (FP): 1952, false negative (FN): 1187, Precision: 88.911%, Recall: 92.950%, F1-measure: 90.886%\n",
            "Finished Predicting\n",
            "-------------------------------------------------------------------------\n",
            "threshold =  0.0033\n",
            "elapsed_time: 24.463s\n",
            "false positive (FP): 1823, false negative (FN): 1379, Precision: 89.451%, Recall: 91.810%, F1-measure: 90.615%\n",
            "Finished Predicting\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "threshold = [0.0026,0.0027,0.0028,0.0029]\n",
        "for i in threshold:\n",
        "  print('-------------------------------------------------------------------------')\n",
        "  print('threshold = ',i)\n",
        "  evaluation(i)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dCegQ5i4-ZKy",
        "outputId": "a1c2f0bc-b96d-48eb-f7e9-2bfd2cff795f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------------------------------------------------------------------\n",
            "threshold =  0.0026\n",
            "elapsed_time: 23.151s\n",
            "false positive (FP): 2892, false negative (FN): 181, Precision: 85.206%, Recall: 98.925%, F1-measure: 91.555%\n",
            "Finished Predicting\n",
            "-------------------------------------------------------------------------\n",
            "threshold =  0.0027\n",
            "elapsed_time: 23.274s\n",
            "false positive (FP): 2803, false negative (FN): 189, Precision: 85.590%, Recall: 98.878%, F1-measure: 91.755%\n",
            "Finished Predicting\n",
            "-------------------------------------------------------------------------\n",
            "threshold =  0.0028\n",
            "elapsed_time: 22.897s\n",
            "false positive (FP): 2480, false negative (FN): 218, Precision: 87.016%, Recall: 98.705%, F1-measure: 92.493%\n",
            "Finished Predicting\n",
            "-------------------------------------------------------------------------\n",
            "threshold =  0.0029\n",
            "elapsed_time: 23.310s\n",
            "false positive (FP): 1973, false negative (FN): 1047, Precision: 88.893%, Recall: 93.782%, F1-measure: 91.272%\n",
            "Finished Predicting\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "threshold = [0.00275,0.0028,0.00285]\n",
        "for i in threshold:\n",
        "  print('-------------------------------------------------------------------------')\n",
        "  print('threshold = ',i)\n",
        "  evaluation(i)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wrfS2UyXORe6",
        "outputId": "95281939-770a-4b08-bef4-36b64411c5db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------------------------------------------------------------------\n",
            "threshold =  0.00275\n",
            "elapsed_time: 22.871s\n",
            "false positive (FP): 2633, false negative (FN): 196, Precision: 86.340%, Recall: 98.836%, F1-measure: 92.166%\n",
            "Finished Predicting\n",
            "-------------------------------------------------------------------------\n",
            "threshold =  0.0028\n",
            "elapsed_time: 24.295s\n",
            "false positive (FP): 2480, false negative (FN): 218, Precision: 87.016%, Recall: 98.705%, F1-measure: 92.493%\n",
            "Finished Predicting\n",
            "-------------------------------------------------------------------------\n",
            "threshold =  0.00285\n",
            "elapsed_time: 24.732s\n",
            "false positive (FP): 2437, false negative (FN): 222, Precision: 87.209%, Recall: 98.682%, F1-measure: 92.591%\n",
            "Finished Predicting\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "threshold = [0.00282,0.00284,0.00286,0.00288]\n",
        "for i in threshold:\n",
        "  print('-------------------------------------------------------------------------')\n",
        "  print('threshold = ',i)\n",
        "  evaluation(i)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KwO_mKDjOuyo",
        "outputId": "729d09c6-804e-4b71-f117-9f40831c526a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------------------------------------------------------------------\n",
            "threshold =  0.00282\n",
            "elapsed_time: 23.915s\n",
            "false positive (FP): 2479, false negative (FN): 221, Precision: 87.018%, Recall: 98.687%, F1-measure: 92.486%\n",
            "Finished Predicting\n",
            "-------------------------------------------------------------------------\n",
            "threshold =  0.00284\n",
            "elapsed_time: 23.665s\n",
            "false positive (FP): 2438, false negative (FN): 222, Precision: 87.205%, Recall: 98.682%, F1-measure: 92.589%\n",
            "Finished Predicting\n",
            "-------------------------------------------------------------------------\n",
            "threshold =  0.00286\n",
            "elapsed_time: 23.782s\n",
            "false positive (FP): 2436, false negative (FN): 222, Precision: 87.214%, Recall: 98.682%, F1-measure: 92.594%\n",
            "Finished Predicting\n",
            "-------------------------------------------------------------------------\n",
            "threshold =  0.00288\n",
            "elapsed_time: 24.027s\n",
            "false positive (FP): 2434, false negative (FN): 1047, Precision: 86.645%, Recall: 93.782%, F1-measure: 90.072%\n",
            "Finished Predicting\n"
          ]
        }
      ]
    }
  ]
}