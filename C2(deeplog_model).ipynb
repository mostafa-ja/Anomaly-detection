{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOg1QnvUNsoB3t0cGwdVs0k",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mostafa-ja/Anomaly-detection/blob/main/C2(deeplog_model).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
      ],
      "metadata": {
        "id": "-cgjPvYjFbKo"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VTdKXioxBmcc",
        "outputId": "26f25bea-1a32-4da1-a46c-4f8c53b4701d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Mount Google Drive to upload datasets (csv files)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import train and datasets based on ratio\n",
        "\n",
        "def split_log_file(input_file, train_ratio=0.7):\n",
        "    # Read the log file and split it into lines\n",
        "    with open(input_file, 'r') as log_file:\n",
        "        log_lines = log_file.readlines()\n",
        "\n",
        "    # Calculate the number of lines for the train and test sets\n",
        "    num_lines = len(log_lines)\n",
        "    num_train_lines = int(num_lines * train_ratio)\n",
        "    num_test_lines = num_lines - num_train_lines\n",
        "\n",
        "    # Write the lines corresponding to the train set to a new train log file\n",
        "    with open('hdfs_train', 'w') as train_file:\n",
        "        train_file.writelines(log_lines[:num_train_lines])\n",
        "\n",
        "    # Write the remaining lines (test set) to a new test log file\n",
        "    with open('hdfs_test_normal', 'w') as test_file:\n",
        "        test_file.writelines(log_lines[num_train_lines:])\n",
        "\n",
        "# split normal log file\n",
        "split_log_file('/content/drive/MyDrive/HDFS/structured_hdfs/hdfs_sequence_normal', train_ratio=0.1)\n",
        "\n",
        "#copy test abnormal file to current directory\n",
        "!cp '/content/drive/MyDrive/HDFS/structured_hdfs/hdfs_test_abnormal' '/content/'"
      ],
      "metadata": {
        "id": "p2LHttoEwKMp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "alternative files :\n",
        "\n",
        "\n",
        "```\n",
        "\n",
        "# download datasets\n",
        "!wget 'https://raw.githubusercontent.com/donglee-afar/logdeep/master/data/hdfs/hdfs_train'\n",
        "!wget 'https://raw.githubusercontent.com/donglee-afar/logdeep/master/data/hdfs/hdfs_test_normal'\n",
        "!wget 'https://raw.githubusercontent.com/donglee-afar/logdeep/master/data/hdfs/hdfs_test_abnormal'\n",
        "     \n",
        "```\n",
        "be careful these files are logs , not csv\n"
      ],
      "metadata": {
        "id": "Oa9CLMUMGGyO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# download datasets\n",
        "!wget 'https://raw.githubusercontent.com/donglee-afar/logdeep/master/data/hdfs/hdfs_train'\n",
        "!wget 'https://raw.githubusercontent.com/donglee-afar/logdeep/master/data/hdfs/hdfs_test_normal'\n",
        "!wget 'https://raw.githubusercontent.com/donglee-afar/logdeep/master/data/hdfs/hdfs_test_abnormal'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qZYNqcwLIPLF",
        "outputId": "301f6621-4205-4c5f-9f0e-d724da9e4122"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-07-25 16:22:07--  https://raw.githubusercontent.com/donglee-afar/logdeep/master/data/hdfs/hdfs_train\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 257875 (252K) [text/plain]\n",
            "Saving to: ‘hdfs_train’\n",
            "\n",
            "\rhdfs_train            0%[                    ]       0  --.-KB/s               \rhdfs_train          100%[===================>] 251.83K  --.-KB/s    in 0.006s  \n",
            "\n",
            "2023-07-25 16:22:07 (44.0 MB/s) - ‘hdfs_train’ saved [257875/257875]\n",
            "\n",
            "--2023-07-25 16:22:08--  https://raw.githubusercontent.com/donglee-afar/logdeep/master/data/hdfs/hdfs_test_normal\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 29284282 (28M) [text/plain]\n",
            "Saving to: ‘hdfs_test_normal’\n",
            "\n",
            "hdfs_test_normal    100%[===================>]  27.93M  60.3MB/s    in 0.5s    \n",
            "\n",
            "2023-07-25 16:22:08 (60.3 MB/s) - ‘hdfs_test_normal’ saved [29284282/29284282]\n",
            "\n",
            "--2023-07-25 16:22:08--  https://raw.githubusercontent.com/donglee-afar/logdeep/master/data/hdfs/hdfs_test_abnormal\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 782479 (764K) [text/plain]\n",
            "Saving to: ‘hdfs_test_abnormal’\n",
            "\n",
            "hdfs_test_abnormal  100%[===================>] 764.14K  --.-KB/s    in 0.01s   \n",
            "\n",
            "2023-07-25 16:22:09 (74.8 MB/s) - ‘hdfs_test_abnormal’ saved [782479/782479]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# count session for each dataset\n",
        "\n",
        "def count_sessions(dataset):\n",
        "    num_sessions = 0\n",
        "    with open('/content/'+ dataset, 'r') as f:\n",
        "        for row in f:\n",
        "            num_sessions += 1\n",
        "    print('Number of sessions({}): {}'.format(dataset, num_sessions))\n",
        "\n",
        "datasets = ['hdfs_train','hdfs_test_normal','hdfs_test_abnormal']\n",
        "\n",
        "for dataset in datasets:\n",
        "  count_sessions(dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BaCB9o7erHFe",
        "outputId": "9d2af22e-02bf-490b-d4e6-063a6c55b70e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of sessions(hdfs_train): 4855\n",
            "Number of sessions(hdfs_test_normal): 553366\n",
            "Number of sessions(hdfs_test_abnormal): 16838\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# all templates in our datasets\n",
        "\n",
        "datasets = ['hdfs_train','hdfs_test_normal','hdfs_test_abnormal']\n",
        "templates = set()\n",
        "\n",
        "for dataset in datasets:\n",
        "  with open('/content/' + dataset, 'r') as f:\n",
        "          for row in f:\n",
        "            for temp in row.split():\n",
        "              templates.add(temp)\n",
        "\n",
        "print(sorted(templates))\n",
        "print('nember of templates : ',len(templates))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pNDAhBmpwKSs",
        "outputId": "9dafb07b-09fe-4325-e610-535c24dd0d09"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['1', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '2', '20', '21', '22', '23', '24', '25', '26', '27', '28', '3', '4', '5', '6', '7', '8', '9']\n",
            "nember of templates :  28\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "test:\n",
        "\n",
        "\n",
        "```\n",
        "name = 'hdfs_train_sequence'\n",
        "window_size = 10\n",
        "num_sessions = 0\n",
        "inputs = []\n",
        "outputs = []\n",
        "\n",
        "with open('/content/' + name, 'r') as f:\n",
        "        for row in f:\n",
        "            num_sessions += 1\n",
        "            line = [ int(i) for i in row.strip().split()]\n",
        "            print(line)\n",
        "            for i in range(len(line) - window_size):\n",
        "                print(line[i:i + window_size])\n",
        "                print(line[i + window_size])\n",
        "                break\n",
        "            break\n",
        "\n",
        "ans:\n",
        "[0, 1, 0, 0, 2, 2, 3, 3, 2, 3, 4, 4, 4, 5,...]\n",
        "[0, 1, 0, 0, 2, 2, 3, 3, 2, 3]\n",
        "4\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "IXB3Npq912Um"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "name = 'hdfs_train'\n",
        "window_size = 10\n",
        "num_sessions = 0\n",
        "inputs = []\n",
        "outputs = []\n",
        "\n",
        "with open('/content/' + name, 'r') as f:\n",
        "        for row in f:\n",
        "            num_sessions += 1\n",
        "            line = [ (int(i)-1) for i in row.strip().split()]\n",
        "            for i in range(len(line) - window_size):\n",
        "                inputs.append(line[i:i + window_size])\n",
        "                outputs.append(line[i + window_size])\n",
        "\n",
        "print('Number of sessions({}): {}'.format(name, num_sessions))\n",
        "print('Number of seqs({}): {}'.format(name, len(inputs)))\n",
        "dataset = TensorDataset(torch.tensor(inputs, dtype=torch.float), torch.tensor(outputs))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w1P4keFzwKDn",
        "outputId": "cdbee43b-437f-42a8-fbff-6f2354862ff5"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of sessions(hdfs_train): 4855\n",
            "Number of seqs(hdfs_train): 46575\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Model(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
        "        super(Model, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
        "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
        "        out, _ = self.lstm(x, (h0, c0))  # out.shape : [batch_size, sequence_length, hidden_size]\n",
        "        out = self.fc(out[:, -1, :]) #The : before , -1, : indicates that we want to include all elements along the first dimension (batch dimension). -1 represents the index of the last element along the second dimension (sequence length). : after , -1 indicates that we want to include all elements along the third dimension (hidden size)\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "vVS_BE9S2J2p"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_size = 1\n",
        "num_layers = 2\n",
        "hidden_size = 64\n",
        "num_classes = 28  # templates + 1 abnormal output\n",
        "batch_size = 2048\n",
        "num_epochs = 375"
      ],
      "metadata": {
        "id": "R0ht4IKd2QFf"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Model(input_size, hidden_size, num_layers, num_classes).to(device)\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n"
      ],
      "metadata": {
        "id": "Ve9GNCW-2zDL"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# learning rate warm-up\n",
        "The reason for this warm-up strategy is that when the training starts, the model's weights are randomly initialized, and the optimizer might make large updates in the first few iterations. If the learning rate is too high during this phase, it can cause the model to diverge or take overly large steps and result in unstable training.\n",
        "\n",
        "If your data set is highly differentiated, you can suffer from a sort of \"early over-fitting\". If your shuffled data happens to include a cluster of related, strongly-featured observations, your model's initial training can skew badly toward those features -- or worse, toward incidental features that aren't truly related to the topic at all.\n",
        "\n",
        "Warm-up is particularly useful when using large-batch training, as it helps prevent sharp changes in the model's parameters, which can destabilize the optimization process.\n",
        "\n"
      ],
      "metadata": {
        "id": "n1ub-yvnzTuH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In deep learning, especially when using advanced optimization techniques like learning rate scheduling or weight decay, you may have multiple parameter groups with different learning rates, weight decay values, or other optimization-specific settings.\n",
        "\n",
        "Each parameter group typically corresponds to a specific set of model parameters. For example, in transfer learning, you may have one parameter group for the pre-trained layers with a lower learning rate and another parameter group for the newly added layers with a higher learning rate.\n",
        "\n",
        "For example, consider the following code:\n",
        "\n",
        "\n",
        "```\n",
        "optimizer = torch.optim.Adam([\n",
        "    {'params': model.fc.parameters(), 'lr': 0.001},  # Learning rate for the fully connected layer\n",
        "    {'params': model.conv.parameters(), 'lr': 0.0001},  # Learning rate for the convolutional layers\n",
        "], weight_decay=0.01)\n",
        "```\n",
        "In this case, the optimizer has two parameter groups: one for the fully connected layer and another for the convolutional layers. The for loop can be used to access and modify the learning rates for each parameter group as follows:\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "for param_group in optimizer.param_groups:\n",
        "    print(param_group['lr'])  # Print the learning rate for each parameter group\n",
        "    param_group['lr'] *= 0.1  # Multiply the learning rate by 0.1 for each parameter group\n",
        "```\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "d9kK4vZP3t_o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = optim.Adam(model.parameters(),lr=0.01)\n",
        "\n",
        "\n",
        "# we just one loop here\n",
        "print(optimizer.param_groups[0].keys())\n",
        "print(optimizer.param_groups[0]['lr'])\n",
        "print(optimizer.param_groups[0]['betas'])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uUOMWkoJ0kTn",
        "outputId": "25accbdd-cbbc-4586-a72b-747eddde25e1"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['params', 'lr', 'betas', 'eps', 'weight_decay', 'amsgrad', 'maximize', 'foreach', 'capturable', 'differentiable', 'fused'])\n",
            "0.01\n",
            "(0.9, 0.999)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "betas=(0.9, 0.999) indicates that the first moment (mean) will be updated using a moving average with a decay rate of 0.9, and the second moment (uncentered variance) will be updated using a moving average with a decay rate of 0.999. These values are commonly used as the default in many deep learning frameworks."
      ],
      "metadata": {
        "id": "X_4qzEBoGoH7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def adjust_learning_rate(optimizer, epoch, lr_step=(300, 350), lr_decay_ratio=0.1):\n",
        "    \"\"\"Adjust the learning rate based on the epoch number.\"\"\"\n",
        "    if epoch == 0:\n",
        "        optimizer.param_groups[0]['lr'] /= 32\n",
        "    elif epoch in [1, 2, 3, 4, 5]:  # in step five , we finish warm up ,and start normal learning rate\n",
        "        optimizer.param_groups[0]['lr'] *= 2\n",
        "    if epoch in lr_step: # in these steps , we are geting close to optimal point so we need to have shorter step\n",
        "        optimizer.param_groups[0]['lr'] *= lr_decay_ratio\n",
        "    return optimizer\n",
        "\n",
        "# Define options here\n",
        "options = {\n",
        "    'lr': 0.001,\n",
        "    'lr_step': (300, 350), #steps(epoch) for updating learning rate\n",
        "    'lr_decay_ratio': 0.1,\n",
        "    # Add other options here\n",
        "}\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=options['lr'], betas=(0.9, 0.999))\n"
      ],
      "metadata": {
        "id": "54g6LFd-IkwN"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nceLUGVXQJlc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "validation part\n",
        "\n",
        "```\n",
        "\n",
        "    if epoch >= num_epochs // 2 and epoch % 2 == 0:\n",
        "        model.eval()\n",
        "        total_losses = 0\n",
        "        for step, (seq, label) in enumerate(dataloader):\n",
        "            # Forward pass\n",
        "            seq = seq.clone().detach().view(-1, window_size, input_size).to(device)\n",
        "            output = model(seq)\n",
        "            loss = criterion(output, label.to(device))\n",
        "\n",
        "            # Backward and optimize\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            train_loss += loss.item()\n",
        "            optimizer.step()\n",
        "        print('Epoch [{}/{}], train_loss: {:.4f}'.format(epoch + 1, num_epochs, train_loss / total_step))```\n",
        "\n"
      ],
      "metadata": {
        "id": "Lu6AEn2kQKB2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Train the model\n",
        "start_time = time.time()\n",
        "total_step = len(dataloader)\n",
        "for epoch in range(num_epochs):  # Loop over the dataset multiple times\n",
        "    optimizer = adjust_learning_rate(optimizer, epoch, options['lr_step'], options['lr_decay_ratio'])\n",
        "    print(optimizer.param_groups[0]['lr'])\n",
        "    train_loss = 0\n",
        "    for step, (seq, label) in enumerate(dataloader):\n",
        "        # Forward pass\n",
        "        seq = seq.clone().detach().view(-1, window_size, input_size).to(device)\n",
        "        output = model(seq)\n",
        "        loss = criterion(output, label.to(device))\n",
        "\n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        train_loss += loss.item()\n",
        "        optimizer.step()\n",
        "    print('Epoch [{}/{}], train_loss: {:.4f}'.format(epoch + 1, num_epochs, train_loss / total_step))\n",
        "elapsed_time = time.time() - start_time\n",
        "print('elapsed_time: {:.3f}s'.format(elapsed_time))\n",
        "print('Finished Training')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MRDa44Rt23Lb",
        "outputId": "9264878b-f85c-49c3-b8bc-acf53d6fbb6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.125e-05\n",
            "Epoch [1/375], train_loss: 3.2987\n",
            "6.25e-05\n",
            "Epoch [2/375], train_loss: 3.2606\n",
            "0.000125\n",
            "Epoch [3/375], train_loss: 3.1662\n",
            "0.00025\n",
            "Epoch [4/375], train_loss: 2.7627\n",
            "0.0005\n",
            "Epoch [5/375], train_loss: 1.9722\n",
            "0.001\n",
            "Epoch [6/375], train_loss: 1.6034\n",
            "0.001\n",
            "Epoch [7/375], train_loss: 1.3347\n",
            "0.001\n",
            "Epoch [8/375], train_loss: 1.1209\n",
            "0.001\n",
            "Epoch [9/375], train_loss: 0.9453\n",
            "0.001\n",
            "Epoch [10/375], train_loss: 0.8230\n",
            "0.001\n",
            "Epoch [11/375], train_loss: 0.7400\n",
            "0.001\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lsa9Wiv-OUfq",
        "outputId": "c3263232-3d63-4a8e-cb90-87ed7ec61a81"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#save the model\n",
        "torch.save(model.state_dict(), '/content/drive/MyDrive/LSTM_model_parameter')"
      ],
      "metadata": {
        "id": "qVHEFEB4NqVS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# upload the model\n",
        "model_path = '/content/drive/MyDrive/LSTM_model_parameter'\n",
        "model.load_state_dict(torch.load(model_path))"
      ],
      "metadata": {
        "id": "pA_FNHo7ORnT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate(name):\n",
        "    # If you what to replicate the DeepLog paper results(Actually, I have a better result than DeepLog paper results),\n",
        "    # you should use the 'list' not 'set' to obtain the full dataset, I use 'set' just for test and acceleration.\n",
        "    #hdfs = set()\n",
        "    hdfs = []\n",
        "    with open('/content/' + name, 'r') as f:\n",
        "        for row in f:\n",
        "            line = [(int(i)-1) for i in row.strip().split()]\n",
        "            line = line + [-1] * (window_size + 1 - len(line)) #if the length of the line is less than windows size, it covers by -1\n",
        "            #hdfs.add(tuple(line))\n",
        "            hdfs.append(tuple(line))\n",
        "    print('Number of sessions({}): {}'.format(name, len(hdfs)))\n",
        "    return hdfs\n"
      ],
      "metadata": {
        "id": "g90HG-zc7V-Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "test_normal_loader = generate('hdfs_test_normal')\n",
        "test_abnormal_loader = generate('hdfs_test_abnormal')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6NnZp1dT75jI",
        "outputId": "6436b937-de56-4061-d9cc-3329709cae62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of sessions(hdfs_test_normal): 553366\n",
            "Number of sessions(hdfs_test_abnormal): 16838\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_candidates = 9 # on paper is g , top-g(here top 9) probabilities to appear next are considered normal"
      ],
      "metadata": {
        "id": "dSDO2VmH8dRF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the model\n",
        "model.eval()\n",
        "\n",
        "TP = 0\n",
        "FP = 0\n",
        "\n",
        "start_time = time.time()\n",
        "with torch.no_grad():\n",
        "    for line in test_normal_loader:\n",
        "        for i in range(len(line) - window_size):\n",
        "            seq = line[i:i + window_size]\n",
        "            label = line[i + window_size]\n",
        "            seq = torch.tensor(seq, dtype=torch.float).view(-1, window_size, input_size).to(device)\n",
        "            label = torch.tensor(label).view(-1).to(device)\n",
        "            output = model(seq)\n",
        "            predicted = torch.argsort(output, 1)[0][-num_candidates:]\n",
        "            if label not in predicted:\n",
        "                FP += 1\n",
        "                break   #with just one wrong prediction in a line , we assume , abnormal\n",
        "with torch.no_grad():\n",
        "    for line in test_abnormal_loader:\n",
        "        for i in range(len(line) - window_size):\n",
        "            seq = line[i:i + window_size]\n",
        "            label = line[i + window_size]\n",
        "            seq = torch.tensor(seq, dtype=torch.float).view(-1, window_size, input_size).to(device)\n",
        "            label = torch.tensor(label).view(-1).to(device)\n",
        "            output = model(seq)\n",
        "            predicted = torch.argsort(output, 1)[0][-num_candidates:]\n",
        "            if label not in predicted:\n",
        "                TP += 1\n",
        "                break\n",
        "elapsed_time = time.time() - start_time\n",
        "print('elapsed_time: {:.3f}s'.format(elapsed_time))\n",
        "# Compute precision, recall and F1-measure\n",
        "FN = len(test_abnormal_loader) - TP\n",
        "P = 100 * TP / (TP + FP)\n",
        "R = 100 * TP / (TP + FN)\n",
        "F1 = 2 * P * R / (P + R)\n",
        "print('false positive (FP): {}, false negative (FN): {}, Precision: {:.3f}%, Recall: {:.3f}%, F1-measure: {:.3f}%'.format(FP, FN, P, R, F1))\n",
        "print('Finished Predicting')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZIDRTh2-8ASi",
        "outputId": "71bcee6c-180c-4299-ae1b-436bc91c8bb4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "elapsed_time: 3368.678s\n",
            "false positive (FP): 867, false negative (FN): 1258, Precision: 94.729%, Recall: 92.529%, F1-measure: 93.616%\n",
            "Finished Predicting\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(TP)\n",
        "print(FN)\n",
        "print(FP)\n",
        "print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MwoQNRr0dfJm",
        "outputId": "530b1d81-bcd7-4491-dc0d-a0995ee42208"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "15580\n",
            "1258\n",
            "867\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "J1VFDfINdlKy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}