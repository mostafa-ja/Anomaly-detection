{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyO450jxOO5Vw08YaeDvTTSd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mostafa-ja/Anomaly-detection/blob/main/C2(deeplog_model).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
      ],
      "metadata": {
        "id": "-cgjPvYjFbKo"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VTdKXioxBmcc",
        "outputId": "26f25bea-1a32-4da1-a46c-4f8c53b4701d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Mount Google Drive to upload datasets (csv files)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import train and datasets based on ratio\n",
        "\n",
        "def split_log_file(input_file, train_ratio=0.7):\n",
        "    # Read the log file and split it into lines\n",
        "    with open(input_file, 'r') as log_file:\n",
        "        log_lines = log_file.readlines()\n",
        "\n",
        "    # Calculate the number of lines for the train and test sets\n",
        "    num_lines = len(log_lines)\n",
        "    num_train_lines = int(num_lines * train_ratio)\n",
        "    num_test_lines = num_lines - num_train_lines\n",
        "\n",
        "    # Write the lines corresponding to the train set to a new train log file\n",
        "    with open('hdfs_train', 'w') as train_file:\n",
        "        train_file.writelines(log_lines[:num_train_lines])\n",
        "\n",
        "    # Write the remaining lines (test set) to a new test log file\n",
        "    with open('hdfs_test_normal', 'w') as test_file:\n",
        "        test_file.writelines(log_lines[num_train_lines:])\n",
        "\n",
        "# split normal log file\n",
        "split_log_file('/content/drive/MyDrive/HDFS/structured_hdfs/hdfs_sequence_normal', train_ratio=0.1)\n",
        "\n",
        "#copy test abnormal file to current directory\n",
        "!cp '/content/drive/MyDrive/HDFS/structured_hdfs/hdfs_test_abnormal' '/content/'"
      ],
      "metadata": {
        "id": "p2LHttoEwKMp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "alternative files :\n",
        "\n",
        "\n",
        "```\n",
        "\n",
        "# download datasets\n",
        "!wget 'https://raw.githubusercontent.com/donglee-afar/logdeep/master/data/hdfs/hdfs_train'\n",
        "!wget 'https://raw.githubusercontent.com/donglee-afar/logdeep/master/data/hdfs/hdfs_test_normal'\n",
        "!wget 'https://raw.githubusercontent.com/donglee-afar/logdeep/master/data/hdfs/hdfs_test_abnormal'\n",
        "     \n",
        "```\n",
        "be careful these files are logs , not csv\n"
      ],
      "metadata": {
        "id": "Oa9CLMUMGGyO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# download datasets\n",
        "!wget 'https://raw.githubusercontent.com/donglee-afar/logdeep/master/data/hdfs/hdfs_train'\n",
        "!wget 'https://raw.githubusercontent.com/donglee-afar/logdeep/master/data/hdfs/hdfs_test_normal'\n",
        "!wget 'https://raw.githubusercontent.com/donglee-afar/logdeep/master/data/hdfs/hdfs_test_abnormal'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qZYNqcwLIPLF",
        "outputId": "301f6621-4205-4c5f-9f0e-d724da9e4122"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-07-25 16:22:07--  https://raw.githubusercontent.com/donglee-afar/logdeep/master/data/hdfs/hdfs_train\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 257875 (252K) [text/plain]\n",
            "Saving to: ‘hdfs_train’\n",
            "\n",
            "\rhdfs_train            0%[                    ]       0  --.-KB/s               \rhdfs_train          100%[===================>] 251.83K  --.-KB/s    in 0.006s  \n",
            "\n",
            "2023-07-25 16:22:07 (44.0 MB/s) - ‘hdfs_train’ saved [257875/257875]\n",
            "\n",
            "--2023-07-25 16:22:08--  https://raw.githubusercontent.com/donglee-afar/logdeep/master/data/hdfs/hdfs_test_normal\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 29284282 (28M) [text/plain]\n",
            "Saving to: ‘hdfs_test_normal’\n",
            "\n",
            "hdfs_test_normal    100%[===================>]  27.93M  60.3MB/s    in 0.5s    \n",
            "\n",
            "2023-07-25 16:22:08 (60.3 MB/s) - ‘hdfs_test_normal’ saved [29284282/29284282]\n",
            "\n",
            "--2023-07-25 16:22:08--  https://raw.githubusercontent.com/donglee-afar/logdeep/master/data/hdfs/hdfs_test_abnormal\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 782479 (764K) [text/plain]\n",
            "Saving to: ‘hdfs_test_abnormal’\n",
            "\n",
            "hdfs_test_abnormal  100%[===================>] 764.14K  --.-KB/s    in 0.01s   \n",
            "\n",
            "2023-07-25 16:22:09 (74.8 MB/s) - ‘hdfs_test_abnormal’ saved [782479/782479]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# count session for each dataset\n",
        "\n",
        "def count_sessions(dataset):\n",
        "    num_sessions = 0\n",
        "    with open('/content/'+ dataset, 'r') as f:\n",
        "        for row in f:\n",
        "            num_sessions += 1\n",
        "    print('Number of sessions({}): {}'.format(dataset, num_sessions))\n",
        "\n",
        "datasets = ['hdfs_train','hdfs_test_normal','hdfs_test_abnormal']\n",
        "\n",
        "for dataset in datasets:\n",
        "  count_sessions(dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BaCB9o7erHFe",
        "outputId": "9d2af22e-02bf-490b-d4e6-063a6c55b70e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of sessions(hdfs_train): 4855\n",
            "Number of sessions(hdfs_test_normal): 553366\n",
            "Number of sessions(hdfs_test_abnormal): 16838\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# all templates in our datasets\n",
        "\n",
        "datasets = ['hdfs_train','hdfs_test_normal','hdfs_test_abnormal']\n",
        "templates = set()\n",
        "\n",
        "for dataset in datasets:\n",
        "  with open('/content/' + dataset, 'r') as f:\n",
        "          for row in f:\n",
        "            for temp in row.split():\n",
        "              templates.add(temp)\n",
        "\n",
        "print(sorted(templates))\n",
        "print('nember of templates : ',len(templates))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pNDAhBmpwKSs",
        "outputId": "9dafb07b-09fe-4325-e610-535c24dd0d09"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['1', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '2', '20', '21', '22', '23', '24', '25', '26', '27', '28', '3', '4', '5', '6', '7', '8', '9']\n",
            "nember of templates :  28\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "test:\n",
        "\n",
        "\n",
        "```\n",
        "name = 'hdfs_train_sequence'\n",
        "window_size = 10\n",
        "num_sessions = 0\n",
        "inputs = []\n",
        "outputs = []\n",
        "\n",
        "with open('/content/' + name, 'r') as f:\n",
        "        for row in f:\n",
        "            num_sessions += 1\n",
        "            line = [ int(i) for i in row.strip().split()]\n",
        "            print(line)\n",
        "            for i in range(len(line) - window_size):\n",
        "                print(line[i:i + window_size])\n",
        "                print(line[i + window_size])\n",
        "                break\n",
        "            break\n",
        "\n",
        "ans:\n",
        "[0, 1, 0, 0, 2, 2, 3, 3, 2, 3, 4, 4, 4, 5,...]\n",
        "[0, 1, 0, 0, 2, 2, 3, 3, 2, 3]\n",
        "4\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "IXB3Npq912Um"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "name = 'hdfs_train'\n",
        "window_size = 10\n",
        "num_sessions = 0\n",
        "inputs = []\n",
        "outputs = []\n",
        "\n",
        "with open('/content/' + name, 'r') as f:\n",
        "        for row in f:\n",
        "            num_sessions += 1\n",
        "            line = [ (int(i)-1) for i in row.strip().split()]\n",
        "            for i in range(len(line) - window_size):\n",
        "                inputs.append(line[i:i + window_size])\n",
        "                outputs.append(line[i + window_size])\n",
        "\n",
        "print('Number of sessions({}): {}'.format(name, num_sessions))\n",
        "print('Number of seqs({}): {}'.format(name, len(inputs)))\n",
        "dataset = TensorDataset(torch.tensor(inputs, dtype=torch.float), torch.tensor(outputs))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w1P4keFzwKDn",
        "outputId": "cdbee43b-437f-42a8-fbff-6f2354862ff5"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of sessions(hdfs_train): 4855\n",
            "Number of seqs(hdfs_train): 46575\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Model(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
        "        super(Model, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
        "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
        "        out, _ = self.lstm(x, (h0, c0))  # out.shape : [batch_size, sequence_length, hidden_size]\n",
        "        out = self.fc(out[:, -1, :]) #The : before , -1, : indicates that we want to include all elements along the first dimension (batch dimension). -1 represents the index of the last element along the second dimension (sequence length). : after , -1 indicates that we want to include all elements along the third dimension (hidden size)\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "vVS_BE9S2J2p"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_size = 1\n",
        "num_layers = 2\n",
        "hidden_size = 64\n",
        "num_classes = 28  # templates + 1 abnormal output\n",
        "batch_size = 2048\n",
        "num_epochs = 375"
      ],
      "metadata": {
        "id": "R0ht4IKd2QFf"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Model(input_size, hidden_size, num_layers, num_classes).to(device)\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n"
      ],
      "metadata": {
        "id": "Ve9GNCW-2zDL"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# learning rate warm-up\n",
        "The reason for this warm-up strategy is that when the training starts, the model's weights are randomly initialized, and the optimizer might make large updates in the first few iterations. If the learning rate is too high during this phase, it can cause the model to diverge or take overly large steps and result in unstable training.\n",
        "\n",
        "If your data set is highly differentiated, you can suffer from a sort of \"early over-fitting\". If your shuffled data happens to include a cluster of related, strongly-featured observations, your model's initial training can skew badly toward those features -- or worse, toward incidental features that aren't truly related to the topic at all.\n",
        "\n",
        "Warm-up is particularly useful when using large-batch training, as it helps prevent sharp changes in the model's parameters, which can destabilize the optimization process.\n",
        "\n"
      ],
      "metadata": {
        "id": "n1ub-yvnzTuH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In deep learning, especially when using advanced optimization techniques like learning rate scheduling or weight decay, you may have multiple parameter groups with different learning rates, weight decay values, or other optimization-specific settings.\n",
        "\n",
        "Each parameter group typically corresponds to a specific set of model parameters. For example, in transfer learning, you may have one parameter group for the pre-trained layers with a lower learning rate and another parameter group for the newly added layers with a higher learning rate.\n",
        "\n",
        "For example, consider the following code:\n",
        "\n",
        "\n",
        "```\n",
        "optimizer = torch.optim.Adam([\n",
        "    {'params': model.fc.parameters(), 'lr': 0.001},  # Learning rate for the fully connected layer\n",
        "    {'params': model.conv.parameters(), 'lr': 0.0001},  # Learning rate for the convolutional layers\n",
        "], weight_decay=0.01)\n",
        "```\n",
        "In this case, the optimizer has two parameter groups: one for the fully connected layer and another for the convolutional layers. The for loop can be used to access and modify the learning rates for each parameter group as follows:\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "for param_group in optimizer.param_groups:\n",
        "    print(param_group['lr'])  # Print the learning rate for each parameter group\n",
        "    param_group['lr'] *= 0.1  # Multiply the learning rate by 0.1 for each parameter group\n",
        "```\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "d9kK4vZP3t_o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = optim.Adam(model.parameters(),lr=0.01)\n",
        "\n",
        "\n",
        "# we just one loop here\n",
        "print(optimizer.param_groups[0].keys())\n",
        "print(optimizer.param_groups[0]['lr'])\n",
        "print(optimizer.param_groups[0]['betas'])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uUOMWkoJ0kTn",
        "outputId": "25accbdd-cbbc-4586-a72b-747eddde25e1"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['params', 'lr', 'betas', 'eps', 'weight_decay', 'amsgrad', 'maximize', 'foreach', 'capturable', 'differentiable', 'fused'])\n",
            "0.01\n",
            "(0.9, 0.999)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "betas=(0.9, 0.999) indicates that the first moment (mean) will be updated using a moving average with a decay rate of 0.9, and the second moment (uncentered variance) will be updated using a moving average with a decay rate of 0.999. These values are commonly used as the default in many deep learning frameworks."
      ],
      "metadata": {
        "id": "X_4qzEBoGoH7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def adjust_learning_rate(optimizer, epoch, lr_step=(300, 350), lr_decay_ratio=0.1):\n",
        "    \"\"\"Adjust the learning rate based on the epoch number.\"\"\"\n",
        "    if epoch == 0:\n",
        "        optimizer.param_groups[0]['lr'] /= 32\n",
        "    elif epoch in [1, 2, 3, 4, 5]:  # in step five , we finish warm up ,and start normal learning rate\n",
        "        optimizer.param_groups[0]['lr'] *= 2\n",
        "    if epoch in lr_step: # in these steps , we are geting close to optimal point so we need to have shorter step\n",
        "        optimizer.param_groups[0]['lr'] *= lr_decay_ratio\n",
        "    return optimizer\n",
        "\n",
        "# Define options here\n",
        "options = {\n",
        "    'lr': 0.001,\n",
        "    'lr_step': (300, 350), #steps(epoch) for updating learning rate\n",
        "    'lr_decay_ratio': 0.1,\n",
        "    # Add other options here\n",
        "}\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=options['lr'], betas=(0.9, 0.999))\n"
      ],
      "metadata": {
        "id": "54g6LFd-IkwN"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nceLUGVXQJlc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "validation part\n",
        "\n",
        "```\n",
        "\n",
        "    if epoch >= num_epochs // 2 and epoch % 2 == 0:\n",
        "        model.eval()\n",
        "        total_losses = 0\n",
        "        for step, (seq, label) in enumerate(dataloader):\n",
        "            # Forward pass\n",
        "            seq = seq.clone().detach().view(-1, window_size, input_size).to(device)\n",
        "            output = model(seq)\n",
        "            loss = criterion(output, label.to(device))\n",
        "\n",
        "            # Backward and optimize\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            train_loss += loss.item()\n",
        "            optimizer.step()\n",
        "        print('Epoch [{}/{}], train_loss: {:.4f}'.format(epoch + 1, num_epochs, train_loss / total_step))```\n",
        "\n"
      ],
      "metadata": {
        "id": "Lu6AEn2kQKB2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Train the model\n",
        "start_time = time.time()\n",
        "total_step = len(dataloader)\n",
        "for epoch in range(num_epochs):  # Loop over the dataset multiple times\n",
        "    optimizer = adjust_learning_rate(optimizer, epoch, options['lr_step'], options['lr_decay_ratio'])\n",
        "    print(optimizer.param_groups[0]['lr'])\n",
        "    train_loss = 0\n",
        "    for step, (seq, label) in enumerate(dataloader):\n",
        "        # Forward pass\n",
        "        seq = seq.clone().detach().view(-1, window_size, input_size).to(device)\n",
        "        output = model(seq)\n",
        "        loss = criterion(output, label.to(device))\n",
        "\n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        train_loss += loss.item()\n",
        "        optimizer.step()\n",
        "    print('Epoch [{}/{}], train_loss: {:.4f}'.format(epoch + 1, num_epochs, train_loss / total_step))\n",
        "elapsed_time = time.time() - start_time\n",
        "print('elapsed_time: {:.3f}s'.format(elapsed_time))\n",
        "print('Finished Training')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MRDa44Rt23Lb",
        "outputId": "9264878b-f85c-49c3-b8bc-acf53d6fbb6c"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.125e-05\n",
            "Epoch [1/375], train_loss: 3.2987\n",
            "6.25e-05\n",
            "Epoch [2/375], train_loss: 3.2606\n",
            "0.000125\n",
            "Epoch [3/375], train_loss: 3.1662\n",
            "0.00025\n",
            "Epoch [4/375], train_loss: 2.7627\n",
            "0.0005\n",
            "Epoch [5/375], train_loss: 1.9722\n",
            "0.001\n",
            "Epoch [6/375], train_loss: 1.6034\n",
            "0.001\n",
            "Epoch [7/375], train_loss: 1.3347\n",
            "0.001\n",
            "Epoch [8/375], train_loss: 1.1209\n",
            "0.001\n",
            "Epoch [9/375], train_loss: 0.9453\n",
            "0.001\n",
            "Epoch [10/375], train_loss: 0.8230\n",
            "0.001\n",
            "Epoch [11/375], train_loss: 0.7400\n",
            "0.001\n",
            "Epoch [12/375], train_loss: 0.6756\n",
            "0.001\n",
            "Epoch [13/375], train_loss: 0.6253\n",
            "0.001\n",
            "Epoch [14/375], train_loss: 0.5848\n",
            "0.001\n",
            "Epoch [15/375], train_loss: 0.5557\n",
            "0.001\n",
            "Epoch [16/375], train_loss: 0.5313\n",
            "0.001\n",
            "Epoch [17/375], train_loss: 0.5242\n",
            "0.001\n",
            "Epoch [18/375], train_loss: 0.4983\n",
            "0.001\n",
            "Epoch [19/375], train_loss: 0.4852\n",
            "0.001\n",
            "Epoch [20/375], train_loss: 0.4734\n",
            "0.001\n",
            "Epoch [21/375], train_loss: 0.4651\n",
            "0.001\n",
            "Epoch [22/375], train_loss: 0.4552\n",
            "0.001\n",
            "Epoch [23/375], train_loss: 0.4462\n",
            "0.001\n",
            "Epoch [24/375], train_loss: 0.4389\n",
            "0.001\n",
            "Epoch [25/375], train_loss: 0.4333\n",
            "0.001\n",
            "Epoch [26/375], train_loss: 0.4257\n",
            "0.001\n",
            "Epoch [27/375], train_loss: 0.4187\n",
            "0.001\n",
            "Epoch [28/375], train_loss: 0.4110\n",
            "0.001\n",
            "Epoch [29/375], train_loss: 0.4036\n",
            "0.001\n",
            "Epoch [30/375], train_loss: 0.3983\n",
            "0.001\n",
            "Epoch [31/375], train_loss: 0.3938\n",
            "0.001\n",
            "Epoch [32/375], train_loss: 0.3896\n",
            "0.001\n",
            "Epoch [33/375], train_loss: 0.3809\n",
            "0.001\n",
            "Epoch [34/375], train_loss: 0.3793\n",
            "0.001\n",
            "Epoch [35/375], train_loss: 0.3746\n",
            "0.001\n",
            "Epoch [36/375], train_loss: 0.3713\n",
            "0.001\n",
            "Epoch [37/375], train_loss: 0.3665\n",
            "0.001\n",
            "Epoch [38/375], train_loss: 0.3631\n",
            "0.001\n",
            "Epoch [39/375], train_loss: 0.3604\n",
            "0.001\n",
            "Epoch [40/375], train_loss: 0.3578\n",
            "0.001\n",
            "Epoch [41/375], train_loss: 0.3565\n",
            "0.001\n",
            "Epoch [42/375], train_loss: 0.3788\n",
            "0.001\n",
            "Epoch [43/375], train_loss: 0.3595\n",
            "0.001\n",
            "Epoch [44/375], train_loss: 0.3501\n",
            "0.001\n",
            "Epoch [45/375], train_loss: 0.3477\n",
            "0.001\n",
            "Epoch [46/375], train_loss: 0.3445\n",
            "0.001\n",
            "Epoch [47/375], train_loss: 0.3419\n",
            "0.001\n",
            "Epoch [48/375], train_loss: 0.3405\n",
            "0.001\n",
            "Epoch [49/375], train_loss: 0.3382\n",
            "0.001\n",
            "Epoch [50/375], train_loss: 0.3366\n",
            "0.001\n",
            "Epoch [51/375], train_loss: 0.3342\n",
            "0.001\n",
            "Epoch [52/375], train_loss: 0.3322\n",
            "0.001\n",
            "Epoch [53/375], train_loss: 0.3316\n",
            "0.001\n",
            "Epoch [54/375], train_loss: 0.3288\n",
            "0.001\n",
            "Epoch [55/375], train_loss: 0.3273\n",
            "0.001\n",
            "Epoch [56/375], train_loss: 0.3268\n",
            "0.001\n",
            "Epoch [57/375], train_loss: 0.3242\n",
            "0.001\n",
            "Epoch [58/375], train_loss: 0.3222\n",
            "0.001\n",
            "Epoch [59/375], train_loss: 0.3637\n",
            "0.001\n",
            "Epoch [60/375], train_loss: 0.3794\n",
            "0.001\n",
            "Epoch [61/375], train_loss: 0.3359\n",
            "0.001\n",
            "Epoch [62/375], train_loss: 0.3247\n",
            "0.001\n",
            "Epoch [63/375], train_loss: 0.3209\n",
            "0.001\n",
            "Epoch [64/375], train_loss: 0.3175\n",
            "0.001\n",
            "Epoch [65/375], train_loss: 0.3166\n",
            "0.001\n",
            "Epoch [66/375], train_loss: 0.3147\n",
            "0.001\n",
            "Epoch [67/375], train_loss: 0.3129\n",
            "0.001\n",
            "Epoch [68/375], train_loss: 0.3126\n",
            "0.001\n",
            "Epoch [69/375], train_loss: 0.3101\n",
            "0.001\n",
            "Epoch [70/375], train_loss: 0.3085\n",
            "0.001\n",
            "Epoch [71/375], train_loss: 0.3082\n",
            "0.001\n",
            "Epoch [72/375], train_loss: 0.3063\n",
            "0.001\n",
            "Epoch [73/375], train_loss: 0.3058\n",
            "0.001\n",
            "Epoch [74/375], train_loss: 0.3042\n",
            "0.001\n",
            "Epoch [75/375], train_loss: 0.3032\n",
            "0.001\n",
            "Epoch [76/375], train_loss: 0.3014\n",
            "0.001\n",
            "Epoch [77/375], train_loss: 0.2994\n",
            "0.001\n",
            "Epoch [78/375], train_loss: 0.2989\n",
            "0.001\n",
            "Epoch [79/375], train_loss: 0.2990\n",
            "0.001\n",
            "Epoch [80/375], train_loss: 0.2978\n",
            "0.001\n",
            "Epoch [81/375], train_loss: 0.2950\n",
            "0.001\n",
            "Epoch [82/375], train_loss: 0.2936\n",
            "0.001\n",
            "Epoch [83/375], train_loss: 0.2945\n",
            "0.001\n",
            "Epoch [84/375], train_loss: 0.2915\n",
            "0.001\n",
            "Epoch [85/375], train_loss: 0.2903\n",
            "0.001\n",
            "Epoch [86/375], train_loss: 0.2884\n",
            "0.001\n",
            "Epoch [87/375], train_loss: 0.2889\n",
            "0.001\n",
            "Epoch [88/375], train_loss: 0.2869\n",
            "0.001\n",
            "Epoch [89/375], train_loss: 0.2849\n",
            "0.001\n",
            "Epoch [90/375], train_loss: 0.2840\n",
            "0.001\n",
            "Epoch [91/375], train_loss: 0.2826\n",
            "0.001\n",
            "Epoch [92/375], train_loss: 0.2819\n",
            "0.001\n",
            "Epoch [93/375], train_loss: 0.2844\n",
            "0.001\n",
            "Epoch [94/375], train_loss: 0.2819\n",
            "0.001\n",
            "Epoch [95/375], train_loss: 0.2802\n",
            "0.001\n",
            "Epoch [96/375], train_loss: 0.2768\n",
            "0.001\n",
            "Epoch [97/375], train_loss: 0.2747\n",
            "0.001\n",
            "Epoch [98/375], train_loss: 0.2739\n",
            "0.001\n",
            "Epoch [99/375], train_loss: 0.2732\n",
            "0.001\n",
            "Epoch [100/375], train_loss: 0.2769\n",
            "0.001\n",
            "Epoch [101/375], train_loss: 0.3337\n",
            "0.001\n",
            "Epoch [102/375], train_loss: 0.2830\n",
            "0.001\n",
            "Epoch [103/375], train_loss: 0.2748\n",
            "0.001\n",
            "Epoch [104/375], train_loss: 0.2708\n",
            "0.001\n",
            "Epoch [105/375], train_loss: 0.2691\n",
            "0.001\n",
            "Epoch [106/375], train_loss: 0.2683\n",
            "0.001\n",
            "Epoch [107/375], train_loss: 0.2670\n",
            "0.001\n",
            "Epoch [108/375], train_loss: 0.2662\n",
            "0.001\n",
            "Epoch [109/375], train_loss: 0.2657\n",
            "0.001\n",
            "Epoch [110/375], train_loss: 0.2647\n",
            "0.001\n",
            "Epoch [111/375], train_loss: 0.2636\n",
            "0.001\n",
            "Epoch [112/375], train_loss: 0.2633\n",
            "0.001\n",
            "Epoch [113/375], train_loss: 0.2617\n",
            "0.001\n",
            "Epoch [114/375], train_loss: 0.2603\n",
            "0.001\n",
            "Epoch [115/375], train_loss: 0.2605\n",
            "0.001\n",
            "Epoch [116/375], train_loss: 0.2595\n",
            "0.001\n",
            "Epoch [117/375], train_loss: 0.2586\n",
            "0.001\n",
            "Epoch [118/375], train_loss: 0.2580\n",
            "0.001\n",
            "Epoch [119/375], train_loss: 0.2577\n",
            "0.001\n",
            "Epoch [120/375], train_loss: 0.2567\n",
            "0.001\n",
            "Epoch [121/375], train_loss: 0.2563\n",
            "0.001\n",
            "Epoch [122/375], train_loss: 0.2550\n",
            "0.001\n",
            "Epoch [123/375], train_loss: 0.2546\n",
            "0.001\n",
            "Epoch [124/375], train_loss: 0.2537\n",
            "0.001\n",
            "Epoch [125/375], train_loss: 0.2525\n",
            "0.001\n",
            "Epoch [126/375], train_loss: 0.2522\n",
            "0.001\n",
            "Epoch [127/375], train_loss: 0.2515\n",
            "0.001\n",
            "Epoch [128/375], train_loss: 0.2509\n",
            "0.001\n",
            "Epoch [129/375], train_loss: 0.2502\n",
            "0.001\n",
            "Epoch [130/375], train_loss: 0.2491\n",
            "0.001\n",
            "Epoch [131/375], train_loss: 0.2488\n",
            "0.001\n",
            "Epoch [132/375], train_loss: 0.2479\n",
            "0.001\n",
            "Epoch [133/375], train_loss: 0.2468\n",
            "0.001\n",
            "Epoch [134/375], train_loss: 0.2459\n",
            "0.001\n",
            "Epoch [135/375], train_loss: 0.2457\n",
            "0.001\n",
            "Epoch [136/375], train_loss: 0.2448\n",
            "0.001\n",
            "Epoch [137/375], train_loss: 0.2440\n",
            "0.001\n",
            "Epoch [138/375], train_loss: 0.2433\n",
            "0.001\n",
            "Epoch [139/375], train_loss: 0.2427\n",
            "0.001\n",
            "Epoch [140/375], train_loss: 0.2423\n",
            "0.001\n",
            "Epoch [141/375], train_loss: 0.2414\n",
            "0.001\n",
            "Epoch [142/375], train_loss: 0.2425\n",
            "0.001\n",
            "Epoch [143/375], train_loss: 0.2419\n",
            "0.001\n",
            "Epoch [144/375], train_loss: 0.2403\n",
            "0.001\n",
            "Epoch [145/375], train_loss: 0.2406\n",
            "0.001\n",
            "Epoch [146/375], train_loss: 0.2401\n",
            "0.001\n",
            "Epoch [147/375], train_loss: 0.2387\n",
            "0.001\n",
            "Epoch [148/375], train_loss: 0.2385\n",
            "0.001\n",
            "Epoch [149/375], train_loss: 0.2383\n",
            "0.001\n",
            "Epoch [150/375], train_loss: 0.2378\n",
            "0.001\n",
            "Epoch [151/375], train_loss: 0.2357\n",
            "0.001\n",
            "Epoch [152/375], train_loss: 0.2352\n",
            "0.001\n",
            "Epoch [153/375], train_loss: 0.2349\n",
            "0.001\n",
            "Epoch [154/375], train_loss: 0.2346\n",
            "0.001\n",
            "Epoch [155/375], train_loss: 0.2346\n",
            "0.001\n",
            "Epoch [156/375], train_loss: 0.2338\n",
            "0.001\n",
            "Epoch [157/375], train_loss: 0.2337\n",
            "0.001\n",
            "Epoch [158/375], train_loss: 0.2325\n",
            "0.001\n",
            "Epoch [159/375], train_loss: 0.2322\n",
            "0.001\n",
            "Epoch [160/375], train_loss: 0.2313\n",
            "0.001\n",
            "Epoch [161/375], train_loss: 0.2313\n",
            "0.001\n",
            "Epoch [162/375], train_loss: 0.2300\n",
            "0.001\n",
            "Epoch [163/375], train_loss: 0.2313\n",
            "0.001\n",
            "Epoch [164/375], train_loss: 0.2310\n",
            "0.001\n",
            "Epoch [165/375], train_loss: 0.2310\n",
            "0.001\n",
            "Epoch [166/375], train_loss: 0.2290\n",
            "0.001\n",
            "Epoch [167/375], train_loss: 0.2292\n",
            "0.001\n",
            "Epoch [168/375], train_loss: 0.2283\n",
            "0.001\n",
            "Epoch [169/375], train_loss: 0.3576\n",
            "0.001\n",
            "Epoch [170/375], train_loss: 0.3161\n",
            "0.001\n",
            "Epoch [171/375], train_loss: 0.2689\n",
            "0.001\n",
            "Epoch [172/375], train_loss: 0.2549\n",
            "0.001\n",
            "Epoch [173/375], train_loss: 0.2480\n",
            "0.001\n",
            "Epoch [174/375], train_loss: 0.2432\n",
            "0.001\n",
            "Epoch [175/375], train_loss: 0.2396\n",
            "0.001\n",
            "Epoch [176/375], train_loss: 0.2379\n",
            "0.001\n",
            "Epoch [177/375], train_loss: 0.2359\n",
            "0.001\n",
            "Epoch [178/375], train_loss: 0.2334\n",
            "0.001\n",
            "Epoch [179/375], train_loss: 0.2317\n",
            "0.001\n",
            "Epoch [180/375], train_loss: 0.2298\n",
            "0.001\n",
            "Epoch [181/375], train_loss: 0.2296\n",
            "0.001\n",
            "Epoch [182/375], train_loss: 0.2286\n",
            "0.001\n",
            "Epoch [183/375], train_loss: 0.2276\n",
            "0.001\n",
            "Epoch [184/375], train_loss: 0.2269\n",
            "0.001\n",
            "Epoch [185/375], train_loss: 0.2262\n",
            "0.001\n",
            "Epoch [186/375], train_loss: 0.2263\n",
            "0.001\n",
            "Epoch [187/375], train_loss: 0.2258\n",
            "0.001\n",
            "Epoch [188/375], train_loss: 0.2246\n",
            "0.001\n",
            "Epoch [189/375], train_loss: 0.2258\n",
            "0.001\n",
            "Epoch [190/375], train_loss: 0.2257\n",
            "0.001\n",
            "Epoch [191/375], train_loss: 0.2246\n",
            "0.001\n",
            "Epoch [192/375], train_loss: 0.2247\n",
            "0.001\n",
            "Epoch [193/375], train_loss: 0.2233\n",
            "0.001\n",
            "Epoch [194/375], train_loss: 0.2230\n",
            "0.001\n",
            "Epoch [195/375], train_loss: 0.2227\n",
            "0.001\n",
            "Epoch [196/375], train_loss: 0.2224\n",
            "0.001\n",
            "Epoch [197/375], train_loss: 0.2221\n",
            "0.001\n",
            "Epoch [198/375], train_loss: 0.2232\n",
            "0.001\n",
            "Epoch [199/375], train_loss: 0.2219\n",
            "0.001\n",
            "Epoch [200/375], train_loss: 0.2216\n",
            "0.001\n",
            "Epoch [201/375], train_loss: 0.2212\n",
            "0.001\n",
            "Epoch [202/375], train_loss: 0.2204\n",
            "0.001\n",
            "Epoch [203/375], train_loss: 0.2206\n",
            "0.001\n",
            "Epoch [204/375], train_loss: 0.2205\n",
            "0.001\n",
            "Epoch [205/375], train_loss: 0.2200\n",
            "0.001\n",
            "Epoch [206/375], train_loss: 0.2193\n",
            "0.001\n",
            "Epoch [207/375], train_loss: 0.2187\n",
            "0.001\n",
            "Epoch [208/375], train_loss: 0.2195\n",
            "0.001\n",
            "Epoch [209/375], train_loss: 0.2190\n",
            "0.001\n",
            "Epoch [210/375], train_loss: 0.2185\n",
            "0.001\n",
            "Epoch [211/375], train_loss: 0.2180\n",
            "0.001\n",
            "Epoch [212/375], train_loss: 0.2181\n",
            "0.001\n",
            "Epoch [213/375], train_loss: 0.2185\n",
            "0.001\n",
            "Epoch [214/375], train_loss: 0.2165\n",
            "0.001\n",
            "Epoch [215/375], train_loss: 0.2185\n",
            "0.001\n",
            "Epoch [216/375], train_loss: 0.2173\n",
            "0.001\n",
            "Epoch [217/375], train_loss: 0.2167\n",
            "0.001\n",
            "Epoch [218/375], train_loss: 0.2168\n",
            "0.001\n",
            "Epoch [219/375], train_loss: 0.2171\n",
            "0.001\n",
            "Epoch [220/375], train_loss: 0.2159\n",
            "0.001\n",
            "Epoch [221/375], train_loss: 0.2163\n",
            "0.001\n",
            "Epoch [222/375], train_loss: 0.2152\n",
            "0.001\n",
            "Epoch [223/375], train_loss: 0.2147\n",
            "0.001\n",
            "Epoch [224/375], train_loss: 0.2150\n",
            "0.001\n",
            "Epoch [225/375], train_loss: 0.2147\n",
            "0.001\n",
            "Epoch [226/375], train_loss: 0.2151\n",
            "0.001\n",
            "Epoch [227/375], train_loss: 0.2146\n",
            "0.001\n",
            "Epoch [228/375], train_loss: 0.2142\n",
            "0.001\n",
            "Epoch [229/375], train_loss: 0.2130\n",
            "0.001\n",
            "Epoch [230/375], train_loss: 0.2131\n",
            "0.001\n",
            "Epoch [231/375], train_loss: 0.2133\n",
            "0.001\n",
            "Epoch [232/375], train_loss: 0.2131\n",
            "0.001\n",
            "Epoch [233/375], train_loss: 0.2124\n",
            "0.001\n",
            "Epoch [234/375], train_loss: 0.2132\n",
            "0.001\n",
            "Epoch [235/375], train_loss: 0.2134\n",
            "0.001\n",
            "Epoch [236/375], train_loss: 0.2123\n",
            "0.001\n",
            "Epoch [237/375], train_loss: 0.2127\n",
            "0.001\n",
            "Epoch [238/375], train_loss: 0.2120\n",
            "0.001\n",
            "Epoch [239/375], train_loss: 0.2124\n",
            "0.001\n",
            "Epoch [240/375], train_loss: 0.2119\n",
            "0.001\n",
            "Epoch [241/375], train_loss: 0.2117\n",
            "0.001\n",
            "Epoch [242/375], train_loss: 0.2113\n",
            "0.001\n",
            "Epoch [243/375], train_loss: 0.2113\n",
            "0.001\n",
            "Epoch [244/375], train_loss: 0.2112\n",
            "0.001\n",
            "Epoch [245/375], train_loss: 0.2118\n",
            "0.001\n",
            "Epoch [246/375], train_loss: 0.2118\n",
            "0.001\n",
            "Epoch [247/375], train_loss: 0.2125\n",
            "0.001\n",
            "Epoch [248/375], train_loss: 0.2116\n",
            "0.001\n",
            "Epoch [249/375], train_loss: 0.2104\n",
            "0.001\n",
            "Epoch [250/375], train_loss: 0.2095\n",
            "0.001\n",
            "Epoch [251/375], train_loss: 0.2096\n",
            "0.001\n",
            "Epoch [252/375], train_loss: 0.2099\n",
            "0.001\n",
            "Epoch [253/375], train_loss: 0.2102\n",
            "0.001\n",
            "Epoch [254/375], train_loss: 0.2102\n",
            "0.001\n",
            "Epoch [255/375], train_loss: 0.2102\n",
            "0.001\n",
            "Epoch [256/375], train_loss: 0.2088\n",
            "0.001\n",
            "Epoch [257/375], train_loss: 0.2090\n",
            "0.001\n",
            "Epoch [258/375], train_loss: 0.2094\n",
            "0.001\n",
            "Epoch [259/375], train_loss: 0.2088\n",
            "0.001\n",
            "Epoch [260/375], train_loss: 0.2091\n",
            "0.001\n",
            "Epoch [261/375], train_loss: 0.2090\n",
            "0.001\n",
            "Epoch [262/375], train_loss: 0.2081\n",
            "0.001\n",
            "Epoch [263/375], train_loss: 0.2085\n",
            "0.001\n",
            "Epoch [264/375], train_loss: 0.2073\n",
            "0.001\n",
            "Epoch [265/375], train_loss: 0.2078\n",
            "0.001\n",
            "Epoch [266/375], train_loss: 0.2074\n",
            "0.001\n",
            "Epoch [267/375], train_loss: 0.2075\n",
            "0.001\n",
            "Epoch [268/375], train_loss: 0.2073\n",
            "0.001\n",
            "Epoch [269/375], train_loss: 0.2071\n",
            "0.001\n",
            "Epoch [270/375], train_loss: 0.2070\n",
            "0.001\n",
            "Epoch [271/375], train_loss: 0.2061\n",
            "0.001\n",
            "Epoch [272/375], train_loss: 0.2062\n",
            "0.001\n",
            "Epoch [273/375], train_loss: 0.2067\n",
            "0.001\n",
            "Epoch [274/375], train_loss: 0.2065\n",
            "0.001\n",
            "Epoch [275/375], train_loss: 0.2053\n",
            "0.001\n",
            "Epoch [276/375], train_loss: 0.2060\n",
            "0.001\n",
            "Epoch [277/375], train_loss: 0.2055\n",
            "0.001\n",
            "Epoch [278/375], train_loss: 0.2055\n",
            "0.001\n",
            "Epoch [279/375], train_loss: 0.2048\n",
            "0.001\n",
            "Epoch [280/375], train_loss: 0.2047\n",
            "0.001\n",
            "Epoch [281/375], train_loss: 0.2052\n",
            "0.001\n",
            "Epoch [282/375], train_loss: 0.2053\n",
            "0.001\n",
            "Epoch [283/375], train_loss: 0.2048\n",
            "0.001\n",
            "Epoch [284/375], train_loss: 0.2061\n",
            "0.001\n",
            "Epoch [285/375], train_loss: 0.2048\n",
            "0.001\n",
            "Epoch [286/375], train_loss: 0.2039\n",
            "0.001\n",
            "Epoch [287/375], train_loss: 0.2043\n",
            "0.001\n",
            "Epoch [288/375], train_loss: 0.2052\n",
            "0.001\n",
            "Epoch [289/375], train_loss: 0.2042\n",
            "0.001\n",
            "Epoch [290/375], train_loss: 0.2035\n",
            "0.001\n",
            "Epoch [291/375], train_loss: 0.2033\n",
            "0.001\n",
            "Epoch [292/375], train_loss: 0.2040\n",
            "0.001\n",
            "Epoch [293/375], train_loss: 0.2040\n",
            "0.001\n",
            "Epoch [294/375], train_loss: 0.2042\n",
            "0.001\n",
            "Epoch [295/375], train_loss: 0.2038\n",
            "0.001\n",
            "Epoch [296/375], train_loss: 0.2035\n",
            "0.001\n",
            "Epoch [297/375], train_loss: 0.2034\n",
            "0.001\n",
            "Epoch [298/375], train_loss: 0.2022\n",
            "0.001\n",
            "Epoch [299/375], train_loss: 0.2105\n",
            "0.001\n",
            "Epoch [300/375], train_loss: 0.2608\n",
            "0.0001\n",
            "Epoch [301/375], train_loss: 0.2190\n",
            "0.0001\n",
            "Epoch [302/375], train_loss: 0.2138\n",
            "0.0001\n",
            "Epoch [303/375], train_loss: 0.2116\n",
            "0.0001\n",
            "Epoch [304/375], train_loss: 0.2105\n",
            "0.0001\n",
            "Epoch [305/375], train_loss: 0.2095\n",
            "0.0001\n",
            "Epoch [306/375], train_loss: 0.2087\n",
            "0.0001\n",
            "Epoch [307/375], train_loss: 0.2082\n",
            "0.0001\n",
            "Epoch [308/375], train_loss: 0.2072\n",
            "0.0001\n",
            "Epoch [309/375], train_loss: 0.2067\n",
            "0.0001\n",
            "Epoch [310/375], train_loss: 0.2062\n",
            "0.0001\n",
            "Epoch [311/375], train_loss: 0.2057\n",
            "0.0001\n",
            "Epoch [312/375], train_loss: 0.2060\n",
            "0.0001\n",
            "Epoch [313/375], train_loss: 0.2054\n",
            "0.0001\n",
            "Epoch [314/375], train_loss: 0.2050\n",
            "0.0001\n",
            "Epoch [315/375], train_loss: 0.2046\n",
            "0.0001\n",
            "Epoch [316/375], train_loss: 0.2042\n",
            "0.0001\n",
            "Epoch [317/375], train_loss: 0.2040\n",
            "0.0001\n",
            "Epoch [318/375], train_loss: 0.2039\n",
            "0.0001\n",
            "Epoch [319/375], train_loss: 0.2037\n",
            "0.0001\n",
            "Epoch [320/375], train_loss: 0.2034\n",
            "0.0001\n",
            "Epoch [321/375], train_loss: 0.2033\n",
            "0.0001\n",
            "Epoch [322/375], train_loss: 0.2032\n",
            "0.0001\n",
            "Epoch [323/375], train_loss: 0.2030\n",
            "0.0001\n",
            "Epoch [324/375], train_loss: 0.2026\n",
            "0.0001\n",
            "Epoch [325/375], train_loss: 0.2028\n",
            "0.0001\n",
            "Epoch [326/375], train_loss: 0.2022\n",
            "0.0001\n",
            "Epoch [327/375], train_loss: 0.2022\n",
            "0.0001\n",
            "Epoch [328/375], train_loss: 0.2020\n",
            "0.0001\n",
            "Epoch [329/375], train_loss: 0.2019\n",
            "0.0001\n",
            "Epoch [330/375], train_loss: 0.2015\n",
            "0.0001\n",
            "Epoch [331/375], train_loss: 0.2017\n",
            "0.0001\n",
            "Epoch [332/375], train_loss: 0.2016\n",
            "0.0001\n",
            "Epoch [333/375], train_loss: 0.2016\n",
            "0.0001\n",
            "Epoch [334/375], train_loss: 0.2011\n",
            "0.0001\n",
            "Epoch [335/375], train_loss: 0.2011\n",
            "0.0001\n",
            "Epoch [336/375], train_loss: 0.2013\n",
            "0.0001\n",
            "Epoch [337/375], train_loss: 0.2008\n",
            "0.0001\n",
            "Epoch [338/375], train_loss: 0.2010\n",
            "0.0001\n",
            "Epoch [339/375], train_loss: 0.2007\n",
            "0.0001\n",
            "Epoch [340/375], train_loss: 0.2007\n",
            "0.0001\n",
            "Epoch [341/375], train_loss: 0.2005\n",
            "0.0001\n",
            "Epoch [342/375], train_loss: 0.2004\n",
            "0.0001\n",
            "Epoch [343/375], train_loss: 0.2004\n",
            "0.0001\n",
            "Epoch [344/375], train_loss: 0.2006\n",
            "0.0001\n",
            "Epoch [345/375], train_loss: 0.2005\n",
            "0.0001\n",
            "Epoch [346/375], train_loss: 0.1999\n",
            "0.0001\n",
            "Epoch [347/375], train_loss: 0.1999\n",
            "0.0001\n",
            "Epoch [348/375], train_loss: 0.2003\n",
            "0.0001\n",
            "Epoch [349/375], train_loss: 0.2003\n",
            "0.0001\n",
            "Epoch [350/375], train_loss: 0.2002\n",
            "1e-05\n",
            "Epoch [351/375], train_loss: 0.1993\n",
            "1e-05\n",
            "Epoch [352/375], train_loss: 0.1994\n",
            "1e-05\n",
            "Epoch [353/375], train_loss: 0.1994\n",
            "1e-05\n",
            "Epoch [354/375], train_loss: 0.1990\n",
            "1e-05\n",
            "Epoch [355/375], train_loss: 0.1991\n",
            "1e-05\n",
            "Epoch [356/375], train_loss: 0.1994\n",
            "1e-05\n",
            "Epoch [357/375], train_loss: 0.1994\n",
            "1e-05\n",
            "Epoch [358/375], train_loss: 0.1993\n",
            "1e-05\n",
            "Epoch [359/375], train_loss: 0.1994\n",
            "1e-05\n",
            "Epoch [360/375], train_loss: 0.1994\n",
            "1e-05\n",
            "Epoch [361/375], train_loss: 0.1994\n",
            "1e-05\n",
            "Epoch [362/375], train_loss: 0.1994\n",
            "1e-05\n",
            "Epoch [363/375], train_loss: 0.1993\n",
            "1e-05\n",
            "Epoch [364/375], train_loss: 0.1992\n",
            "1e-05\n",
            "Epoch [365/375], train_loss: 0.1992\n",
            "1e-05\n",
            "Epoch [366/375], train_loss: 0.1991\n",
            "1e-05\n",
            "Epoch [367/375], train_loss: 0.1992\n",
            "1e-05\n",
            "Epoch [368/375], train_loss: 0.1996\n",
            "1e-05\n",
            "Epoch [369/375], train_loss: 0.1992\n",
            "1e-05\n",
            "Epoch [370/375], train_loss: 0.1991\n",
            "1e-05\n",
            "Epoch [371/375], train_loss: 0.1992\n",
            "1e-05\n",
            "Epoch [372/375], train_loss: 0.1992\n",
            "1e-05\n",
            "Epoch [373/375], train_loss: 0.1989\n",
            "1e-05\n",
            "Epoch [374/375], train_loss: 0.1990\n",
            "1e-05\n",
            "Epoch [375/375], train_loss: 0.1990\n",
            "elapsed_time: 1897.896s\n",
            "Finished Training\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lsa9Wiv-OUfq",
        "outputId": "dacf1c90-0603-428d-9649-87ba25602d1e"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#save the model\n",
        "torch.save(model.state_dict(), '/content/drive/MyDrive/HDFS/LSTM_model_parameter')"
      ],
      "metadata": {
        "id": "qVHEFEB4NqVS"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# upload the model\n",
        "model_path = '/content/drive/MyDrive/LSTM_model_parameter'\n",
        "model.load_state_dict(torch.load(model_path))"
      ],
      "metadata": {
        "id": "pA_FNHo7ORnT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate(name):\n",
        "    window_size = 10\n",
        "    hdfs = {} #store the unique sequences and their counts.\n",
        "    length = 0\n",
        "    with open('/content/' + name, 'r') as f:\n",
        "        for ln in f.readlines():\n",
        "            ln = [(int(i)-1) for i in ln.strip().split()]\n",
        "            ln = ln + [-1] * (window_size + 1 - len(ln))     #ensure that all sequences have a fixed length of window_size + 1, even if the original line had fewer elements.\n",
        "            hdfs[tuple(ln)] = hdfs.get(tuple(ln), 0) + 1   #If the tuple is not present in the dictionary, hdfs.get(tuple(ln), 0) returns 0, and the code initializes the count to 1.\n",
        "            length += 1\n",
        "    print('Number of sessions({}): {}'.format(name, len(hdfs)))\n",
        "    return hdfs, length"
      ],
      "metadata": {
        "id": "ntfFr0QmwD6x"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "test_normal_loader, test_normal_length = generate('hdfs_test_normal')\n",
        "test_abnormal_loader, test_abnormal_length = generate('hdfs_test_abnormal')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6NnZp1dT75jI",
        "outputId": "a9b6c899-ba86-4f9d-deff-2f15981a8a9f"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of sessions(hdfs_test_normal): 14177\n",
            "Number of sessions(hdfs_test_abnormal): 4123\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in test_normal_loader.keys():\n",
        "  print(i)\n",
        "  break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bsaf82Bqz1qK",
        "outputId": "6d6be1a7-a98e-4921-8201-111a5e1930f2"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(4, 4, 4, 21, 10, 8, 10, 8, 10, 8, 25, 25, 25, 22, 22, 22, 20, 20, 20)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_candidates = 9 # on paper is g , top-g(here top 9) probabilities to appear next are considered normal"
      ],
      "metadata": {
        "id": "dSDO2VmH8dRF"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the model\n",
        "model.eval()\n",
        "\n",
        "TP = 0\n",
        "FP = 0\n",
        "\n",
        "start_time = time.time()\n",
        "with torch.no_grad():\n",
        "    for line in test_normal_loader.keys():\n",
        "        for i in range(len(line) - window_size):\n",
        "            seq = line[i:i + window_size]\n",
        "            label = line[i + window_size]\n",
        "            seq = torch.tensor(seq, dtype=torch.float).view(-1, window_size, input_size).to(device)\n",
        "            label = torch.tensor(label).view(-1).to(device)\n",
        "            output = model(seq)\n",
        "            predicted = torch.argsort(output, 1)[0][-num_candidates:]\n",
        "            if label not in predicted:\n",
        "                FP += test_normal_loader[line] # numbers of that set we have\n",
        "                break   #with just one wrong prediction in a line , we assume , abnormal\n",
        "with torch.no_grad():\n",
        "    for line in test_abnormal_loader.keys():\n",
        "        for i in range(len(line) - window_size):\n",
        "            seq = line[i:i + window_size]\n",
        "            label = line[i + window_size]\n",
        "            seq = torch.tensor(seq, dtype=torch.float).view(-1, window_size, input_size).to(device)\n",
        "            label = torch.tensor(label).view(-1).to(device)\n",
        "            output = model(seq)\n",
        "            predicted = torch.argsort(output, 1)[0][-num_candidates:]\n",
        "            if label not in predicted:\n",
        "                TP += test_abnormal_loader[line]\n",
        "                break\n",
        "elapsed_time = time.time() - start_time\n",
        "print('elapsed_time: {:.3f}s'.format(elapsed_time))\n",
        "# Compute precision, recall and F1-measure\n",
        "FN = test_abnormal_length - TP\n",
        "P = 100 * TP / (TP + FP)\n",
        "R = 100 * TP / (TP + FN)\n",
        "F1 = 2 * P * R / (P + R)\n",
        "print('false positive (FP): {}, false negative (FN): {}, Precision: {:.3f}%, Recall: {:.3f}%, F1-measure: {:.3f}%'.format(FP, FN, P, R, F1))\n",
        "print('Finished Predicting')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZIDRTh2-8ASi",
        "outputId": "b729cf78-5247-41b7-93e3-90178d94bec9"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "elapsed_time: 101.217s\n",
            "false positive (FP): 903, false negative (FN): 254, Precision: 94.836%, Recall: 98.492%, F1-measure: 96.629%\n",
            "Finished Predicting\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(TP)\n",
        "print(FN)\n",
        "print(FP)\n",
        "print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MwoQNRr0dfJm",
        "outputId": "bb9183d6-babf-452f-bd3e-23a5269c014b"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "16584\n",
            "254\n",
            "903\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "J1VFDfINdlKy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}