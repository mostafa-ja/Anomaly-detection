{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOakDnEgOVl9PFBrcYByi/F",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mostafa-ja/Anomaly-detection/blob/main/semantic_vector4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Pretrained sentence-transformers models](https://www.sbert.net/docs/pretrained_models.html)\n",
        "\n",
        "[Our chosen light model in hugging face](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2)\n",
        "\n",
        "[ Convert a collection of raw documents to a matrix of TF-IDF features ](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\n",
        ")"
      ],
      "metadata": {
        "id": "hVl38Zow856S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U sentence-transformers\n",
        "!pip install stop_words"
      ],
      "metadata": {
        "id": "Sboow_Yr6mP-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V6W7INZP6Vr0"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sentence_transformers import SentenceTransformer, util  #util for importing cosine similarity\n",
        "import numpy as np\n",
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download(\"wordnet\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Read log templates file into a DataFrame\n",
        "df = pd.read_csv('/content/HDFS_templates.csv')\n",
        "df.head(3)"
      ],
      "metadata": {
        "id": "k2PeSUbxNaYv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_template(text):\n",
        "  \"\"\"\n",
        "  Normalize text to extract most salient tokens\n",
        "  \"\"\"\n",
        "  # replace special characters with space and remove digits\n",
        "  text = re.sub(r'\\W+', ' ', text) # replaces one or more non-alphanumeric characters (\\W+) with a single space in the text\n",
        "  text = re.sub('\\d', '', text)    #  replaces any digit (\\d) with an empty string in the text\n",
        "\n",
        "\n",
        "  word_tokens = word_tokenize(text)\n",
        "\n",
        "  # converts the words in word_tokens to lower case and then checks whether\n",
        "  #they are present in stop_words or not\n",
        "  lemmatizer = WordNetLemmatizer()\n",
        "  normalized_tokens = [lemmatizer.lemmatize(w.lower(), pos='v') for w in word_tokens if w not in stop_words]\n",
        "\n",
        "  # Reconstruct the sentence\n",
        "  filtered_sentence = ' '.join(normalized_tokens) # our output will be sentences not a list of words\n",
        "  return filtered_sentence"
      ],
      "metadata": {
        "id": "iPnZBeeLNxFr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_sent = '<*>BLOCK* NameSystem<*>addStoredBlock: addStoredBlock request received for<*>on<*>size<*>But it does not belong to any file remove  removing removed. '\n",
        "normalize_template(example_sent)"
      ],
      "metadata": {
        "id": "pJxK8sTSN20M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "normalized_templates = [normalize_template(sentence) for sentence in df['EventTemplate'] ]\n",
        "print(normalized_templates)"
      ],
      "metadata": {
        "id": "M5R6g26UON5A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "normalized_templates = []\n",
        "for sentence in df['EventTemplate']:\n",
        "  normalized_templates.append(normalize_template(sentence))\n",
        "\n",
        "print(normalized_templates)"
      ],
      "metadata": {
        "id": "RZMSPX15OAoo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create TF-IDF vectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Compute TF-IDF features\n",
        "tfidf_features = tfidf_vectorizer.fit_transform(normalized_templates)\n",
        "\n",
        "# Access the TF-IDF feature matrix\n",
        "print(tfidf_features.toarray().shape)"
      ],
      "metadata": {
        "id": "XtpbumNLOdrK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf_vectorizer.vocabulary_"
      ],
      "metadata": {
        "id": "iMEgUUTbOemw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}