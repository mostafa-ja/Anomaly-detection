{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM6jbRuSWEkePnLDLtKaAvn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mostafa-ja/Anomaly-detection/blob/main/B(vectors).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "PART 1: semantic vectors"
      ],
      "metadata": {
        "id": "WJyYifF-r6GA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "7Was4MPLrWiS"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import numpy as np\n",
        "import re\n",
        "import pandas as pd\n",
        "import json\n",
        "import gensim.downloader\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(list(gensim.downloader.info()['models'].keys()))\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sOR117KpsvEC",
        "outputId": "76db7590-93c8-4cde-8124-a79309eaa2eb"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['fasttext-wiki-news-subwords-300', 'conceptnet-numberbatch-17-06-300', 'word2vec-ruscorpora-300', 'word2vec-google-news-300', 'glove-wiki-gigaword-50', 'glove-wiki-gigaword-100', 'glove-wiki-gigaword-200', 'glove-wiki-gigaword-300', 'glove-twitter-25', 'glove-twitter-50', 'glove-twitter-100', 'glove-twitter-200', '__testing_word2vec-matrix-synopsis']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the 'word2vec-google-news-300' embeddings\n",
        "word2vec = gensim.downloader.load('word2vec-google-news-300')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vNdSNKmosqi9",
        "outputId": "247e0326-b5d7-4024-f117-9f40a6c3d1b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[===================================---------------] 70.3% 1168.9/1662.8MB downloaded"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Read log templates file into a DataFrame\n",
        "df = pd.read_csv('/content/HDFS_templates.csv')\n",
        "df.head(3)"
      ],
      "metadata": {
        "id": "ZPSOCzd-uVjJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "templates = df['EventTemplate'].tolist()\n",
        "templates[:3]"
      ],
      "metadata": {
        "id": "YvG_7oXWt8TS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# we keep some stop words such as on, over, not, .. which can have significant meaning\n",
        "stop_words = {\n",
        "    'a', 'about', 'above', 'after', 'again', 'against', 'ain', 'all', 'am', 'an', 'and', 'any', 'are', 'aren',\n",
        "    \"aren't\", 'as', 'at', 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by',\n",
        "    'can', 'couldn', \"couldn't\", 'd', 'did', 'didn', \"didn't\", 'do', 'does', 'doesn', \"doesn't\", 'doing', 'don',\n",
        "    \"don't\", 'during', 'each', 'few', 'for', 'from', 'further', 'had', 'hadn', \"hadn't\", 'has', 'hasn', \"hasn't\",\n",
        "    'have', 'haven', \"haven't\", 'having', 'he', 'her', 'here', 'hers', 'herself', 'him', 'himself', 'his', 'how',\n",
        "    'i', 'if', 'in', 'into', 'is', 'isn', \"isn't\", 'it', \"it's\", 'its', 'itself', 'just', 'll', 'm', 'ma', 'me',\n",
        "    'mightn', \"mightn't\", 'more', 'most', 'mustn', \"mustn't\", 'my', 'myself', 'needn', \"needn't\", 'nor', 'now', 'o',\n",
        "    'of', 'once', 'only', 'or', 'other', 'our', 'ours', 'ourselves', 'out', 'own', 're', 's', 'same', 'shan',\n",
        "    \"shan't\", 'she', \"she's\", 'should', \"should've\", 'shouldn', \"shouldn't\", 'so', 'some', 'such', 't', 'than',\n",
        "    'that', \"that'll\", 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'there', 'these', 'they', 'this',\n",
        "    'those', 'through', 'to', 'too', 'until', 've', 'very', 'was', 'wasn', \"wasn't\", 'we', 'were', 'weren',\n",
        "    \"weren't\", 'what', 'when', 'where', 'which', 'while', 'who', 'whom', 'why', 'will', 'with', 'won', \"won't\",\n",
        "    'wouldn', \"wouldn't\", 'y', 'you', \"you'd\", \"you'll\", \"you're\", \"you've\", 'your', 'yours', 'yourself',\n",
        "    'yourselves'\n",
        "}\n",
        "\n",
        "# Pre-compiling the regular expression pattern using re.compile() can improve the performance of the regular expression operations\n",
        "pattern = re.compile(r'\\W+|\\d')"
      ],
      "metadata": {
        "id": "3axd_hbZt9hA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenized(text):\n",
        "    \"\"\"\n",
        "    Normalize text to extract most salient tokens\n",
        "    \"\"\"\n",
        "    # Replace special characters with space and remove digits\n",
        "    text = pattern.sub(' ', text)\n",
        "\n",
        "    # Convert camel case to snake case, then replace _ with space\n",
        "    text = re.sub('(.)([A-Z][a-z]+)', r'\\1_\\2', text)\n",
        "    text = re.sub('([a-z0-9])([A-Z])', r'\\1_\\2', text).lower().replace('_', ' ')\n",
        "\n",
        "    normalized_tokens = [w for w in text.split() if w not in stop_words]\n",
        "\n",
        "    # Return the filtered sentence, our output will be sentences not a list of words\n",
        "    return ' '.join(normalized_tokens)\n"
      ],
      "metadata": {
        "id": "hOK9roIEuAdb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_template = [tokenized(sentence) for sentence in df['EventTemplate'] ]\n",
        "print(tokenized_template)"
      ],
      "metadata": {
        "id": "b5QR7lO8uC3N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "IMPORTANT POINTS :\n",
        "\n",
        "1 . this model, word2vec_model.get_vector(word) can gives vector for any meaningless word\n",
        "\n",
        "2 . TfidfVectorizer is more commonly used as it combines tokenization and TF-IDF transformation in a single step, making it easier to use for most text tasks. On the other hand, TfidfTransformer is useful when you already have a matrix of term frequencies and want to compute the corresponding TF-IDF matrix. If you have a collection of text documents and want to obtain their TF-IDF representation, it is more straightforward to use TfidfVectorizer.\n",
        "\n",
        "3 . because matrix_weight , are normalized , we dont need for each template, we get the mean of the vectors we sum\n",
        "\n",
        "4 . we use strategy = 'average' in situation we have new template which we havent seen before\n",
        "\n",
        "5 . some points about normalizing word2vec(because of them we dont normalize) :\n",
        "\n",
        "Vectors are normalized to unit length before they are used for similarity calculation, making cosine similarity and dot-product equivalent. Most applications of word embeddings explore not the word vectors themselves, but relations between them to solve, for example, similarity and word relation tasks. For these tasks, it was found that using normalised word vectors improves performance. Word vector length is therefore typically ignored. A word that is consistently used in a similar context will be represented by a longer vector than a word of the same frequency that is used in different contexts(two same meaning words , have same angle but the size depends on ferequency). Not only the direction, but also the length of word vectors carries important information. Word vector length furnishes, in combination with term frequency, a useful measure of word significance.\n",
        "\n",
        "6 . with the methode of tfidf, we find importance of words based on avaible templates not in general"
      ],
      "metadata": {
        "id": "p4zcaE7as3wX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_embeddings(templates, strategy = 'tfidf'):\n",
        "  \"\"\"\n",
        "  Generate embeddings for templates using fasttext\n",
        "  Parameters\n",
        "  ----------\n",
        "  templates: list of templates\n",
        "  strategy: average or tfidf\n",
        "\n",
        "  Returns\n",
        "  -------\n",
        "  embeddings: dict of embeddings\n",
        "  \"\"\"\n",
        "\n",
        "  cleaned_templates = [tokenized(template) for template in templates]\n",
        "\n",
        "  embedding_shape = word2vec.get_vector('word').shape\n",
        "  num_templates = len(cleaned_templates)\n",
        "  embeddings = np.zeros((num_templates, embedding_shape[0]))\n",
        "\n",
        "  if strategy == 'average':\n",
        "    for i, cleaned_template in enumerate(cleaned_templates):\n",
        "      vector = np.zeros(embedding_shape)\n",
        "      for word in cleaned_template.split():\n",
        "              vector += (1/(len(cleaned_template.split()))) * word2vec.get_vector(word)\n",
        "\n",
        "      embeddings[i] = vector\n",
        "\n",
        "  elif strategy == 'tfidf':\n",
        "\n",
        "\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    matrix_weight = vectorizer.fit_transform(cleaned_templates)\n",
        "    dic = vectorizer.vocabulary_\n",
        "\n",
        "    for i, cleaned_template in enumerate(cleaned_templates):\n",
        "        vector = np.zeros(embedding_shape)\n",
        "        for word in cleaned_template.split():\n",
        "            j = dic.get(word)  # If the key is not present, dic.get(word)(or dic.get(word, default_value)) will return None (or any default value you provide), while dic[word] will raise a KeyError if the key is not found.\n",
        "            if j is not None:\n",
        "                vector += matrix_weight[i, j] * word2vec.get_vector(word)\n",
        "\n",
        "        embeddings[i] = vector\n",
        "\n",
        "  return embeddings"
      ],
      "metadata": {
        "id": "bniXqQGFs4u0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# generate semantic vectors from templates\n",
        "embeddings = generate_embeddings(templates, strategy = 'tfidf')\n",
        "embeddings\n",
        ""
      ],
      "metadata": {
        "id": "7fA62IcAtH5L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save semantic vectors\n",
        "with open('/content/embeddings_tfidf.json', 'w') as f:\n",
        "        json.dump(embeddings, f)"
      ],
      "metadata": {
        "id": "zSvaNr1XtLSj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "k59iODZ-tgND"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}