{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMd23hBVpcPMnPnwI6X6dto",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mostafa-ja/Anomaly-detection/blob/main/LogADEmpirical2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generate_embeddings"
      ],
      "metadata": {
        "id": "895kAvTqxe-K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget 'https://dl.fbaipublicfiles.com/fasttext/vectors-english/crawl-300d-2M.vec.zip'\n"
      ],
      "metadata": {
        "id": "yvhQJR1bWqb0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "850b38b6-9480-4517-eabe-40fb0bd60c45"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-07-22 08:39:40--  https://dl.fbaipublicfiles.com/fasttext/vectors-english/crawl-300d-2M.vec.zip\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 18.65.229.46, 18.65.229.89, 18.65.229.121, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|18.65.229.46|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1523785255 (1.4G) [application/zip]\n",
            "Saving to: ‘crawl-300d-2M.vec.zip’\n",
            "\n",
            "crawl-300d-2M.vec.z 100%[===================>]   1.42G  32.1MB/s    in 45s     \n",
            "\n",
            "2023-07-22 08:40:25 (32.6 MB/s) - ‘crawl-300d-2M.vec.zip’ saved [1523785255/1523785255]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip \"/content/crawl-300d-2M.vec.zip\" -d \"/content/\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_CJ8K77WwTQa",
        "outputId": "98ca276d-53b4-482e-db18-c9cce95932d1"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/crawl-300d-2M.vec.zip\n",
            "  inflating: /content/crawl-300d-2M.vec  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import gensim\n",
        "from typing import List\n",
        "from time import time\n",
        "import json"
      ],
      "metadata": {
        "id": "CjYovyDRXMg2"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "template_df = pd.read_csv('/content/HDFS.log_templates.csv')\n",
        "templates = template_df['EventTemplate'].tolist()\n",
        "print(templates[:5])\n",
        "print(len(templates))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WaSJN6y4vAeT",
        "outputId": "e3ad0405-535a-4214-ea5e-54158c172056"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Receiving block <*> src: <*> dest: <*>', 'BLOCK* NameSystem.allocateBlock: <*> <*>', 'PacketResponder <*> for block <*> <*>', 'Received block <*> of size <*> from <*>', 'BLOCK* NameSystem.addStoredBlock: blockMap updated: <*> is added to <*> size <*>']\n",
            "48\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MdNyoLqRzNkK",
        "outputId": "245469c5-55ec-40f4-c363-a203f0ac24a0"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Loading word2vec model...\")\n",
        "st = time()\n",
        "word2vec_model = gensim.models.KeyedVectors.load_word2vec_format('./crawl-300d-2M.vec', binary=False)\n",
        "stop_words = set(stopwords.words('english'))\n",
        "tokenizer = RegexpTokenizer(r'\\w+')\n",
        "print(\"Loaded word2vec model in {:.2f} seconds\".format(time() - st))"
      ],
      "metadata": {
        "id": "RrRP1PGQXNW9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01123829-c5e0-4241-e8f4-d33238e47225"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading word2vec model...\n",
            "Loaded word2vec model in 0.00 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# EXAMPLE\n",
        "import re\n",
        "\n",
        "#The r'\\w+' regular expression matches sequences of alphanumeric characters and underscores. This means that\n",
        "#any word containing only letters, digits, or underscores will be treated as a token,\n",
        "# and all other characters (e.g., punctuation) will be ignored.\n",
        "\n",
        "text = \"The quick:brown_fox_jumps dog@19 #x!sd877 \"\n",
        "matches = re.findall(r'\\w+', text)\n",
        "print(matches)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H78RA-Iax061",
        "outputId": "ab0a7e68-4967-4aed-c30e-e8eafae48e2c"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['The', 'quick', 'brown_fox_jumps', 'dog', '19', 'x', 'sd877']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "template = 'Receiving block NameSystem.allocateBlock PacketResponder'\n",
        "\n",
        "def replace_uppercase_with_space(match):\n",
        "    print(\"x:\", match)       # Print the match object\n",
        "    print(\"x.group():\", match.group())     # Print the matched substring\n",
        "    print(\"x.group(0):\", match.group(0))   # Print the first matched substring (same as match.group())\n",
        "    return \" \" + match.group(0)   # Replace the uppercase letter with a space and the uppercase letter itself\n",
        "\n",
        "# Replace each uppercase letter with a space followed by the uppercase letter\n",
        "template = re.sub('[A-Z]', replace_uppercase_with_space, template)\n",
        "print(\"Result:\", template)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pc1sC8CY0xM1",
        "outputId": "eebcb94a-ab4f-4607-8498-cc3b4aa910f9"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x: <re.Match object; span=(0, 1), match='R'>\n",
            "x.group(): R\n",
            "x.group(0): R\n",
            "x: <re.Match object; span=(16, 17), match='N'>\n",
            "x.group(): N\n",
            "x.group(0): N\n",
            "x: <re.Match object; span=(20, 21), match='S'>\n",
            "x.group(): S\n",
            "x.group(0): S\n",
            "x: <re.Match object; span=(35, 36), match='B'>\n",
            "x.group(): B\n",
            "x.group(0): B\n",
            "x: <re.Match object; span=(41, 42), match='P'>\n",
            "x.group(): P\n",
            "x.group(0): P\n",
            "x: <re.Match object; span=(47, 48), match='R'>\n",
            "x.group(): R\n",
            "x.group(0): R\n",
            "Result:  Receiving block  Name System.allocate Block  Packet Responder\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "template = 'Receiving block NameSystem.allocateBlock PacketResponder  '\n",
        "template = re.sub('[A-Z]', lambda x: \" \" + x.group(0), template)\n",
        "print(template)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LWbIrF3FyuW0",
        "outputId": "b889327a-cc94-4763-e124-573637699513"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Receiving block  Name System.allocate Block  Packet Responder  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "template = 'Receiving-block  Name System.allocate Responder swq '\n",
        "template = tokenizer.tokenize(template)\n",
        "print(template)\n",
        "template_clean = \" \".join(template)\n",
        "print(template_clean)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lN8Zj5C51XoG",
        "outputId": "9643f0ad-5c40-4150-c9ca-b5940129fae6"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Receiving', 'block', 'Name', 'System', 'allocate', 'Responder', 'swq']\n",
            "Receiving block Name System allocate Responder swq\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# remove stop word and  punctuation, split by camel case\n",
        "def clean_template(template: str, remove_stop_words: bool = True):\n",
        "    template = \" \".join([word.lower() if word.isupper() else word for word in template.strip().split()])\n",
        "\n",
        "    # camel case: The purpose of this regular expression substitution is to split words in camel case notation by inserting spaces before each uppercase letter.\n",
        "    template = re.sub('[A-Z]', lambda x: \" \" + x.group(0), template)\n",
        "\n",
        "    word_tokens = tokenizer.tokenize(template)  # tokenize\n",
        "    word_tokens = [w for w in word_tokens if not w.isdigit()]  # remove digital\n",
        "\n",
        "    if remove_stop_words:  # remove stop words, we can close this function\n",
        "        filtered_sentence = [w.lower() for w in word_tokens if w not in stop_words]\n",
        "    else:\n",
        "        filtered_sentence = [w.lower() for w in word_tokens]\n",
        "\n",
        "    template_clean = \" \".join(filtered_sentence)\n",
        "    return template_clean  # return string\n"
      ],
      "metadata": {
        "id": "a1XSJ7eJZfWl"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def log_key2vec(log_template: str, weight: List[float] = None):\n",
        "    \"\"\"\n",
        "    Get word vec of words in log key, using weight\n",
        "    Parameters\n",
        "    ----------\n",
        "    log_template\n",
        "    weight\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    log_template_vec: list of word vec\n",
        "    \"\"\"\n",
        "\n",
        "    #The strip() function removes any leading or trailing whitespace from the log template\n",
        "    words = log_template.strip().split()\n",
        "    log_template_vec = []\n",
        "\n",
        "    if not weight:  # if not weight, uniform weight\n",
        "        weight = [1] * len(words)\n",
        "\n",
        "    for index, word in enumerate(words):\n",
        "        try:  # catch the exception when word not in pre-trained word vector dictionary\n",
        "            log_template_vec.append(word2vec_model[word] * weight[index])\n",
        "        except Exception as _:\n",
        "            pass\n",
        "    if len(log_template_vec) == 0: #in the condition , we dont have any meaningful word(vector) in the template\n",
        "        log_template_vec = np.zeros(300)\n",
        "    return log_template_vec"
      ],
      "metadata": {
        "id": "NcHMjNZNkiVr"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# If an exception occurs (i.e., the word is not present in the pre-trained word vector dictionary),\n",
        "# the code will not raise an error but simply continue to the next word in the loop.\n",
        "\n",
        "try:\n",
        "  word2vec_model['swq']\n",
        "except Exception as e: # if we use _ instead of e , we throw away variable\n",
        "  print(e)\n",
        "  print('error')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3lgexXu-3FWR",
        "outputId": "5bd5aa5c-d580-410c-bb9f-34b5dd18aa37"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\"Key 'swq' not present\"\n",
            "error\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "  word2vec_model['swq']\n",
        "except Exception as _: # if we use _ instead of e , we throw away variable\n",
        "  print('error')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2jiIz-8e4UgA",
        "outputId": "9585f30b-6f8c-4589-add8-838491346cc4"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "error\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "template_clean = 'Receiving block Name System allocate Responder swq'\n",
        "print(len(log_key2vec(template_clean)))\n",
        "print(len(log_key2vec(template_clean)[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rutmrj282cPR",
        "outputId": "0b2ff204-b0db-4318-c3d6-9d7a9305c4bf"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6\n",
            "300\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OzW3lrg84-_D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# be careful, not mix with function clean_template()\n",
        "templates = ['Receiving block <*> src: <*> dest: <*>', 'BLOCK* NameSystem.allocateBlock: <*> <*>', 'PacketResponder <*> for block <*> <*>']\n",
        "cleaned_templates = [clean_template(template) for template in templates]\n",
        "zipp = zip(cleaned_templates, templates)\n",
        "\n",
        "embeddings = {}\n",
        "for cleaned_template, template in zipp:\n",
        "    print(template)\n",
        "    embeddings[template] = np.mean(log_key2vec(cleaned_template), axis=0).tolist()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Od-G8rPn4-03",
        "outputId": "a8ed548f-ec2c-490b-e22c-3b7e95e70ec8"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Receiving block <*> src: <*> dest: <*>\n",
            "BLOCK* NameSystem.allocateBlock: <*> <*>\n",
            "PacketResponder <*> for block <*> <*>\n",
            "Received block <*> of size <*> from <*>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(embeddings['Receiving block <*> src: <*> dest: <*>'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aftXO5PN6FQT",
        "outputId": "1600e259-bb7a-4cbf-fe4c-fe043494c130"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "300"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#It learns the vocabulary from the cleaned_templates and builds the document-term matrix (the word frequency representation of the sentences).\n",
        "#The output will be a 2-dimensional numpy array, where each row represents a cleaned template and each column represents a word in the vocabulary.\n",
        "#The values in the array represent the word frequency\n",
        "\n",
        "templates = ['Receiving block <*> src: <*> dest: <*>', 'BLOCK* NameSystem.allocateBlock: <*> <*>', 'PacketResponder <*> for block <*> <*>','Receiving block PacketResponder']\n",
        "cleaned_templates = [clean_template(template) for template in templates]\n",
        "print(cleaned_templates)\n",
        "vectorizer = CountVectorizer()\n",
        "transformer = TfidfTransformer()\n",
        "X = vectorizer.fit_transform(cleaned_templates)\n",
        "print(vectorizer.vocabulary_)\n",
        "print(X)\n",
        "print(X.toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D3f1QIgA8smV",
        "outputId": "563d3cba-22f2-41c1-eeb9-b055a2e27d1c"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['receiving block src dest', 'block name system allocate block', 'packet responder block', 'receiving block packet responder']\n",
            "{'receiving': 5, 'block': 1, 'src': 7, 'dest': 2, 'name': 3, 'system': 8, 'allocate': 0, 'packet': 4, 'responder': 6}\n",
            "  (0, 5)\t1\n",
            "  (0, 1)\t1\n",
            "  (0, 7)\t1\n",
            "  (0, 2)\t1\n",
            "  (1, 1)\t2\n",
            "  (1, 3)\t1\n",
            "  (1, 8)\t1\n",
            "  (1, 0)\t1\n",
            "  (2, 1)\t1\n",
            "  (2, 4)\t1\n",
            "  (2, 6)\t1\n",
            "  (3, 5)\t1\n",
            "  (3, 1)\t1\n",
            "  (3, 4)\t1\n",
            "  (3, 6)\t1\n",
            "[[0 1 1 0 0 1 0 1 0]\n",
            " [1 2 0 1 0 0 0 0 1]\n",
            " [0 1 0 0 1 0 1 0 0]\n",
            " [0 1 0 0 1 1 1 0 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "templates = ['Receiving block <*> src: <*> dest: <*>', 'BLOCK* NameSystem.allocateBlock: <*> <*>', 'PacketResponder <*> for block <*> <*>']\n",
        "cleaned_templates = [clean_template(template) for template in templates]\n",
        "\n",
        "vectorizer = CountVectorizer()\n",
        "transformer = TfidfTransformer()\n",
        "X = vectorizer.fit_transform(cleaned_templates)\n",
        "tfidf = transformer.fit_transform(X)\n",
        "tfidf = tfidf.toarray()\n",
        "words = vectorizer.get_feature_names()\n",
        "single_weights = []\n",
        "for i, (template, k) in enumerate(templates):\n",
        "    for word in template.strip().split():\n",
        "        if word in words:\n",
        "            single_weights.append(tfidf[i][words.index(word)])\n",
        "        else:\n",
        "            single_weights.append(0)\n",
        "    embeddings[k] = np.mean(log_key2vec(template, single_weights), axis=0).tolist()"
      ],
      "metadata": {
        "id": "31935suu4_b9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MyX_6pdF8tZv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_embeddings_fasttext(templates: List[str], strategy: str = 'average') -> dict:\n",
        "    \"\"\"\n",
        "    Generate embeddings for templates using fasttext\n",
        "    Parameters\n",
        "    ----------\n",
        "    templates: list of templates\n",
        "    strategy: average or tfidf\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    embeddings: dict of embeddings\n",
        "    \"\"\"\n",
        "    clean_templates = [clean_template(template) for template in templates]\n",
        "    templates = zip(clean_templates, templates)\n",
        "    embeddings = {}\n",
        "    if strategy == 'average':\n",
        "        for template, k in templates:\n",
        "            embeddings[k] = np.mean(log_key2vec(template), axis=0).tolist()\n",
        "    elif strategy == 'tfidf':\n",
        "        vectorizer = CountVectorizer()\n",
        "        transformer = TfidfTransformer()\n",
        "        X = vectorizer.fit_transform(clean_templates)\n",
        "        tfidf = transformer.fit_transform(X)\n",
        "        tfidf = tfidf.toarray()\n",
        "        words = vectorizer.get_feature_names()\n",
        "        single_weights = []\n",
        "        for i, (template, k) in enumerate(templates):\n",
        "            for word in template.strip().split():\n",
        "                if word in words:\n",
        "                    single_weights.append(tfidf[i][words.index(word)])\n",
        "                else:\n",
        "                    single_weights.append(0)\n",
        "            embeddings[k] = np.mean(log_key2vec(template, single_weights), axis=0).tolist()\n",
        "    else:\n",
        "        raise ValueError('Invalid strategy')\n",
        "\n",
        "    return embeddings\n"
      ],
      "metadata": {
        "id": "K50opgHUkowp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}