{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMFC4n4rdwVe9grjd2Q+RmB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mostafa-ja/Anomaly-detection/blob/main/LogADEmpirical3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generate_embeddings"
      ],
      "metadata": {
        "id": "f3HDurKvPM6a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget 'https://dl.fbaipublicfiles.com/fasttext/vectors-english/crawl-300d-2M.vec.zip'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yuotFDM4PMHR",
        "outputId": "06131dd5-54f2-40fa-8351-7a0593c34ec3"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-07-22 15:34:44--  https://dl.fbaipublicfiles.com/fasttext/vectors-english/crawl-300d-2M.vec.zip\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 13.225.142.121, 13.225.142.88, 13.225.142.76, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|13.225.142.121|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1523785255 (1.4G) [application/zip]\n",
            "Saving to: ‘crawl-300d-2M.vec.zip’\n",
            "\n",
            "crawl-300d-2M.vec.z 100%[===================>]   1.42G  40.1MB/s    in 39s     \n",
            "\n",
            "2023-07-22 15:35:24 (37.2 MB/s) - ‘crawl-300d-2M.vec.zip’ saved [1523785255/1523785255]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip \"/content/crawl-300d-2M.vec.zip\" -d \"/content/\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u0l6az2uPSLM",
        "outputId": "c0082e89-0079-4970-ff50-40585f1ae1d3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/crawl-300d-2M.vec.zip\n",
            "  inflating: /content/crawl-300d-2M.vec  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import gensim\n",
        "from typing import List\n",
        "from time import time\n",
        "import json"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SI0Kq8o-PVd7",
        "outputId": "a6878250-d3e8-4118-a7bf-c04cc4adf4fe"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "template_df = pd.read_csv('/content/HDFS.log_templates.csv')\n",
        "templates = template_df['EventTemplate'].tolist()\n",
        "print(templates[:5])\n",
        "print(len(templates))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qHiO5BbhPWSV",
        "outputId": "fb2ed0ff-7d2c-40b2-8178-772cd0f0234a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Receiving block <*> src: <*> dest: <*>', 'BLOCK* NameSystem.allocateBlock: <*> <*>', 'PacketResponder <*> for block <*> <*>', 'Received block <*> of size <*> from <*>', 'BLOCK* NameSystem.addStoredBlock: blockMap updated: <*> is added to <*> size <*>']\n",
            "48\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "st = time()\n",
        "word2vec_model = gensim.models.KeyedVectors.load_word2vec_format('./crawl-300d-2M.vec', binary=False)\n",
        "stop_words = set(stopwords.words('english'))\n",
        "tokenizer = RegexpTokenizer(r'\\w+')\n",
        "print(\"Loaded word2vec model in {:.2f} seconds\".format(time() - st))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B374-3gOPZgs",
        "outputId": "218b909d-9e4b-40e3-c233-90ab3fd69c54"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded word2vec model in 0.01 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# remove stop word and  punctuation, split by camel case\n",
        "def clean_template(template: str, remove_stop_words: bool = True):\n",
        "    template = \" \".join([word.lower() if word.isupper() else word for word in template.strip().split()])\n",
        "    template = re.sub('[A-Z]', lambda x: \" \" + x.group(0), template)  # camel case\n",
        "    word_tokens = tokenizer.tokenize(template)  # tokenize\n",
        "    word_tokens = [w for w in word_tokens if not w.isdigit()]  # remove digital\n",
        "    if remove_stop_words:  # remove stop words, we can close this function\n",
        "        filtered_sentence = [w.lower() for w in word_tokens if w not in stop_words]\n",
        "    else:\n",
        "        filtered_sentence = [w.lower() for w in word_tokens]\n",
        "\n",
        "    template_clean = \" \".join(filtered_sentence)\n",
        "    return template_clean  # return string"
      ],
      "metadata": {
        "id": "YCqKRIaxPw83"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Br2jOh6nWstf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "IMPORTANT POINTS :\n",
        "\n",
        "1 . this model, word2vec_model.get_vector(word) can gives vector for any meaningless word\n",
        "\n",
        "2 . TfidfVectorizer is more commonly used as it combines tokenization and TF-IDF\n",
        "transformation in a single step, making it easier to use for most text\n",
        "tasks. On the other hand, TfidfTransformer is useful when you already have a\n",
        "matrix of term frequencies and want to compute the corresponding TF-IDF matrix.\n",
        "If you have a collection of text documents and want to obtain their TF-IDF\n",
        "representation, it is more straightforward to use TfidfVectorizer.\n",
        "\n",
        "3 . because matrix_weight , are normalized , we dont need for each template, we get the mean of the vectors we sum  \n",
        "\n",
        "4 . we use strategy = 'average' in situation we have new template which we havent seen before\n",
        "\n",
        "5 . some points about normalizing word2vec :\n",
        "- Vectors are normalized to unit length before they are used for similarity calculation, making cosine similarity and dot-product equivalent.\n",
        "- Most applications of word embeddings explore not the word vectors themselves, but relations between them to solve, for example, similarity and word relation tasks. For these tasks, it was found that using normalised word vectors improves performance. Word vector length is therefore typically ignored.\n",
        "- A word that is consistently used in a similar context will be represented by a longer vector than a word of the same frequency that is used in different contexts.\n",
        "Not only the direction, but also the length of word vectors carries important information.\n",
        "Word vector length furnishes, in combination with term frequency, a useful measure of word significance."
      ],
      "metadata": {
        "id": "7-3NVYI-Wu_W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_embeddings_fasttext(templates, strategy = 'average'):\n",
        "  \"\"\"\n",
        "  Generate embeddings for templates using fasttext\n",
        "  Parameters\n",
        "  ----------\n",
        "  templates: list of templates\n",
        "  strategy: average or tfidf\n",
        "\n",
        "  Returns\n",
        "  -------\n",
        "  embeddings: dict of embeddings\n",
        "  \"\"\"\n",
        "\n",
        "  cleaned_templates = [clean_template(template) for template in templates]\n",
        "  embeddings = {}\n",
        "\n",
        "  if strategy == 'average':\n",
        "    for i, (cleaned_template, template) in enumerate(zip(cleaned_templates, templates)):\n",
        "      template2vec = np.zeros(300) #300 = word2vector size\n",
        "      for word in cleaned_template.split():\n",
        "              template2vec += 1/(len(cleaned_template.split())) * word2vec_model.get_vector(word)\n",
        "\n",
        "    embeddings[template] = template2vec.tolist()\n",
        "\n",
        "  elif strategy == 'tfidf':\n",
        "\n",
        "\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    matrix_weight = vectorizer.fit_transform(cleaned_templates)\n",
        "    dic = vectorizer.vocabulary_\n",
        "\n",
        "    for i, (cleaned_template, template) in enumerate(zip(cleaned_templates, templates)):\n",
        "        template2vec = np.zeros(300) #300 = word2vector size\n",
        "        for word in cleaned_template.split():\n",
        "            j = dic.get(word)  # If the key is not present, dic.get(word)(or dic.get(word, default_value)) will return None (or any default value you provide), while dic[word] will raise a KeyError if the key is not found.\n",
        "            if j is not None:\n",
        "                template2vec += matrix_weight[i, j] * word2vec_model.get_vector(word)\n",
        "\n",
        "        embeddings[template] = template2vec.tolist()\n",
        "\n",
        "  return embeddings"
      ],
      "metadata": {
        "id": "tL58fY0xP5XV"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = generate_embeddings_fasttext(templates, strategy='tfidf')\n",
        "with open('/content/embeddings_tfidf.json', 'w') as f:\n",
        "        json.dump(embeddings, f)\n"
      ],
      "metadata": {
        "id": "yz12C0oHYbWX"
      },
      "execution_count": 36,
      "outputs": []
    }
  ]
}