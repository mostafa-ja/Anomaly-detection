{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyO23G25fHBupFSnA6sibvPW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mostafa-ja/Anomaly-detection/blob/main/TCN2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the TCN model and load pre-trained weights if available\n",
        "input_size = 1  # Assuming each log template is represented by a single number\n",
        "output_size = 28  # Number of classes in the one-hot encodings\n",
        "num_channels = [16, 32]\n",
        "kernel_size = 5\n",
        "dropout = 0.2"
      ],
      "metadata": {
        "id": "fZrefvn1Ano6"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_blocks = len(num_channels)  # Number of blocks in the TCN\n",
        "dilations = [2 ** i for i in range(num_blocks)]  # [1, 2, 4] as before\n",
        "\n",
        "receptive_field_per_layer = (kernel_size - 1) * (max(dilations) - 1) + 1\n",
        "\n",
        "total_receptive_field = receptive_field_per_layer * num_blocks\n",
        "\n",
        "print(f\"Total Receptive Field of TCN (with each block consisting of two layers): {total_receptive_field}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MKkz0lF0Fb9Y",
        "outputId": "a2e3445c-fe06-4f08-eae7-959058cb6d50"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Receptive Field of TCN (with each block consisting of two layers): 10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# download datasets\n",
        "!wget 'https://raw.githubusercontent.com/donglee-afar/logdeep/master/data/hdfs/hdfs_train'\n",
        "!wget 'https://raw.githubusercontent.com/donglee-afar/logdeep/master/data/hdfs/hdfs_test_normal'\n",
        "!wget 'https://raw.githubusercontent.com/donglee-afar/logdeep/master/data/hdfs/hdfs_test_abnormal'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o4Rwy8JxsY3q",
        "outputId": "6a5f5bbd-3df9-4ab1-f7bd-8720a58cb91e"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-07-29 15:18:38--  https://raw.githubusercontent.com/donglee-afar/logdeep/master/data/hdfs/hdfs_train\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.109.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 257875 (252K) [text/plain]\n",
            "Saving to: ‘hdfs_train.1’\n",
            "\n",
            "hdfs_train.1        100%[===================>] 251.83K  --.-KB/s    in 0.03s   \n",
            "\n",
            "2023-07-29 15:18:38 (8.04 MB/s) - ‘hdfs_train.1’ saved [257875/257875]\n",
            "\n",
            "--2023-07-29 15:18:38--  https://raw.githubusercontent.com/donglee-afar/logdeep/master/data/hdfs/hdfs_test_normal\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 29284282 (28M) [text/plain]\n",
            "Saving to: ‘hdfs_test_normal.1’\n",
            "\n",
            "hdfs_test_normal.1  100%[===================>]  27.93M   177MB/s    in 0.2s    \n",
            "\n",
            "2023-07-29 15:18:38 (177 MB/s) - ‘hdfs_test_normal.1’ saved [29284282/29284282]\n",
            "\n",
            "--2023-07-29 15:18:38--  https://raw.githubusercontent.com/donglee-afar/logdeep/master/data/hdfs/hdfs_test_abnormal\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 782479 (764K) [text/plain]\n",
            "Saving to: ‘hdfs_test_abnormal.1’\n",
            "\n",
            "hdfs_test_abnormal. 100%[===================>] 764.14K  --.-KB/s    in 0.04s   \n",
            "\n",
            "2023-07-29 15:18:38 (16.9 MB/s) - ‘hdfs_test_abnormal.1’ saved [782479/782479]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn.utils import weight_norm\n",
        "import time\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "-yTpujeh70P8"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Chomp1d(nn.Module):\n",
        "    def __init__(self, chomp_size):\n",
        "        \"\"\"\n",
        "        Removes the extra elements at the end of the input tensor along the time dimension.\n",
        "\n",
        "        Args:\n",
        "        - chomp_size (int): The number of elements to be removed from the end of each input tensor.\n",
        "        \"\"\"\n",
        "        super(Chomp1d, self).__init__()\n",
        "        self.chomp_size = chomp_size\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass of Chomp1d.\n",
        "\n",
        "        Args:\n",
        "        - x (torch.Tensor): Input tensor with dimension (N, C, L). (batch size, number of input features(channels), input sequence length)\n",
        "\n",
        "        Returns:\n",
        "        - torch.Tensor: Output tensor with dimension (N, C, L - chomp_size).\n",
        "        \"\"\"\n",
        "        return x[:, :, :-self.chomp_size].contiguous()\n",
        "\n",
        "\n",
        "class TemporalBlock(nn.Module):\n",
        "    def __init__(self, n_inputs, n_outputs, kernel_size, stride, dilation, padding, dropout=0.2):\n",
        "        \"\"\"\n",
        "        A single temporal block in the Temporal Convolutional Network.\n",
        "\n",
        "        Args:\n",
        "        - n_inputs (int): Number of input channels.\n",
        "        - n_outputs (int): Number of output channels.\n",
        "        - kernel_size (int): Size of the convolutional kernels.\n",
        "        - stride (int): Stride for the convolutional layers.\n",
        "        - dilation (int): Dilation rate for the convolutional layers.\n",
        "        - padding (int): Padding size for the convolutional layers.\n",
        "        - dropout (float): Dropout rate to be applied after each convolutional layer.\n",
        "        \"\"\"\n",
        "        super(TemporalBlock, self).__init__()\n",
        "        self.conv1 = weight_norm(nn.Conv1d(n_inputs, n_outputs, kernel_size,\n",
        "                                           stride=stride, padding=padding, dilation=dilation))\n",
        "        self.chomp1 = Chomp1d(padding)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "\n",
        "        self.conv2 = weight_norm(nn.Conv1d(n_outputs, n_outputs, kernel_size,\n",
        "                                           stride=stride, padding=padding, dilation=dilation))\n",
        "        self.chomp2 = Chomp1d(padding)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "\n",
        "        self.net = nn.Sequential(self.conv1, self.chomp1, self.relu1, self.dropout1,\n",
        "                                 self.conv2, self.chomp2, self.relu2, self.dropout2)\n",
        "\n",
        "        # The downsample variable stores a 1x1 convolutional layer\n",
        "        # (if n_inputs is not equal to n_outputs) to match the input and output dimensions of the block. Otherwise, it is set to None.\n",
        "        #IMPORTANT : The purpose of self.downsample is to adjust the dimensions of the input tensor so that it can be added element-wise to the output of the\n",
        "        #temporal block,This addition is part of the residual connection that helps with training deeper networks and improves gradient flow during\n",
        "        # backpropagation. CAREFUL, because here we have kernel size one, we dont need  Chomp1d() to equlize their size\n",
        "        self.downsample = nn.Conv1d(n_inputs, n_outputs, 1) if n_inputs != n_outputs else None\n",
        "\n",
        "        self.relu = nn.ReLU()\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        \"\"\"\n",
        "        Initialize the weights of the convolutional layers with a normal distribution (mean=0, std=0.01).\n",
        "        \"\"\"\n",
        "        self.conv1.weight.data.normal_(0, 0.01)\n",
        "        self.conv2.weight.data.normal_(0, 0.01)\n",
        "        if self.downsample is not None:\n",
        "            self.downsample.weight.data.normal_(0, 0.01)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass of TemporalBlock.\n",
        "\n",
        "        Args:\n",
        "        - x (torch.Tensor): Input tensor with dimension (N, C, L).\n",
        "\n",
        "        Returns:\n",
        "        - torch.Tensor: Output tensor with dimension (N, C, L).\n",
        "        \"\"\"\n",
        "        out = self.net(x)\n",
        "        res = x if self.downsample is None else self.downsample(x)\n",
        "        return self.relu(out + res)\n",
        "\n",
        "\n",
        "class TemporalConvNet(nn.Module):\n",
        "    def __init__(self, num_inputs, num_channels, kernel_size=2, dropout=0.2):\n",
        "        \"\"\"\n",
        "        Temporal Convolutional Network (TCN) for sequence processing.\n",
        "        This class represents the Temporal Convolutional Network (TCN) architecture that contains multiple temporal blocks stacked together.\n",
        "\n",
        "        Args:\n",
        "        - num_inputs (int): Number of input channels.\n",
        "        - num_channels (list of int): Number of channels for each temporal block.\n",
        "        - kernel_size (int): Size of the convolutional kernels in the temporal blocks.\n",
        "        - dropout (float): Dropout rate to be applied after each temporal block.\n",
        "        \"\"\"\n",
        "        super(TemporalConvNet, self).__init__()\n",
        "        layers = []\n",
        "        num_levels = len(num_channels)\n",
        "        for i in range(num_levels):\n",
        "            dilation_size = 2 ** i\n",
        "            #The input channels for each block are determined based on the number of input channels (num_inputs) for the first block,\n",
        "            #and for subsequent blocks, the number of input channels is set to the number of output channels of the previous block (num_channels[i-1]).\n",
        "            in_channels = num_inputs if i == 0 else num_channels[i - 1]\n",
        "            out_channels = num_channels[i]\n",
        "\n",
        "            layers += [TemporalBlock(in_channels, out_channels, kernel_size, stride=1, dilation=dilation_size,\n",
        "                                     padding=(kernel_size - 1) * dilation_size, dropout=dropout)]\n",
        "\n",
        "        self.network = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass of TemporalConvNet.\n",
        "\n",
        "        Args:\n",
        "        - x (torch.Tensor): Input tensor with dimension (N, C, L).\n",
        "\n",
        "        Returns:\n",
        "        - torch.Tensor: Output tensor with dimension (N, C, L).\n",
        "        \"\"\"\n",
        "        return self.network(x)\n",
        "\n",
        "\n",
        "\n",
        "class TCN(nn.Module):\n",
        "    def __init__(self, input_size, output_size, num_channels, kernel_size, dropout):\n",
        "        \"\"\"\n",
        "        Temporal Convolutional Network (TCN) for sequence classification.\n",
        "\n",
        "        Args:\n",
        "        - input_size (int): The number of input features (C_in).\n",
        "        - output_size (int): The number of classes for classification.\n",
        "        - num_channels (list of int): Number of channels for each TCN layer.\n",
        "        - kernel_size (int): The size of the convolutional kernels in TCN layers.\n",
        "        - dropout (float): The dropout rate to be applied after each TCN layer.\n",
        "        \"\"\"\n",
        "        super(TCN, self).__init__()\n",
        "        self.tcn = TemporalConvNet(input_size, num_channels, kernel_size=kernel_size, dropout=dropout)\n",
        "        self.linear = nn.Linear(num_channels[-1], output_size)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        \"\"\"\n",
        "        Forward pass of the TCN model.\n",
        "\n",
        "        Args:\n",
        "        - inputs (torch.Tensor): Input tensor with dimension (N, C_in, L_in).(batch size, number of input features(channels), input sequence length)\n",
        "\n",
        "        Returns:\n",
        "        - torch.Tensor: The model's output tensor with dimension (N, output_size).\n",
        "        \"\"\"\n",
        "        # Input should have dimension (N, C_in, L_in)\n",
        "        y = self.tcn(inputs)\n",
        "        o = self.linear(y[:, :, -1])\n",
        "        return o\n"
      ],
      "metadata": {
        "id": "OLzRjIugabSh"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AttentionBlock(nn.Module):\n",
        "  \"\"\"An attention mechanism similar to Vaswani et al (2017)\n",
        "  The input of the AttentionBlock is `BxTxD` where `B` is the input\n",
        "  minibatch size, `T` is the length of the sequence `D` is the dimensions of\n",
        "  each feature.\n",
        "  The output of the AttentionBlock is `BxTx(D+V)` where `V` is the size of the\n",
        "  attention values.\n",
        "  Arguments:\n",
        "      dims (int): the number of dimensions (or channels) of each element in\n",
        "          the input sequence\n",
        "      k_size (int): the size of the attention keys\n",
        "      v_size (int): the size of the attention values\n",
        "      seq_len (int): the length of the input and output sequences\n",
        "  \"\"\"\n",
        "  def __init__(self, dims, k_size, v_size, seq_len=None):\n",
        "    super(AttentionBlock, self).__init__()\n",
        "    self.key_layer = nn.Linear(dims, k_size)\n",
        "    self.query_layer = nn.Linear(dims, k_size)\n",
        "    self.value_layer = nn.Linear(dims, v_size)\n",
        "    self.sqrt_k = math.sqrt(k_size)\n",
        "\n",
        "  def forward(self, minibatch):\n",
        "    keys = self.key_layer(minibatch)\n",
        "    queries = self.query_layer(minibatch)\n",
        "    values = self.value_layer(minibatch)\n",
        "    logits = torch.bmm(queries, keys.transpose(2,1))\n",
        "    # Use numpy triu because you can't do 3D triu with PyTorch\n",
        "    # TODO: using float32 here might break for non FloatTensor inputs.\n",
        "    # Should update this later to use numpy/PyTorch types of the input.\n",
        "    mask = np.triu(np.ones(logits.size()), k=1).astype('uint8')\n",
        "    mask = torch.from_numpy(mask).cuda()\n",
        "    # do masked_fill_ on data rather than Variable because PyTorch doesn't\n",
        "    # support masked_fill_ w/-inf directly on Variables for some reason.\n",
        "    logits.data.masked_fill_(mask, float('-inf'))\n",
        "    probs = F.softmax(logits, dim=1) / self.sqrt_k\n",
        "    read = torch.bmm(probs, values)\n",
        "    return minibatch + read"
      ],
      "metadata": {
        "id": "BHSmY-ZUXR9i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class ConvBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride, padding):\n",
        "        super(ConvBlock, self).__init__()\n",
        "        self.conv1d = nn.Conv1d(in_channels, out_channels, kernel_size, stride=stride, padding=padding)\n",
        "        self.max_pool = nn.MaxPool1d(kernel_size=2)  # Assuming max-pooling with kernel_size=2\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1d(x)\n",
        "        x = self.max_pool(x)\n",
        "        x = self.relu(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class AttentionBlock(nn.Module):\n",
        "  \"\"\"An attention mechanism similar to Vaswani et al (2017)\n",
        "  The input of the AttentionBlock is `BxTxD` where `B` is the input\n",
        "  minibatch size, `T` is the length of the sequence `D` is the dimensions of\n",
        "  each feature.\n",
        "  The output of the AttentionBlock is `BxTx(D+V)` where `V` is the size of the\n",
        "  attention values.\n",
        "  Arguments:\n",
        "      dims (int): the number of dimensions (or channels) of each element in\n",
        "          the input sequence\n",
        "      k_size (int): the size of the attention keys\n",
        "      v_size (int): the size of the attention values\n",
        "      seq_len (int): the length of the input and output sequences\n",
        "  \"\"\"\n",
        "  def __init__(self, dims, k_size, v_size, seq_len=None):\n",
        "    super(AttentionBlock, self).__init__()\n",
        "    self.key_layer = nn.Linear(dims, k_size)\n",
        "    self.query_layer = nn.Linear(dims, k_size)\n",
        "    self.value_layer = nn.Linear(dims, v_size)\n",
        "    self.sqrt_k = math.sqrt(k_size)\n",
        "\n",
        "  def forward(self, minibatch):\n",
        "    keys = self.key_layer(minibatch)\n",
        "    queries = self.query_layer(minibatch)\n",
        "    values = self.value_layer(minibatch)\n",
        "    logits = torch.bmm(queries, keys.transpose(2,1))\n",
        "    # Use numpy triu because you can't do 3D triu with PyTorch\n",
        "    # TODO: using float32 here might break for non FloatTensor inputs.\n",
        "    # Should update this later to use numpy/PyTorch types of the input.\n",
        "    mask = np.triu(np.ones(logits.size()), k=1).astype('uint8')\n",
        "    mask = torch.from_numpy(mask).cuda()\n",
        "    # do masked_fill_ on data rather than Variable because PyTorch doesn't\n",
        "    # support masked_fill_ w/-inf directly on Variables for some reason.\n",
        "    logits.data.masked_fill_(mask, float('-inf'))\n",
        "    probs = F.softmax(logits, dim=1) / self.sqrt_k\n",
        "    read = torch.bmm(probs, values)\n",
        "    return minibatch + read\n",
        "\n",
        "\n",
        "class ACNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size, kernel_size, stride, padding):\n",
        "        super(ACNN, self).__init__()\n",
        "        self.conv_block1 = ConvBlock(input_size, hidden_size, kernel_size, stride, padding)\n",
        "        self.conv_block2 = ConvBlock(hidden_size, hidden_size, kernel_size, stride, padding)\n",
        "        self.attention_layer = AttentionLayer(hidden_size)\n",
        "        self.linear = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv_block1(x)\n",
        "        x = self.conv_block2(x)\n",
        "        x = self.attention_layer(x)\n",
        "        x = torch.sum(x, dim=2)  # Sum along the time dimension\n",
        "        x = self.linear(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "7kwhJWikXTRf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CMd3M79MXTOX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Hlng-bR0XTLc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NF-lWsh9XTIY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ICeU1en0XTE7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rRE1ROeOXTAT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "name = 'hdfs_train'\n",
        "window_size = 10\n",
        "num_sessions = 0\n",
        "inputs = []\n",
        "outputs = []\n",
        "\n",
        "with open('/content/' + name, 'r') as f:\n",
        "        for row in f:\n",
        "            num_sessions += 1\n",
        "            line = [ (int(i)-1) for i in row.strip().split()]\n",
        "            for i in range(len(line) - window_size):\n",
        "                inputs.append(line[i:i + window_size])\n",
        "                outputs.append(line[i + window_size])\n",
        "\n",
        "print('Number of sessions({}): {}'.format(name, num_sessions))\n",
        "print('Number of seqs({}): {}'.format(name, len(inputs)))\n",
        "dataset = TensorDataset(torch.tensor(inputs, dtype=torch.float), torch.tensor(outputs))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J3W3rU_msfoV",
        "outputId": "a63c44db-8eb8-44c8-92fc-f4fe3b782a58"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of sessions(hdfs_train): 4855\n",
            "Number of seqs(hdfs_train): 46575\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the TCN model and load pre-trained weights if available\n",
        "input_size = 1  # Assuming each log template is represented by a single number\n",
        "output_size = 28  # Number of classes in the one-hot encodings\n",
        "num_channels = [16, 32, 64]\n",
        "kernel_size = 3\n",
        "dropout = 0.2\n",
        "batch_size = 2048\n",
        "num_epochs = 400\n",
        "\n",
        "model = TCN(input_size, output_size, num_channels, kernel_size, dropout).to(device)\n",
        "\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n"
      ],
      "metadata": {
        "id": "zq5EvBjotS-B"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def adjust_learning_rate(optimizer, epoch, lr_step=(300, 350), lr_decay_ratio=0.1):\n",
        "    \"\"\"Adjust the learning rate based on the epoch number.\"\"\"\n",
        "    if epoch == 0:\n",
        "        optimizer.param_groups[0]['lr'] /= 32\n",
        "    elif epoch in [3, 6, 9, 12, 15]:  # in step five , we finish warm up ,and start normal learning rate\n",
        "        optimizer.param_groups[0]['lr'] *= 2\n",
        "    if epoch in lr_step: # in these steps , we are geting close to optimal point so we need to have shorter step\n",
        "        optimizer.param_groups[0]['lr'] *= lr_decay_ratio\n",
        "    return optimizer\n",
        "\n",
        "# Define options here\n",
        "options = {\n",
        "    'lr': 0.001,\n",
        "    'lr_step': (300, 350), #steps(epoch) for updating learning rate\n",
        "    'lr_decay_ratio': 0.1,\n",
        "    # Add other options here\n",
        "}\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=options['lr'], betas=(0.9, 0.999))"
      ],
      "metadata": {
        "id": "7wv8vFTX4z_i"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "start_time = time.time()\n",
        "total_step = len(dataloader)\n",
        "for epoch in range(num_epochs):  # Loop over the dataset multiple times\n",
        "    optimizer = adjust_learning_rate(optimizer, epoch, options['lr_step'], options['lr_decay_ratio'])\n",
        "    print(optimizer.param_groups[0]['lr'])\n",
        "    train_loss = 0\n",
        "    for step, (seq, label) in enumerate(dataloader):\n",
        "        # Forward pass\n",
        "        seq = seq.clone().detach().view(-1, input_size, window_size).to(device)\n",
        "        output = model(seq)\n",
        "        loss = criterion(output, label.to(device))\n",
        "\n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        train_loss += loss.item()\n",
        "        optimizer.step()\n",
        "    print('Epoch [{}/{}], train_loss: {:.4f}'.format(epoch + 1, num_epochs, train_loss / total_step))\n",
        "elapsed_time = time.time() - start_time\n",
        "print('elapsed_time: {:.3f}s'.format(elapsed_time))\n",
        "print('Finished Training')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ku-o2J_t42m",
        "outputId": "55aea48b-1c3f-487c-9ae6-3c217d3d42cb"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.125e-05\n",
            "Epoch [1/400], train_loss: 3.3178\n",
            "3.125e-05\n",
            "Epoch [2/400], train_loss: 3.2599\n",
            "3.125e-05\n",
            "Epoch [3/400], train_loss: 3.1785\n",
            "6.25e-05\n",
            "Epoch [4/400], train_loss: 2.9664\n",
            "6.25e-05\n",
            "Epoch [5/400], train_loss: 2.5037\n",
            "6.25e-05\n",
            "Epoch [6/400], train_loss: 2.1759\n",
            "0.000125\n",
            "Epoch [7/400], train_loss: 2.0356\n",
            "0.000125\n",
            "Epoch [8/400], train_loss: 1.9346\n",
            "0.000125\n",
            "Epoch [9/400], train_loss: 1.8783\n",
            "0.00025\n",
            "Epoch [10/400], train_loss: 1.8174\n",
            "0.00025\n",
            "Epoch [11/400], train_loss: 1.7463\n",
            "0.00025\n",
            "Epoch [12/400], train_loss: 1.6927\n",
            "0.0005\n",
            "Epoch [13/400], train_loss: 1.6263\n",
            "0.0005\n",
            "Epoch [14/400], train_loss: 1.5354\n",
            "0.0005\n",
            "Epoch [15/400], train_loss: 1.4464\n",
            "0.001\n",
            "Epoch [16/400], train_loss: 1.3269\n",
            "0.001\n",
            "Epoch [17/400], train_loss: 1.1586\n",
            "0.001\n",
            "Epoch [18/400], train_loss: 0.9889\n",
            "0.001\n",
            "Epoch [19/400], train_loss: 0.8490\n",
            "0.001\n",
            "Epoch [20/400], train_loss: 0.7459\n",
            "0.001\n",
            "Epoch [21/400], train_loss: 0.6859\n",
            "0.001\n",
            "Epoch [22/400], train_loss: 0.6453\n",
            "0.001\n",
            "Epoch [23/400], train_loss: 0.6129\n",
            "0.001\n",
            "Epoch [24/400], train_loss: 0.5924\n",
            "0.001\n",
            "Epoch [25/400], train_loss: 0.5753\n",
            "0.001\n",
            "Epoch [26/400], train_loss: 0.5635\n",
            "0.001\n",
            "Epoch [27/400], train_loss: 0.5497\n",
            "0.001\n",
            "Epoch [28/400], train_loss: 0.5401\n",
            "0.001\n",
            "Epoch [29/400], train_loss: 0.5285\n",
            "0.001\n",
            "Epoch [30/400], train_loss: 0.5191\n",
            "0.001\n",
            "Epoch [31/400], train_loss: 0.5085\n",
            "0.001\n",
            "Epoch [32/400], train_loss: 0.5028\n",
            "0.001\n",
            "Epoch [33/400], train_loss: 0.5021\n",
            "0.001\n",
            "Epoch [34/400], train_loss: 0.4949\n",
            "0.001\n",
            "Epoch [35/400], train_loss: 0.4902\n",
            "0.001\n",
            "Epoch [36/400], train_loss: 0.4849\n",
            "0.001\n",
            "Epoch [37/400], train_loss: 0.4806\n",
            "0.001\n",
            "Epoch [38/400], train_loss: 0.4806\n",
            "0.001\n",
            "Epoch [39/400], train_loss: 0.4721\n",
            "0.001\n",
            "Epoch [40/400], train_loss: 0.4675\n",
            "0.001\n",
            "Epoch [41/400], train_loss: 0.4664\n",
            "0.001\n",
            "Epoch [42/400], train_loss: 0.4640\n",
            "0.001\n",
            "Epoch [43/400], train_loss: 0.4575\n",
            "0.001\n",
            "Epoch [44/400], train_loss: 0.4562\n",
            "0.001\n",
            "Epoch [45/400], train_loss: 0.4509\n",
            "0.001\n",
            "Epoch [46/400], train_loss: 0.4543\n",
            "0.001\n",
            "Epoch [47/400], train_loss: 0.4475\n",
            "0.001\n",
            "Epoch [48/400], train_loss: 0.4395\n",
            "0.001\n",
            "Epoch [49/400], train_loss: 0.4404\n",
            "0.001\n",
            "Epoch [50/400], train_loss: 0.4371\n",
            "0.001\n",
            "Epoch [51/400], train_loss: 0.4335\n",
            "0.001\n",
            "Epoch [52/400], train_loss: 0.4328\n",
            "0.001\n",
            "Epoch [53/400], train_loss: 0.4297\n",
            "0.001\n",
            "Epoch [54/400], train_loss: 0.4271\n",
            "0.001\n",
            "Epoch [55/400], train_loss: 0.4226\n",
            "0.001\n",
            "Epoch [56/400], train_loss: 0.4227\n",
            "0.001\n",
            "Epoch [57/400], train_loss: 0.4198\n",
            "0.001\n",
            "Epoch [58/400], train_loss: 0.4187\n",
            "0.001\n",
            "Epoch [59/400], train_loss: 0.4160\n",
            "0.001\n",
            "Epoch [60/400], train_loss: 0.4167\n",
            "0.001\n",
            "Epoch [61/400], train_loss: 0.4103\n",
            "0.001\n",
            "Epoch [62/400], train_loss: 0.4052\n",
            "0.001\n",
            "Epoch [63/400], train_loss: 0.4065\n",
            "0.001\n",
            "Epoch [64/400], train_loss: 0.4056\n",
            "0.001\n",
            "Epoch [65/400], train_loss: 0.4039\n",
            "0.001\n",
            "Epoch [66/400], train_loss: 0.4010\n",
            "0.001\n",
            "Epoch [67/400], train_loss: 0.3981\n",
            "0.001\n",
            "Epoch [68/400], train_loss: 0.3960\n",
            "0.001\n",
            "Epoch [69/400], train_loss: 0.3926\n",
            "0.001\n",
            "Epoch [70/400], train_loss: 0.3945\n",
            "0.001\n",
            "Epoch [71/400], train_loss: 0.3937\n",
            "0.001\n",
            "Epoch [72/400], train_loss: 0.3873\n",
            "0.001\n",
            "Epoch [73/400], train_loss: 0.3868\n",
            "0.001\n",
            "Epoch [74/400], train_loss: 0.3866\n",
            "0.001\n",
            "Epoch [75/400], train_loss: 0.3862\n",
            "0.001\n",
            "Epoch [76/400], train_loss: 0.3817\n",
            "0.001\n",
            "Epoch [77/400], train_loss: 0.3798\n",
            "0.001\n",
            "Epoch [78/400], train_loss: 0.3791\n",
            "0.001\n",
            "Epoch [79/400], train_loss: 0.3770\n",
            "0.001\n",
            "Epoch [80/400], train_loss: 0.3749\n",
            "0.001\n",
            "Epoch [81/400], train_loss: 0.3761\n",
            "0.001\n",
            "Epoch [82/400], train_loss: 0.3744\n",
            "0.001\n",
            "Epoch [83/400], train_loss: 0.3733\n",
            "0.001\n",
            "Epoch [84/400], train_loss: 0.3717\n",
            "0.001\n",
            "Epoch [85/400], train_loss: 0.3707\n",
            "0.001\n",
            "Epoch [86/400], train_loss: 0.3676\n",
            "0.001\n",
            "Epoch [87/400], train_loss: 0.3684\n",
            "0.001\n",
            "Epoch [88/400], train_loss: 0.3669\n",
            "0.001\n",
            "Epoch [89/400], train_loss: 0.3682\n",
            "0.001\n",
            "Epoch [90/400], train_loss: 0.3624\n",
            "0.001\n",
            "Epoch [91/400], train_loss: 0.3606\n",
            "0.001\n",
            "Epoch [92/400], train_loss: 0.3620\n",
            "0.001\n",
            "Epoch [93/400], train_loss: 0.3616\n",
            "0.001\n",
            "Epoch [94/400], train_loss: 0.3593\n",
            "0.001\n",
            "Epoch [95/400], train_loss: 0.3577\n",
            "0.001\n",
            "Epoch [96/400], train_loss: 0.3558\n",
            "0.001\n",
            "Epoch [97/400], train_loss: 0.3556\n",
            "0.001\n",
            "Epoch [98/400], train_loss: 0.3551\n",
            "0.001\n",
            "Epoch [99/400], train_loss: 0.3551\n",
            "0.001\n",
            "Epoch [100/400], train_loss: 0.3541\n",
            "0.001\n",
            "Epoch [101/400], train_loss: 0.3501\n",
            "0.001\n",
            "Epoch [102/400], train_loss: 0.3520\n",
            "0.001\n",
            "Epoch [103/400], train_loss: 0.3511\n",
            "0.001\n",
            "Epoch [104/400], train_loss: 0.3473\n",
            "0.001\n",
            "Epoch [105/400], train_loss: 0.3490\n",
            "0.001\n",
            "Epoch [106/400], train_loss: 0.3478\n",
            "0.001\n",
            "Epoch [107/400], train_loss: 0.3482\n",
            "0.001\n",
            "Epoch [108/400], train_loss: 0.3483\n",
            "0.001\n",
            "Epoch [109/400], train_loss: 0.3446\n",
            "0.001\n",
            "Epoch [110/400], train_loss: 0.3439\n",
            "0.001\n",
            "Epoch [111/400], train_loss: 0.3486\n",
            "0.001\n",
            "Epoch [112/400], train_loss: 0.3447\n",
            "0.001\n",
            "Epoch [113/400], train_loss: 0.3460\n",
            "0.001\n",
            "Epoch [114/400], train_loss: 0.3421\n",
            "0.001\n",
            "Epoch [115/400], train_loss: 0.3417\n",
            "0.001\n",
            "Epoch [116/400], train_loss: 0.3428\n",
            "0.001\n",
            "Epoch [117/400], train_loss: 0.3402\n",
            "0.001\n",
            "Epoch [118/400], train_loss: 0.3391\n",
            "0.001\n",
            "Epoch [119/400], train_loss: 0.3374\n",
            "0.001\n",
            "Epoch [120/400], train_loss: 0.3358\n",
            "0.001\n",
            "Epoch [121/400], train_loss: 0.3364\n",
            "0.001\n",
            "Epoch [122/400], train_loss: 0.3349\n",
            "0.001\n",
            "Epoch [123/400], train_loss: 0.3354\n",
            "0.001\n",
            "Epoch [124/400], train_loss: 0.3364\n",
            "0.001\n",
            "Epoch [125/400], train_loss: 0.3352\n",
            "0.001\n",
            "Epoch [126/400], train_loss: 0.3359\n",
            "0.001\n",
            "Epoch [127/400], train_loss: 0.3335\n",
            "0.001\n",
            "Epoch [128/400], train_loss: 0.3341\n",
            "0.001\n",
            "Epoch [129/400], train_loss: 0.3316\n",
            "0.001\n",
            "Epoch [130/400], train_loss: 0.3300\n",
            "0.001\n",
            "Epoch [131/400], train_loss: 0.3297\n",
            "0.001\n",
            "Epoch [132/400], train_loss: 0.3327\n",
            "0.001\n",
            "Epoch [133/400], train_loss: 0.3315\n",
            "0.001\n",
            "Epoch [134/400], train_loss: 0.3291\n",
            "0.001\n",
            "Epoch [135/400], train_loss: 0.3290\n",
            "0.001\n",
            "Epoch [136/400], train_loss: 0.3299\n",
            "0.001\n",
            "Epoch [137/400], train_loss: 0.3281\n",
            "0.001\n",
            "Epoch [138/400], train_loss: 0.3277\n",
            "0.001\n",
            "Epoch [139/400], train_loss: 0.3269\n",
            "0.001\n",
            "Epoch [140/400], train_loss: 0.3242\n",
            "0.001\n",
            "Epoch [141/400], train_loss: 0.3274\n",
            "0.001\n",
            "Epoch [142/400], train_loss: 0.3276\n",
            "0.001\n",
            "Epoch [143/400], train_loss: 0.3217\n",
            "0.001\n",
            "Epoch [144/400], train_loss: 0.3215\n",
            "0.001\n",
            "Epoch [145/400], train_loss: 0.3234\n",
            "0.001\n",
            "Epoch [146/400], train_loss: 0.3235\n",
            "0.001\n",
            "Epoch [147/400], train_loss: 0.3238\n",
            "0.001\n",
            "Epoch [148/400], train_loss: 0.3204\n",
            "0.001\n",
            "Epoch [149/400], train_loss: 0.3224\n",
            "0.001\n",
            "Epoch [150/400], train_loss: 0.3190\n",
            "0.001\n",
            "Epoch [151/400], train_loss: 0.3188\n",
            "0.001\n",
            "Epoch [152/400], train_loss: 0.3213\n",
            "0.001\n",
            "Epoch [153/400], train_loss: 0.3221\n",
            "0.001\n",
            "Epoch [154/400], train_loss: 0.3199\n",
            "0.001\n",
            "Epoch [155/400], train_loss: 0.3217\n",
            "0.001\n",
            "Epoch [156/400], train_loss: 0.3196\n",
            "0.001\n",
            "Epoch [157/400], train_loss: 0.3212\n",
            "0.001\n",
            "Epoch [158/400], train_loss: 0.3196\n",
            "0.001\n",
            "Epoch [159/400], train_loss: 0.3176\n",
            "0.001\n",
            "Epoch [160/400], train_loss: 0.3165\n",
            "0.001\n",
            "Epoch [161/400], train_loss: 0.3184\n",
            "0.001\n",
            "Epoch [162/400], train_loss: 0.3172\n",
            "0.001\n",
            "Epoch [163/400], train_loss: 0.3151\n",
            "0.001\n",
            "Epoch [164/400], train_loss: 0.3151\n",
            "0.001\n",
            "Epoch [165/400], train_loss: 0.3163\n",
            "0.001\n",
            "Epoch [166/400], train_loss: 0.3135\n",
            "0.001\n",
            "Epoch [167/400], train_loss: 0.3128\n",
            "0.001\n",
            "Epoch [168/400], train_loss: 0.3119\n",
            "0.001\n",
            "Epoch [169/400], train_loss: 0.3129\n",
            "0.001\n",
            "Epoch [170/400], train_loss: 0.3140\n",
            "0.001\n",
            "Epoch [171/400], train_loss: 0.3108\n",
            "0.001\n",
            "Epoch [172/400], train_loss: 0.3135\n",
            "0.001\n",
            "Epoch [173/400], train_loss: 0.3108\n",
            "0.001\n",
            "Epoch [174/400], train_loss: 0.3095\n",
            "0.001\n",
            "Epoch [175/400], train_loss: 0.3113\n",
            "0.001\n",
            "Epoch [176/400], train_loss: 0.3116\n",
            "0.001\n",
            "Epoch [177/400], train_loss: 0.3093\n",
            "0.001\n",
            "Epoch [178/400], train_loss: 0.3094\n",
            "0.001\n",
            "Epoch [179/400], train_loss: 0.3093\n",
            "0.001\n",
            "Epoch [180/400], train_loss: 0.3104\n",
            "0.001\n",
            "Epoch [181/400], train_loss: 0.3098\n",
            "0.001\n",
            "Epoch [182/400], train_loss: 0.3072\n",
            "0.001\n",
            "Epoch [183/400], train_loss: 0.3081\n",
            "0.001\n",
            "Epoch [184/400], train_loss: 0.3085\n",
            "0.001\n",
            "Epoch [185/400], train_loss: 0.3087\n",
            "0.001\n",
            "Epoch [186/400], train_loss: 0.3071\n",
            "0.001\n",
            "Epoch [187/400], train_loss: 0.3062\n",
            "0.001\n",
            "Epoch [188/400], train_loss: 0.3052\n",
            "0.001\n",
            "Epoch [189/400], train_loss: 0.3040\n",
            "0.001\n",
            "Epoch [190/400], train_loss: 0.3072\n",
            "0.001\n",
            "Epoch [191/400], train_loss: 0.3066\n",
            "0.001\n",
            "Epoch [192/400], train_loss: 0.3053\n",
            "0.001\n",
            "Epoch [193/400], train_loss: 0.3042\n",
            "0.001\n",
            "Epoch [194/400], train_loss: 0.3042\n",
            "0.001\n",
            "Epoch [195/400], train_loss: 0.3042\n",
            "0.001\n",
            "Epoch [196/400], train_loss: 0.3019\n",
            "0.001\n",
            "Epoch [197/400], train_loss: 0.3034\n",
            "0.001\n",
            "Epoch [198/400], train_loss: 0.3008\n",
            "0.001\n",
            "Epoch [199/400], train_loss: 0.3029\n",
            "0.001\n",
            "Epoch [200/400], train_loss: 0.3030\n",
            "0.001\n",
            "Epoch [201/400], train_loss: 0.3030\n",
            "0.001\n",
            "Epoch [202/400], train_loss: 0.3011\n",
            "0.001\n",
            "Epoch [203/400], train_loss: 0.3008\n",
            "0.001\n",
            "Epoch [204/400], train_loss: 0.2994\n",
            "0.001\n",
            "Epoch [205/400], train_loss: 0.2997\n",
            "0.001\n",
            "Epoch [206/400], train_loss: 0.3025\n",
            "0.001\n",
            "Epoch [207/400], train_loss: 0.3005\n",
            "0.001\n",
            "Epoch [208/400], train_loss: 0.2976\n",
            "0.001\n",
            "Epoch [209/400], train_loss: 0.2966\n",
            "0.001\n",
            "Epoch [210/400], train_loss: 0.2971\n",
            "0.001\n",
            "Epoch [211/400], train_loss: 0.2969\n",
            "0.001\n",
            "Epoch [212/400], train_loss: 0.2971\n",
            "0.001\n",
            "Epoch [213/400], train_loss: 0.2969\n",
            "0.001\n",
            "Epoch [214/400], train_loss: 0.2971\n",
            "0.001\n",
            "Epoch [215/400], train_loss: 0.2981\n",
            "0.001\n",
            "Epoch [216/400], train_loss: 0.2999\n",
            "0.001\n",
            "Epoch [217/400], train_loss: 0.2954\n",
            "0.001\n",
            "Epoch [218/400], train_loss: 0.2938\n",
            "0.001\n",
            "Epoch [219/400], train_loss: 0.2972\n",
            "0.001\n",
            "Epoch [220/400], train_loss: 0.2935\n",
            "0.001\n",
            "Epoch [221/400], train_loss: 0.2923\n",
            "0.001\n",
            "Epoch [222/400], train_loss: 0.2930\n",
            "0.001\n",
            "Epoch [223/400], train_loss: 0.2919\n",
            "0.001\n",
            "Epoch [224/400], train_loss: 0.2919\n",
            "0.001\n",
            "Epoch [225/400], train_loss: 0.2912\n",
            "0.001\n",
            "Epoch [226/400], train_loss: 0.2941\n",
            "0.001\n",
            "Epoch [227/400], train_loss: 0.2922\n",
            "0.001\n",
            "Epoch [228/400], train_loss: 0.2925\n",
            "0.001\n",
            "Epoch [229/400], train_loss: 0.2940\n",
            "0.001\n",
            "Epoch [230/400], train_loss: 0.2930\n",
            "0.001\n",
            "Epoch [231/400], train_loss: 0.2907\n",
            "0.001\n",
            "Epoch [232/400], train_loss: 0.2943\n",
            "0.001\n",
            "Epoch [233/400], train_loss: 0.2915\n",
            "0.001\n",
            "Epoch [234/400], train_loss: 0.2910\n",
            "0.001\n",
            "Epoch [235/400], train_loss: 0.2892\n",
            "0.001\n",
            "Epoch [236/400], train_loss: 0.2902\n",
            "0.001\n",
            "Epoch [237/400], train_loss: 0.2877\n",
            "0.001\n",
            "Epoch [238/400], train_loss: 0.2869\n",
            "0.001\n",
            "Epoch [239/400], train_loss: 0.2877\n",
            "0.001\n",
            "Epoch [240/400], train_loss: 0.2872\n",
            "0.001\n",
            "Epoch [241/400], train_loss: 0.2862\n",
            "0.001\n",
            "Epoch [242/400], train_loss: 0.2892\n",
            "0.001\n",
            "Epoch [243/400], train_loss: 0.2875\n",
            "0.001\n",
            "Epoch [244/400], train_loss: 0.2869\n",
            "0.001\n",
            "Epoch [245/400], train_loss: 0.2877\n",
            "0.001\n",
            "Epoch [246/400], train_loss: 0.2872\n",
            "0.001\n",
            "Epoch [247/400], train_loss: 0.2873\n",
            "0.001\n",
            "Epoch [248/400], train_loss: 0.2850\n",
            "0.001\n",
            "Epoch [249/400], train_loss: 0.2866\n",
            "0.001\n",
            "Epoch [250/400], train_loss: 0.2832\n",
            "0.001\n",
            "Epoch [251/400], train_loss: 0.2839\n",
            "0.001\n",
            "Epoch [252/400], train_loss: 0.2846\n",
            "0.001\n",
            "Epoch [253/400], train_loss: 0.2870\n",
            "0.001\n",
            "Epoch [254/400], train_loss: 0.2855\n",
            "0.001\n",
            "Epoch [255/400], train_loss: 0.2828\n",
            "0.001\n",
            "Epoch [256/400], train_loss: 0.2812\n",
            "0.001\n",
            "Epoch [257/400], train_loss: 0.2831\n",
            "0.001\n",
            "Epoch [258/400], train_loss: 0.2822\n",
            "0.001\n",
            "Epoch [259/400], train_loss: 0.2828\n",
            "0.001\n",
            "Epoch [260/400], train_loss: 0.2839\n",
            "0.001\n",
            "Epoch [261/400], train_loss: 0.2810\n",
            "0.001\n",
            "Epoch [262/400], train_loss: 0.2815\n",
            "0.001\n",
            "Epoch [263/400], train_loss: 0.2818\n",
            "0.001\n",
            "Epoch [264/400], train_loss: 0.2821\n",
            "0.001\n",
            "Epoch [265/400], train_loss: 0.2824\n",
            "0.001\n",
            "Epoch [266/400], train_loss: 0.2813\n",
            "0.001\n",
            "Epoch [267/400], train_loss: 0.2846\n",
            "0.001\n",
            "Epoch [268/400], train_loss: 0.2833\n",
            "0.001\n",
            "Epoch [269/400], train_loss: 0.2808\n",
            "0.001\n",
            "Epoch [270/400], train_loss: 0.2798\n",
            "0.001\n",
            "Epoch [271/400], train_loss: 0.2801\n",
            "0.001\n",
            "Epoch [272/400], train_loss: 0.2817\n",
            "0.001\n",
            "Epoch [273/400], train_loss: 0.2820\n",
            "0.001\n",
            "Epoch [274/400], train_loss: 0.2827\n",
            "0.001\n",
            "Epoch [275/400], train_loss: 0.2783\n",
            "0.001\n",
            "Epoch [276/400], train_loss: 0.2796\n",
            "0.001\n",
            "Epoch [277/400], train_loss: 0.2771\n",
            "0.001\n",
            "Epoch [278/400], train_loss: 0.2785\n",
            "0.001\n",
            "Epoch [279/400], train_loss: 0.2796\n",
            "0.001\n",
            "Epoch [280/400], train_loss: 0.2774\n",
            "0.001\n",
            "Epoch [281/400], train_loss: 0.2797\n",
            "0.001\n",
            "Epoch [282/400], train_loss: 0.2801\n",
            "0.001\n",
            "Epoch [283/400], train_loss: 0.2789\n",
            "0.001\n",
            "Epoch [284/400], train_loss: 0.2800\n",
            "0.001\n",
            "Epoch [285/400], train_loss: 0.2749\n",
            "0.001\n",
            "Epoch [286/400], train_loss: 0.2748\n",
            "0.001\n",
            "Epoch [287/400], train_loss: 0.2764\n",
            "0.001\n",
            "Epoch [288/400], train_loss: 0.2771\n",
            "0.001\n",
            "Epoch [289/400], train_loss: 0.2773\n",
            "0.001\n",
            "Epoch [290/400], train_loss: 0.2783\n",
            "0.001\n",
            "Epoch [291/400], train_loss: 0.2765\n",
            "0.001\n",
            "Epoch [292/400], train_loss: 0.2777\n",
            "0.001\n",
            "Epoch [293/400], train_loss: 0.2755\n",
            "0.001\n",
            "Epoch [294/400], train_loss: 0.2755\n",
            "0.001\n",
            "Epoch [295/400], train_loss: 0.2789\n",
            "0.001\n",
            "Epoch [296/400], train_loss: 0.2768\n",
            "0.001\n",
            "Epoch [297/400], train_loss: 0.2744\n",
            "0.001\n",
            "Epoch [298/400], train_loss: 0.2766\n",
            "0.001\n",
            "Epoch [299/400], train_loss: 0.2757\n",
            "0.001\n",
            "Epoch [300/400], train_loss: 0.2747\n",
            "0.0002\n",
            "Epoch [301/400], train_loss: 0.2703\n",
            "0.0002\n",
            "Epoch [302/400], train_loss: 0.2700\n",
            "0.0002\n",
            "Epoch [303/400], train_loss: 0.2691\n",
            "0.0002\n",
            "Epoch [304/400], train_loss: 0.2696\n",
            "0.0002\n",
            "Epoch [305/400], train_loss: 0.2704\n",
            "0.0002\n",
            "Epoch [306/400], train_loss: 0.2692\n",
            "0.0002\n",
            "Epoch [307/400], train_loss: 0.2692\n",
            "0.0002\n",
            "Epoch [308/400], train_loss: 0.2680\n",
            "0.0002\n",
            "Epoch [309/400], train_loss: 0.2694\n",
            "0.0002\n",
            "Epoch [310/400], train_loss: 0.2681\n",
            "0.0002\n",
            "Epoch [311/400], train_loss: 0.2679\n",
            "0.0002\n",
            "Epoch [312/400], train_loss: 0.2684\n",
            "0.0002\n",
            "Epoch [313/400], train_loss: 0.2682\n",
            "0.0002\n",
            "Epoch [314/400], train_loss: 0.2684\n",
            "0.0002\n",
            "Epoch [315/400], train_loss: 0.2661\n",
            "0.0002\n",
            "Epoch [316/400], train_loss: 0.2669\n",
            "0.0002\n",
            "Epoch [317/400], train_loss: 0.2684\n",
            "0.0002\n",
            "Epoch [318/400], train_loss: 0.2673\n",
            "0.0002\n",
            "Epoch [319/400], train_loss: 0.2678\n",
            "0.0002\n",
            "Epoch [320/400], train_loss: 0.2666\n",
            "0.0002\n",
            "Epoch [321/400], train_loss: 0.2680\n",
            "0.0002\n",
            "Epoch [322/400], train_loss: 0.2681\n",
            "0.0002\n",
            "Epoch [323/400], train_loss: 0.2690\n",
            "0.0002\n",
            "Epoch [324/400], train_loss: 0.2665\n",
            "0.0002\n",
            "Epoch [325/400], train_loss: 0.2664\n",
            "0.0002\n",
            "Epoch [326/400], train_loss: 0.2682\n",
            "0.0002\n",
            "Epoch [327/400], train_loss: 0.2664\n",
            "0.0002\n",
            "Epoch [328/400], train_loss: 0.2690\n",
            "0.0002\n",
            "Epoch [329/400], train_loss: 0.2685\n",
            "0.0002\n",
            "Epoch [330/400], train_loss: 0.2675\n",
            "0.0002\n",
            "Epoch [331/400], train_loss: 0.2660\n",
            "0.0002\n",
            "Epoch [332/400], train_loss: 0.2665\n",
            "0.0002\n",
            "Epoch [333/400], train_loss: 0.2679\n",
            "0.0002\n",
            "Epoch [334/400], train_loss: 0.2686\n",
            "0.0002\n",
            "Epoch [335/400], train_loss: 0.2675\n",
            "0.0002\n",
            "Epoch [336/400], train_loss: 0.2669\n",
            "0.0002\n",
            "Epoch [337/400], train_loss: 0.2668\n",
            "0.0002\n",
            "Epoch [338/400], train_loss: 0.2682\n",
            "0.0002\n",
            "Epoch [339/400], train_loss: 0.2668\n",
            "0.0002\n",
            "Epoch [340/400], train_loss: 0.2679\n",
            "0.0002\n",
            "Epoch [341/400], train_loss: 0.2674\n",
            "0.0002\n",
            "Epoch [342/400], train_loss: 0.2674\n",
            "0.0002\n",
            "Epoch [343/400], train_loss: 0.2668\n",
            "0.0002\n",
            "Epoch [344/400], train_loss: 0.2681\n",
            "0.0002\n",
            "Epoch [345/400], train_loss: 0.2657\n",
            "0.0002\n",
            "Epoch [346/400], train_loss: 0.2674\n",
            "0.0002\n",
            "Epoch [347/400], train_loss: 0.2658\n",
            "0.0002\n",
            "Epoch [348/400], train_loss: 0.2663\n",
            "0.0002\n",
            "Epoch [349/400], train_loss: 0.2661\n",
            "0.0002\n",
            "Epoch [350/400], train_loss: 0.2674\n",
            "4e-05\n",
            "Epoch [351/400], train_loss: 0.2666\n",
            "4e-05\n",
            "Epoch [352/400], train_loss: 0.2663\n",
            "4e-05\n",
            "Epoch [353/400], train_loss: 0.2651\n",
            "4e-05\n",
            "Epoch [354/400], train_loss: 0.2647\n",
            "4e-05\n",
            "Epoch [355/400], train_loss: 0.2645\n",
            "4e-05\n",
            "Epoch [356/400], train_loss: 0.2668\n",
            "4e-05\n",
            "Epoch [357/400], train_loss: 0.2638\n",
            "4e-05\n",
            "Epoch [358/400], train_loss: 0.2650\n",
            "4e-05\n",
            "Epoch [359/400], train_loss: 0.2648\n",
            "4e-05\n",
            "Epoch [360/400], train_loss: 0.2633\n",
            "4e-05\n",
            "Epoch [361/400], train_loss: 0.2654\n",
            "4e-05\n",
            "Epoch [362/400], train_loss: 0.2657\n",
            "4e-05\n",
            "Epoch [363/400], train_loss: 0.2652\n",
            "4e-05\n",
            "Epoch [364/400], train_loss: 0.2662\n",
            "4e-05\n",
            "Epoch [365/400], train_loss: 0.2666\n",
            "4e-05\n",
            "Epoch [366/400], train_loss: 0.2649\n",
            "4e-05\n",
            "Epoch [367/400], train_loss: 0.2656\n",
            "4e-05\n",
            "Epoch [368/400], train_loss: 0.2641\n",
            "4e-05\n",
            "Epoch [369/400], train_loss: 0.2640\n",
            "4e-05\n",
            "Epoch [370/400], train_loss: 0.2664\n",
            "4e-05\n",
            "Epoch [371/400], train_loss: 0.2651\n",
            "4e-05\n",
            "Epoch [372/400], train_loss: 0.2653\n",
            "4e-05\n",
            "Epoch [373/400], train_loss: 0.2660\n",
            "4e-05\n",
            "Epoch [374/400], train_loss: 0.2655\n",
            "4e-05\n",
            "Epoch [375/400], train_loss: 0.2644\n",
            "4e-05\n",
            "Epoch [376/400], train_loss: 0.2646\n",
            "4e-05\n",
            "Epoch [377/400], train_loss: 0.2652\n",
            "4e-05\n",
            "Epoch [378/400], train_loss: 0.2641\n",
            "4e-05\n",
            "Epoch [379/400], train_loss: 0.2634\n",
            "4e-05\n",
            "Epoch [380/400], train_loss: 0.2648\n",
            "4e-05\n",
            "Epoch [381/400], train_loss: 0.2645\n",
            "4e-05\n",
            "Epoch [382/400], train_loss: 0.2655\n",
            "4e-05\n",
            "Epoch [383/400], train_loss: 0.2662\n",
            "4e-05\n",
            "Epoch [384/400], train_loss: 0.2643\n",
            "4e-05\n",
            "Epoch [385/400], train_loss: 0.2652\n",
            "4e-05\n",
            "Epoch [386/400], train_loss: 0.2636\n",
            "4e-05\n",
            "Epoch [387/400], train_loss: 0.2640\n",
            "4e-05\n",
            "Epoch [388/400], train_loss: 0.2628\n",
            "4e-05\n",
            "Epoch [389/400], train_loss: 0.2654\n",
            "4e-05\n",
            "Epoch [390/400], train_loss: 0.2636\n",
            "4e-05\n",
            "Epoch [391/400], train_loss: 0.2644\n",
            "4e-05\n",
            "Epoch [392/400], train_loss: 0.2649\n",
            "4e-05\n",
            "Epoch [393/400], train_loss: 0.2653\n",
            "4e-05\n",
            "Epoch [394/400], train_loss: 0.2638\n",
            "4e-05\n",
            "Epoch [395/400], train_loss: 0.2645\n",
            "4e-05\n",
            "Epoch [396/400], train_loss: 0.2632\n",
            "4e-05\n",
            "Epoch [397/400], train_loss: 0.2644\n",
            "4e-05\n",
            "Epoch [398/400], train_loss: 0.2648\n",
            "4e-05\n",
            "Epoch [399/400], train_loss: 0.2641\n",
            "4e-05\n",
            "Epoch [400/400], train_loss: 0.2640\n",
            "elapsed_time: 295.177s\n",
            "Finished Training\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate(name):\n",
        "    window_size = 10\n",
        "    hdfs = {} #store the unique sequences and their counts.\n",
        "    length = 0\n",
        "    with open('/content/' + name, 'r') as f:\n",
        "        for ln in f.readlines():\n",
        "            ln = [(int(i)-1) for i in ln.strip().split()]\n",
        "            ln = ln + [-1] * (window_size + 1 - len(ln))     #ensure that all sequences have a fixed length of window_size + 1, even if the original line had fewer elements.\n",
        "            hdfs[tuple(ln)] = hdfs.get(tuple(ln), 0) + 1   #If the tuple is not present in the dictionary, hdfs.get(tuple(ln), 0) returns 0, and the code initializes the count to 1.\n",
        "            length += 1\n",
        "    print('Number of sessions({}): {}'.format(name, len(hdfs)))\n",
        "    return hdfs, length\n"
      ],
      "metadata": {
        "id": "hvq7Ivt3w0EY"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_normal_loader, test_normal_length = generate('hdfs_test_normal')\n",
        "test_abnormal_loader, test_abnormal_length = generate('hdfs_test_abnormal')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dUaWdas_w2Yg",
        "outputId": "4371c4e8-319d-4e46-f052-b9cc13bd6858"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of sessions(hdfs_test_normal): 14177\n",
            "Number of sessions(hdfs_test_abnormal): 4123\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_candidates = 9"
      ],
      "metadata": {
        "id": "W-hWDi2kxXQi"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Test the model\n",
        "model.eval()\n",
        "\n",
        "TP = 0\n",
        "FP = 0\n",
        "\n",
        "start_time = time.time()\n",
        "with torch.no_grad():\n",
        "    for line in tqdm(test_normal_loader.keys()):\n",
        "        for i in range(len(line) - window_size):\n",
        "            seq = line[i:i + window_size]\n",
        "            label = line[i + window_size]\n",
        "            seq = torch.tensor(seq, dtype=torch.float).view(-1, input_size, window_size).to(device)\n",
        "            label = torch.tensor(label).view(-1).to(device)\n",
        "            output = model(seq)\n",
        "            predicted = torch.argsort(output, 1)[0][-num_candidates:]\n",
        "            if label not in predicted:\n",
        "                FP += test_normal_loader[line] # numbers of that set we have\n",
        "                break   #with just one wrong prediction in a line , we assume , abnormal\n",
        "with torch.no_grad():\n",
        "    for line in tqdm(test_abnormal_loader.keys()):\n",
        "        for i in range(len(line) - window_size):\n",
        "            seq = line[i:i + window_size]\n",
        "            label = line[i + window_size]\n",
        "            seq = torch.tensor(seq, dtype=torch.float).view(-1, input_size, window_size).to(device)\n",
        "            label = torch.tensor(label).view(-1).to(device)\n",
        "            output = model(seq)\n",
        "            predicted = torch.argsort(output, 1)[0][-num_candidates:]\n",
        "            if label not in predicted:\n",
        "                TP += test_abnormal_loader[line]\n",
        "                break\n",
        "elapsed_time = time.time() - start_time\n",
        "print('elapsed_time: {:.3f}s'.format(elapsed_time))\n",
        "# Compute precision, recall and F1-measure\n",
        "FN = test_abnormal_length - TP\n",
        "P = 100 * TP / (TP + FP)\n",
        "R = 100 * TP / (TP + FN)\n",
        "F1 = 2 * P * R / (P + R)\n",
        "print('false positive (FP): {}, false negative (FN): {}, Precision: {:.3f}%, Recall: {:.3f}%, F1-measure: {:.3f}%'.format(FP, FN, P, R, F1))\n",
        "print('Finished Predicting')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5SsulX0iw6GI",
        "outputId": "8f4483f1-c741-45c3-d155-043eead8b81e"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 14177/14177 [06:57<00:00, 33.96it/s]\n",
            "100%|██████████| 4123/4123 [01:16<00:00, 54.04it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "elapsed_time: 493.824s\n",
            "false positive (FP): 1146, false negative (FN): 943, Precision: 93.275%, Recall: 94.400%, F1-measure: 93.834%\n",
            "Finished Predicting\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9QvRyxK0XO0n"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}