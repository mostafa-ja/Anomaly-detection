{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPgzWkrUV+Ach0wxKr8S9ff",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mostafa-ja/Anomaly-detection/blob/main/TCN2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the TCN model and load pre-trained weights if available\n",
        "input_size = 1  # Assuming each log template is represented by a single number\n",
        "output_size = 28  # Number of classes in the one-hot encodings\n",
        "num_channels = [16, 32]\n",
        "kernel_size = 5\n",
        "dropout = 0.2"
      ],
      "metadata": {
        "id": "fZrefvn1Ano6"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_blocks = len(num_channels)  # Number of blocks in the TCN\n",
        "dilations = [2 ** i for i in range(num_blocks)]  # [1, 2, 4] as before\n",
        "\n",
        "receptive_field_per_layer = (kernel_size - 1) * (max(dilations) - 1) + 1\n",
        "\n",
        "total_receptive_field = receptive_field_per_layer * num_blocks\n",
        "\n",
        "print(f\"Total Receptive Field of TCN (with each block consisting of two layers): {total_receptive_field}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MKkz0lF0Fb9Y",
        "outputId": "4f6a24ed-70ef-4f6b-ab33-7ef5230e3ab0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Receptive Field of TCN (with each block consisting of two layers): 10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# download datasets\n",
        "!wget 'https://raw.githubusercontent.com/donglee-afar/logdeep/master/data/hdfs/hdfs_train'\n",
        "!wget 'https://raw.githubusercontent.com/donglee-afar/logdeep/master/data/hdfs/hdfs_test_normal'\n",
        "!wget 'https://raw.githubusercontent.com/donglee-afar/logdeep/master/data/hdfs/hdfs_test_abnormal'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o4Rwy8JxsY3q",
        "outputId": "d7cfa42f-6d2f-4024-c6f6-53b74ec7a137"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-07-29 15:03:55--  https://raw.githubusercontent.com/donglee-afar/logdeep/master/data/hdfs/hdfs_train\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 257875 (252K) [text/plain]\n",
            "Saving to: ‘hdfs_train’\n",
            "\n",
            "hdfs_train          100%[===================>] 251.83K  --.-KB/s    in 0.03s   \n",
            "\n",
            "2023-07-29 15:03:55 (7.34 MB/s) - ‘hdfs_train’ saved [257875/257875]\n",
            "\n",
            "--2023-07-29 15:03:55--  https://raw.githubusercontent.com/donglee-afar/logdeep/master/data/hdfs/hdfs_test_normal\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.110.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 29284282 (28M) [text/plain]\n",
            "Saving to: ‘hdfs_test_normal’\n",
            "\n",
            "hdfs_test_normal    100%[===================>]  27.93M  64.5MB/s    in 0.4s    \n",
            "\n",
            "2023-07-29 15:03:56 (64.5 MB/s) - ‘hdfs_test_normal’ saved [29284282/29284282]\n",
            "\n",
            "--2023-07-29 15:03:56--  https://raw.githubusercontent.com/donglee-afar/logdeep/master/data/hdfs/hdfs_test_abnormal\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 782479 (764K) [text/plain]\n",
            "Saving to: ‘hdfs_test_abnormal’\n",
            "\n",
            "hdfs_test_abnormal  100%[===================>] 764.14K  --.-KB/s    in 0.05s   \n",
            "\n",
            "2023-07-29 15:03:56 (14.7 MB/s) - ‘hdfs_test_abnormal’ saved [782479/782479]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn.utils import weight_norm\n",
        "import time\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "-yTpujeh70P8"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Chomp1d(nn.Module):\n",
        "    def __init__(self, chomp_size):\n",
        "        \"\"\"\n",
        "        Removes the extra elements at the end of the input tensor along the time dimension.\n",
        "\n",
        "        Args:\n",
        "        - chomp_size (int): The number of elements to be removed from the end of each input tensor.\n",
        "        \"\"\"\n",
        "        super(Chomp1d, self).__init__()\n",
        "        self.chomp_size = chomp_size\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass of Chomp1d.\n",
        "\n",
        "        Args:\n",
        "        - x (torch.Tensor): Input tensor with dimension (N, C, L). (batch size, number of input features(channels), input sequence length)\n",
        "\n",
        "        Returns:\n",
        "        - torch.Tensor: Output tensor with dimension (N, C, L - chomp_size).\n",
        "        \"\"\"\n",
        "        return x[:, :, :-self.chomp_size].contiguous()\n",
        "\n",
        "\n",
        "class TemporalBlock(nn.Module):\n",
        "    def __init__(self, n_inputs, n_outputs, kernel_size, stride, dilation, padding, dropout=0.2):\n",
        "        \"\"\"\n",
        "        A single temporal block in the Temporal Convolutional Network.\n",
        "\n",
        "        Args:\n",
        "        - n_inputs (int): Number of input channels.\n",
        "        - n_outputs (int): Number of output channels.\n",
        "        - kernel_size (int): Size of the convolutional kernels.\n",
        "        - stride (int): Stride for the convolutional layers.\n",
        "        - dilation (int): Dilation rate for the convolutional layers.\n",
        "        - padding (int): Padding size for the convolutional layers.\n",
        "        - dropout (float): Dropout rate to be applied after each convolutional layer.\n",
        "        \"\"\"\n",
        "        super(TemporalBlock, self).__init__()\n",
        "        self.conv1 = weight_norm(nn.Conv1d(n_inputs, n_outputs, kernel_size,\n",
        "                                           stride=stride, padding=padding, dilation=dilation))\n",
        "        self.chomp1 = Chomp1d(padding)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "\n",
        "        self.conv2 = weight_norm(nn.Conv1d(n_outputs, n_outputs, kernel_size,\n",
        "                                           stride=stride, padding=padding, dilation=dilation))\n",
        "        self.chomp2 = Chomp1d(padding)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "\n",
        "        self.net = nn.Sequential(self.conv1, self.chomp1, self.relu1, self.dropout1,\n",
        "                                 self.conv2, self.chomp2, self.relu2, self.dropout2)\n",
        "\n",
        "        # The downsample variable stores a 1x1 convolutional layer\n",
        "        # (if n_inputs is not equal to n_outputs) to match the input and output dimensions of the block. Otherwise, it is set to None.\n",
        "        #IMPORTANT : The purpose of self.downsample is to adjust the dimensions of the input tensor so that it can be added element-wise to the output of the\n",
        "        #temporal block,This addition is part of the residual connection that helps with training deeper networks and improves gradient flow during\n",
        "        # backpropagation. CAREFUL, because here we have kernel size one, we dont need  Chomp1d() to equlize their size\n",
        "        self.downsample = nn.Conv1d(n_inputs, n_outputs, 1) if n_inputs != n_outputs else None\n",
        "\n",
        "        self.relu = nn.ReLU()\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        \"\"\"\n",
        "        Initialize the weights of the convolutional layers with a normal distribution (mean=0, std=0.01).\n",
        "        \"\"\"\n",
        "        self.conv1.weight.data.normal_(0, 0.01)\n",
        "        self.conv2.weight.data.normal_(0, 0.01)\n",
        "        if self.downsample is not None:\n",
        "            self.downsample.weight.data.normal_(0, 0.01)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass of TemporalBlock.\n",
        "\n",
        "        Args:\n",
        "        - x (torch.Tensor): Input tensor with dimension (N, C, L).\n",
        "\n",
        "        Returns:\n",
        "        - torch.Tensor: Output tensor with dimension (N, C, L).\n",
        "        \"\"\"\n",
        "        out = self.net(x)\n",
        "        res = x if self.downsample is None else self.downsample(x)\n",
        "        return self.relu(out + res)\n",
        "\n",
        "\n",
        "class TemporalConvNet(nn.Module):\n",
        "    def __init__(self, num_inputs, num_channels, kernel_size=2, dropout=0.2):\n",
        "        \"\"\"\n",
        "        Temporal Convolutional Network (TCN) for sequence processing.\n",
        "        This class represents the Temporal Convolutional Network (TCN) architecture that contains multiple temporal blocks stacked together.\n",
        "\n",
        "        Args:\n",
        "        - num_inputs (int): Number of input channels.\n",
        "        - num_channels (list of int): Number of channels for each temporal block.\n",
        "        - kernel_size (int): Size of the convolutional kernels in the temporal blocks.\n",
        "        - dropout (float): Dropout rate to be applied after each temporal block.\n",
        "        \"\"\"\n",
        "        super(TemporalConvNet, self).__init__()\n",
        "        layers = []\n",
        "        num_levels = len(num_channels)\n",
        "        for i in range(num_levels):\n",
        "            dilation_size = 2 ** i\n",
        "            #The input channels for each block are determined based on the number of input channels (num_inputs) for the first block,\n",
        "            #and for subsequent blocks, the number of input channels is set to the number of output channels of the previous block (num_channels[i-1]).\n",
        "            in_channels = num_inputs if i == 0 else num_channels[i - 1]\n",
        "            out_channels = num_channels[i]\n",
        "\n",
        "            layers += [TemporalBlock(in_channels, out_channels, kernel_size, stride=1, dilation=dilation_size,\n",
        "                                     padding=(kernel_size - 1) * dilation_size, dropout=dropout)]\n",
        "\n",
        "        self.network = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass of TemporalConvNet.\n",
        "\n",
        "        Args:\n",
        "        - x (torch.Tensor): Input tensor with dimension (N, C, L).\n",
        "\n",
        "        Returns:\n",
        "        - torch.Tensor: Output tensor with dimension (N, C, L).\n",
        "        \"\"\"\n",
        "        return self.network(x)\n",
        "\n",
        "\n",
        "\n",
        "class TCN(nn.Module):\n",
        "    def __init__(self, input_size, output_size, num_channels, kernel_size, dropout):\n",
        "        \"\"\"\n",
        "        Temporal Convolutional Network (TCN) for sequence classification.\n",
        "\n",
        "        Args:\n",
        "        - input_size (int): The number of input features (C_in).\n",
        "        - output_size (int): The number of classes for classification.\n",
        "        - num_channels (list of int): Number of channels for each TCN layer.\n",
        "        - kernel_size (int): The size of the convolutional kernels in TCN layers.\n",
        "        - dropout (float): The dropout rate to be applied after each TCN layer.\n",
        "        \"\"\"\n",
        "        super(TCN, self).__init__()\n",
        "        self.tcn = TemporalConvNet(input_size, num_channels, kernel_size=kernel_size, dropout=dropout)\n",
        "        self.linear = nn.Linear(num_channels[-1], output_size)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        \"\"\"\n",
        "        Forward pass of the TCN model.\n",
        "\n",
        "        Args:\n",
        "        - inputs (torch.Tensor): Input tensor with dimension (N, C_in, L_in).(batch size, number of input features(channels), input sequence length)\n",
        "\n",
        "        Returns:\n",
        "        - torch.Tensor: The model's output tensor with dimension (N, output_size).\n",
        "        \"\"\"\n",
        "        # Input should have dimension (N, C_in, L_in)\n",
        "        y = self.tcn(inputs)\n",
        "        o = self.linear(y[:, :, -1])\n",
        "        return o\n"
      ],
      "metadata": {
        "id": "OLzRjIugabSh"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "name = 'hdfs_train'\n",
        "window_size = 10\n",
        "num_sessions = 0\n",
        "inputs = []\n",
        "outputs = []\n",
        "\n",
        "with open('/content/' + name, 'r') as f:\n",
        "        for row in f:\n",
        "            num_sessions += 1\n",
        "            line = [ (int(i)-1) for i in row.strip().split()]\n",
        "            for i in range(len(line) - window_size):\n",
        "                inputs.append(line[i:i + window_size])\n",
        "                outputs.append(line[i + window_size])\n",
        "\n",
        "print('Number of sessions({}): {}'.format(name, num_sessions))\n",
        "print('Number of seqs({}): {}'.format(name, len(inputs)))\n",
        "dataset = TensorDataset(torch.tensor(inputs, dtype=torch.float), torch.tensor(outputs))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J3W3rU_msfoV",
        "outputId": "2d4375dc-de41-4a91-99db-dbb73e05c0cb"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of sessions(hdfs_train): 4855\n",
            "Number of seqs(hdfs_train): 46575\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the TCN model and load pre-trained weights if available\n",
        "input_size = 1  # Assuming each log template is represented by a single number\n",
        "output_size = 28  # Number of classes in the one-hot encodings\n",
        "num_channels = [16, 32]\n",
        "kernel_size = 3\n",
        "dropout = 0.2\n",
        "batch_size = 2048\n",
        "num_epochs = 400\n",
        "\n",
        "model = TCN(input_size, output_size, num_channels, kernel_size, dropout).to(device)\n",
        "\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n"
      ],
      "metadata": {
        "id": "zq5EvBjotS-B"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def adjust_learning_rate(optimizer, epoch, lr_step=(300, 350), lr_decay_ratio=0.1):\n",
        "    \"\"\"Adjust the learning rate based on the epoch number.\"\"\"\n",
        "    if epoch == 0:\n",
        "        optimizer.param_groups[0]['lr'] /= 32\n",
        "    elif epoch in [2, 4, 6, 8, 10]:  # in step five , we finish warm up ,and start normal learning rate\n",
        "        optimizer.param_groups[0]['lr'] *= 2\n",
        "    if epoch in lr_step: # in these steps , we are geting close to optimal point so we need to have shorter step\n",
        "        optimizer.param_groups[0]['lr'] *= lr_decay_ratio\n",
        "    return optimizer\n",
        "\n",
        "# Define options here\n",
        "options = {\n",
        "    'lr': 0.001,\n",
        "    'lr_step': (300, 350), #steps(epoch) for updating learning rate\n",
        "    'lr_decay_ratio': 0.1,\n",
        "    # Add other options here\n",
        "}\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=options['lr'], betas=(0.9, 0.999))"
      ],
      "metadata": {
        "id": "7wv8vFTX4z_i"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "start_time = time.time()\n",
        "total_step = len(dataloader)\n",
        "for epoch in range(num_epochs):  # Loop over the dataset multiple times\n",
        "    optimizer = adjust_learning_rate(optimizer, epoch, options['lr_step'], options['lr_decay_ratio'])\n",
        "    print(optimizer.param_groups[0]['lr'])\n",
        "    train_loss = 0\n",
        "    for step, (seq, label) in enumerate(dataloader):\n",
        "        # Forward pass\n",
        "        seq = seq.clone().detach().view(-1, input_size, window_size).to(device)\n",
        "        output = model(seq)\n",
        "        loss = criterion(output, label.to(device))\n",
        "\n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        train_loss += loss.item()\n",
        "        optimizer.step()\n",
        "    print('Epoch [{}/{}], train_loss: {:.4f}'.format(epoch + 1, num_epochs, train_loss / total_step))\n",
        "elapsed_time = time.time() - start_time\n",
        "print('elapsed_time: {:.3f}s'.format(elapsed_time))\n",
        "print('Finished Training')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ku-o2J_t42m",
        "outputId": "3fd5ef54-8284-4700-af40-c3b9979c0479"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.125e-05\n",
            "Epoch [1/400], train_loss: 3.4702\n",
            "3.125e-05\n",
            "Epoch [2/400], train_loss: 3.3824\n",
            "6.25e-05\n",
            "Epoch [3/400], train_loss: 3.2660\n",
            "6.25e-05\n",
            "Epoch [4/400], train_loss: 3.1064\n",
            "0.000125\n",
            "Epoch [5/400], train_loss: 2.8130\n",
            "0.000125\n",
            "Epoch [6/400], train_loss: 2.4321\n",
            "0.00025\n",
            "Epoch [7/400], train_loss: 2.0935\n",
            "0.00025\n",
            "Epoch [8/400], train_loss: 1.8911\n",
            "0.0005\n",
            "Epoch [9/400], train_loss: 1.7364\n",
            "0.0005\n",
            "Epoch [10/400], train_loss: 1.5602\n",
            "0.001\n",
            "Epoch [11/400], train_loss: 1.3358\n",
            "0.001\n",
            "Epoch [12/400], train_loss: 1.1025\n",
            "0.001\n",
            "Epoch [13/400], train_loss: 0.9523\n",
            "0.001\n",
            "Epoch [14/400], train_loss: 0.8491\n",
            "0.001\n",
            "Epoch [15/400], train_loss: 0.7889\n",
            "0.001\n",
            "Epoch [16/400], train_loss: 0.7406\n",
            "0.001\n",
            "Epoch [17/400], train_loss: 0.7069\n",
            "0.001\n",
            "Epoch [18/400], train_loss: 0.6795\n",
            "0.001\n",
            "Epoch [19/400], train_loss: 0.6543\n",
            "0.001\n",
            "Epoch [20/400], train_loss: 0.6300\n",
            "0.001\n",
            "Epoch [21/400], train_loss: 0.6153\n",
            "0.001\n",
            "Epoch [22/400], train_loss: 0.6012\n",
            "0.001\n",
            "Epoch [23/400], train_loss: 0.5895\n",
            "0.001\n",
            "Epoch [24/400], train_loss: 0.5760\n",
            "0.001\n",
            "Epoch [25/400], train_loss: 0.5644\n",
            "0.001\n",
            "Epoch [26/400], train_loss: 0.5557\n",
            "0.001\n",
            "Epoch [27/400], train_loss: 0.5447\n",
            "0.001\n",
            "Epoch [28/400], train_loss: 0.5366\n",
            "0.001\n",
            "Epoch [29/400], train_loss: 0.5321\n",
            "0.001\n",
            "Epoch [30/400], train_loss: 0.5213\n",
            "0.001\n",
            "Epoch [31/400], train_loss: 0.5135\n",
            "0.001\n",
            "Epoch [32/400], train_loss: 0.5107\n",
            "0.001\n",
            "Epoch [33/400], train_loss: 0.5030\n",
            "0.001\n",
            "Epoch [34/400], train_loss: 0.4985\n",
            "0.001\n",
            "Epoch [35/400], train_loss: 0.4937\n",
            "0.001\n",
            "Epoch [36/400], train_loss: 0.4876\n",
            "0.001\n",
            "Epoch [37/400], train_loss: 0.4813\n",
            "0.001\n",
            "Epoch [38/400], train_loss: 0.4756\n",
            "0.001\n",
            "Epoch [39/400], train_loss: 0.4750\n",
            "0.001\n",
            "Epoch [40/400], train_loss: 0.4708\n",
            "0.001\n",
            "Epoch [41/400], train_loss: 0.4679\n",
            "0.001\n",
            "Epoch [42/400], train_loss: 0.4624\n",
            "0.001\n",
            "Epoch [43/400], train_loss: 0.4605\n",
            "0.001\n",
            "Epoch [44/400], train_loss: 0.4560\n",
            "0.001\n",
            "Epoch [45/400], train_loss: 0.4542\n",
            "0.001\n",
            "Epoch [46/400], train_loss: 0.4523\n",
            "0.001\n",
            "Epoch [47/400], train_loss: 0.4527\n",
            "0.001\n",
            "Epoch [48/400], train_loss: 0.4469\n",
            "0.001\n",
            "Epoch [49/400], train_loss: 0.4411\n",
            "0.001\n",
            "Epoch [50/400], train_loss: 0.4392\n",
            "0.001\n",
            "Epoch [51/400], train_loss: 0.4384\n",
            "0.001\n",
            "Epoch [52/400], train_loss: 0.4360\n",
            "0.001\n",
            "Epoch [53/400], train_loss: 0.4349\n",
            "0.001\n",
            "Epoch [54/400], train_loss: 0.4311\n",
            "0.001\n",
            "Epoch [55/400], train_loss: 0.4268\n",
            "0.001\n",
            "Epoch [56/400], train_loss: 0.4253\n",
            "0.001\n",
            "Epoch [57/400], train_loss: 0.4238\n",
            "0.001\n",
            "Epoch [58/400], train_loss: 0.4181\n",
            "0.001\n",
            "Epoch [59/400], train_loss: 0.4168\n",
            "0.001\n",
            "Epoch [60/400], train_loss: 0.4174\n",
            "0.001\n",
            "Epoch [61/400], train_loss: 0.4166\n",
            "0.001\n",
            "Epoch [62/400], train_loss: 0.4128\n",
            "0.001\n",
            "Epoch [63/400], train_loss: 0.4125\n",
            "0.001\n",
            "Epoch [64/400], train_loss: 0.4087\n",
            "0.001\n",
            "Epoch [65/400], train_loss: 0.4085\n",
            "0.001\n",
            "Epoch [66/400], train_loss: 0.4065\n",
            "0.001\n",
            "Epoch [67/400], train_loss: 0.4035\n",
            "0.001\n",
            "Epoch [68/400], train_loss: 0.4029\n",
            "0.001\n",
            "Epoch [69/400], train_loss: 0.3988\n",
            "0.001\n",
            "Epoch [70/400], train_loss: 0.3982\n",
            "0.001\n",
            "Epoch [71/400], train_loss: 0.3977\n",
            "0.001\n",
            "Epoch [72/400], train_loss: 0.3966\n",
            "0.001\n",
            "Epoch [73/400], train_loss: 0.3933\n",
            "0.001\n",
            "Epoch [74/400], train_loss: 0.3939\n",
            "0.001\n",
            "Epoch [75/400], train_loss: 0.3907\n",
            "0.001\n",
            "Epoch [76/400], train_loss: 0.3900\n",
            "0.001\n",
            "Epoch [77/400], train_loss: 0.3863\n",
            "0.001\n",
            "Epoch [78/400], train_loss: 0.3893\n",
            "0.001\n",
            "Epoch [79/400], train_loss: 0.3858\n",
            "0.001\n",
            "Epoch [80/400], train_loss: 0.3860\n",
            "0.001\n",
            "Epoch [81/400], train_loss: 0.3833\n",
            "0.001\n",
            "Epoch [82/400], train_loss: 0.3805\n",
            "0.001\n",
            "Epoch [83/400], train_loss: 0.3819\n",
            "0.001\n",
            "Epoch [84/400], train_loss: 0.3808\n",
            "0.001\n",
            "Epoch [85/400], train_loss: 0.3814\n",
            "0.001\n",
            "Epoch [86/400], train_loss: 0.3780\n",
            "0.001\n",
            "Epoch [87/400], train_loss: 0.3755\n",
            "0.001\n",
            "Epoch [88/400], train_loss: 0.3741\n",
            "0.001\n",
            "Epoch [89/400], train_loss: 0.3740\n",
            "0.001\n",
            "Epoch [90/400], train_loss: 0.3738\n",
            "0.001\n",
            "Epoch [91/400], train_loss: 0.3716\n",
            "0.001\n",
            "Epoch [92/400], train_loss: 0.3725\n",
            "0.001\n",
            "Epoch [93/400], train_loss: 0.3687\n",
            "0.001\n",
            "Epoch [94/400], train_loss: 0.3704\n",
            "0.001\n",
            "Epoch [95/400], train_loss: 0.3696\n",
            "0.001\n",
            "Epoch [96/400], train_loss: 0.3684\n",
            "0.001\n",
            "Epoch [97/400], train_loss: 0.3682\n",
            "0.001\n",
            "Epoch [98/400], train_loss: 0.3635\n",
            "0.001\n",
            "Epoch [99/400], train_loss: 0.3655\n",
            "0.001\n",
            "Epoch [100/400], train_loss: 0.3637\n",
            "0.001\n",
            "Epoch [101/400], train_loss: 0.3636\n",
            "0.001\n",
            "Epoch [102/400], train_loss: 0.3611\n",
            "0.001\n",
            "Epoch [103/400], train_loss: 0.3598\n",
            "0.001\n",
            "Epoch [104/400], train_loss: 0.3615\n",
            "0.001\n",
            "Epoch [105/400], train_loss: 0.3606\n",
            "0.001\n",
            "Epoch [106/400], train_loss: 0.3598\n",
            "0.001\n",
            "Epoch [107/400], train_loss: 0.3559\n",
            "0.001\n",
            "Epoch [108/400], train_loss: 0.3578\n",
            "0.001\n",
            "Epoch [109/400], train_loss: 0.3546\n",
            "0.001\n",
            "Epoch [110/400], train_loss: 0.3548\n",
            "0.001\n",
            "Epoch [111/400], train_loss: 0.3540\n",
            "0.001\n",
            "Epoch [112/400], train_loss: 0.3513\n",
            "0.001\n",
            "Epoch [113/400], train_loss: 0.3530\n",
            "0.001\n",
            "Epoch [114/400], train_loss: 0.3510\n",
            "0.001\n",
            "Epoch [115/400], train_loss: 0.3513\n",
            "0.001\n",
            "Epoch [116/400], train_loss: 0.3507\n",
            "0.001\n",
            "Epoch [117/400], train_loss: 0.3515\n",
            "0.001\n",
            "Epoch [118/400], train_loss: 0.3493\n",
            "0.001\n",
            "Epoch [119/400], train_loss: 0.3479\n",
            "0.001\n",
            "Epoch [120/400], train_loss: 0.3463\n",
            "0.001\n",
            "Epoch [121/400], train_loss: 0.3454\n",
            "0.001\n",
            "Epoch [122/400], train_loss: 0.3457\n",
            "0.001\n",
            "Epoch [123/400], train_loss: 0.3447\n",
            "0.001\n",
            "Epoch [124/400], train_loss: 0.3442\n",
            "0.001\n",
            "Epoch [125/400], train_loss: 0.3446\n",
            "0.001\n",
            "Epoch [126/400], train_loss: 0.3424\n",
            "0.001\n",
            "Epoch [127/400], train_loss: 0.3415\n",
            "0.001\n",
            "Epoch [128/400], train_loss: 0.3428\n",
            "0.001\n",
            "Epoch [129/400], train_loss: 0.3418\n",
            "0.001\n",
            "Epoch [130/400], train_loss: 0.3409\n",
            "0.001\n",
            "Epoch [131/400], train_loss: 0.3390\n",
            "0.001\n",
            "Epoch [132/400], train_loss: 0.3393\n",
            "0.001\n",
            "Epoch [133/400], train_loss: 0.3375\n",
            "0.001\n",
            "Epoch [134/400], train_loss: 0.3386\n",
            "0.001\n",
            "Epoch [135/400], train_loss: 0.3367\n",
            "0.001\n",
            "Epoch [136/400], train_loss: 0.3369\n",
            "0.001\n",
            "Epoch [137/400], train_loss: 0.3341\n",
            "0.001\n",
            "Epoch [138/400], train_loss: 0.3356\n",
            "0.001\n",
            "Epoch [139/400], train_loss: 0.3333\n",
            "0.001\n",
            "Epoch [140/400], train_loss: 0.3362\n",
            "0.001\n",
            "Epoch [141/400], train_loss: 0.3335\n",
            "0.001\n",
            "Epoch [142/400], train_loss: 0.3317\n",
            "0.001\n",
            "Epoch [143/400], train_loss: 0.3322\n",
            "0.001\n",
            "Epoch [144/400], train_loss: 0.3315\n",
            "0.001\n",
            "Epoch [145/400], train_loss: 0.3307\n",
            "0.001\n",
            "Epoch [146/400], train_loss: 0.3300\n",
            "0.001\n",
            "Epoch [147/400], train_loss: 0.3297\n",
            "0.001\n",
            "Epoch [148/400], train_loss: 0.3314\n",
            "0.001\n",
            "Epoch [149/400], train_loss: 0.3320\n",
            "0.001\n",
            "Epoch [150/400], train_loss: 0.3279\n",
            "0.001\n",
            "Epoch [151/400], train_loss: 0.3297\n",
            "0.001\n",
            "Epoch [152/400], train_loss: 0.3290\n",
            "0.001\n",
            "Epoch [153/400], train_loss: 0.3263\n",
            "0.001\n",
            "Epoch [154/400], train_loss: 0.3241\n",
            "0.001\n",
            "Epoch [155/400], train_loss: 0.3282\n",
            "0.001\n",
            "Epoch [156/400], train_loss: 0.3261\n",
            "0.001\n",
            "Epoch [157/400], train_loss: 0.3261\n",
            "0.001\n",
            "Epoch [158/400], train_loss: 0.3239\n",
            "0.001\n",
            "Epoch [159/400], train_loss: 0.3259\n",
            "0.001\n",
            "Epoch [160/400], train_loss: 0.3249\n",
            "0.001\n",
            "Epoch [161/400], train_loss: 0.3250\n",
            "0.001\n",
            "Epoch [162/400], train_loss: 0.3222\n",
            "0.001\n",
            "Epoch [163/400], train_loss: 0.3225\n",
            "0.001\n",
            "Epoch [164/400], train_loss: 0.3233\n",
            "0.001\n",
            "Epoch [165/400], train_loss: 0.3202\n",
            "0.001\n",
            "Epoch [166/400], train_loss: 0.3205\n",
            "0.001\n",
            "Epoch [167/400], train_loss: 0.3185\n",
            "0.001\n",
            "Epoch [168/400], train_loss: 0.3203\n",
            "0.001\n",
            "Epoch [169/400], train_loss: 0.3180\n",
            "0.001\n",
            "Epoch [170/400], train_loss: 0.3208\n",
            "0.001\n",
            "Epoch [171/400], train_loss: 0.3174\n",
            "0.001\n",
            "Epoch [172/400], train_loss: 0.3172\n",
            "0.001\n",
            "Epoch [173/400], train_loss: 0.3171\n",
            "0.001\n",
            "Epoch [174/400], train_loss: 0.3175\n",
            "0.001\n",
            "Epoch [175/400], train_loss: 0.3192\n",
            "0.001\n",
            "Epoch [176/400], train_loss: 0.3167\n",
            "0.001\n",
            "Epoch [177/400], train_loss: 0.3159\n",
            "0.001\n",
            "Epoch [178/400], train_loss: 0.3165\n",
            "0.001\n",
            "Epoch [179/400], train_loss: 0.3158\n",
            "0.001\n",
            "Epoch [180/400], train_loss: 0.3134\n",
            "0.001\n",
            "Epoch [181/400], train_loss: 0.3154\n",
            "0.001\n",
            "Epoch [182/400], train_loss: 0.3151\n",
            "0.001\n",
            "Epoch [183/400], train_loss: 0.3132\n",
            "0.001\n",
            "Epoch [184/400], train_loss: 0.3123\n",
            "0.001\n",
            "Epoch [185/400], train_loss: 0.3131\n",
            "0.001\n",
            "Epoch [186/400], train_loss: 0.3114\n",
            "0.001\n",
            "Epoch [187/400], train_loss: 0.3134\n",
            "0.001\n",
            "Epoch [188/400], train_loss: 0.3121\n",
            "0.001\n",
            "Epoch [189/400], train_loss: 0.3103\n",
            "0.001\n",
            "Epoch [190/400], train_loss: 0.3112\n",
            "0.001\n",
            "Epoch [191/400], train_loss: 0.3099\n",
            "0.001\n",
            "Epoch [192/400], train_loss: 0.3107\n",
            "0.001\n",
            "Epoch [193/400], train_loss: 0.3104\n",
            "0.001\n",
            "Epoch [194/400], train_loss: 0.3077\n",
            "0.001\n",
            "Epoch [195/400], train_loss: 0.3092\n",
            "0.001\n",
            "Epoch [196/400], train_loss: 0.3080\n",
            "0.001\n",
            "Epoch [197/400], train_loss: 0.3060\n",
            "0.001\n",
            "Epoch [198/400], train_loss: 0.3060\n",
            "0.001\n",
            "Epoch [199/400], train_loss: 0.3081\n",
            "0.001\n",
            "Epoch [200/400], train_loss: 0.3058\n",
            "0.001\n",
            "Epoch [201/400], train_loss: 0.3055\n",
            "0.001\n",
            "Epoch [202/400], train_loss: 0.3043\n",
            "0.001\n",
            "Epoch [203/400], train_loss: 0.3027\n",
            "0.001\n",
            "Epoch [204/400], train_loss: 0.3047\n",
            "0.001\n",
            "Epoch [205/400], train_loss: 0.3054\n",
            "0.001\n",
            "Epoch [206/400], train_loss: 0.3027\n",
            "0.001\n",
            "Epoch [207/400], train_loss: 0.3056\n",
            "0.001\n",
            "Epoch [208/400], train_loss: 0.3019\n",
            "0.001\n",
            "Epoch [209/400], train_loss: 0.3031\n",
            "0.001\n",
            "Epoch [210/400], train_loss: 0.3055\n",
            "0.001\n",
            "Epoch [211/400], train_loss: 0.3049\n",
            "0.001\n",
            "Epoch [212/400], train_loss: 0.3026\n",
            "0.001\n",
            "Epoch [213/400], train_loss: 0.3022\n",
            "0.001\n",
            "Epoch [214/400], train_loss: 0.3013\n",
            "0.001\n",
            "Epoch [215/400], train_loss: 0.3021\n",
            "0.001\n",
            "Epoch [216/400], train_loss: 0.3004\n",
            "0.001\n",
            "Epoch [217/400], train_loss: 0.3028\n",
            "0.001\n",
            "Epoch [218/400], train_loss: 0.3008\n",
            "0.001\n",
            "Epoch [219/400], train_loss: 0.2989\n",
            "0.001\n",
            "Epoch [220/400], train_loss: 0.3040\n",
            "0.001\n",
            "Epoch [221/400], train_loss: 0.2989\n",
            "0.001\n",
            "Epoch [222/400], train_loss: 0.3030\n",
            "0.001\n",
            "Epoch [223/400], train_loss: 0.2995\n",
            "0.001\n",
            "Epoch [224/400], train_loss: 0.3013\n",
            "0.001\n",
            "Epoch [225/400], train_loss: 0.2988\n",
            "0.001\n",
            "Epoch [226/400], train_loss: 0.2965\n",
            "0.001\n",
            "Epoch [227/400], train_loss: 0.2991\n",
            "0.001\n",
            "Epoch [228/400], train_loss: 0.2992\n",
            "0.001\n",
            "Epoch [229/400], train_loss: 0.2996\n",
            "0.001\n",
            "Epoch [230/400], train_loss: 0.2968\n",
            "0.001\n",
            "Epoch [231/400], train_loss: 0.2995\n",
            "0.001\n",
            "Epoch [232/400], train_loss: 0.2950\n",
            "0.001\n",
            "Epoch [233/400], train_loss: 0.2966\n",
            "0.001\n",
            "Epoch [234/400], train_loss: 0.2978\n",
            "0.001\n",
            "Epoch [235/400], train_loss: 0.2939\n",
            "0.001\n",
            "Epoch [236/400], train_loss: 0.2951\n",
            "0.001\n",
            "Epoch [237/400], train_loss: 0.2957\n",
            "0.001\n",
            "Epoch [238/400], train_loss: 0.2945\n",
            "0.001\n",
            "Epoch [239/400], train_loss: 0.2953\n",
            "0.001\n",
            "Epoch [240/400], train_loss: 0.2943\n",
            "0.001\n",
            "Epoch [241/400], train_loss: 0.2941\n",
            "0.001\n",
            "Epoch [242/400], train_loss: 0.2939\n",
            "0.001\n",
            "Epoch [243/400], train_loss: 0.2952\n",
            "0.001\n",
            "Epoch [244/400], train_loss: 0.2926\n",
            "0.001\n",
            "Epoch [245/400], train_loss: 0.2954\n",
            "0.001\n",
            "Epoch [246/400], train_loss: 0.2923\n",
            "0.001\n",
            "Epoch [247/400], train_loss: 0.2920\n",
            "0.001\n",
            "Epoch [248/400], train_loss: 0.2914\n",
            "0.001\n",
            "Epoch [249/400], train_loss: 0.2915\n",
            "0.001\n",
            "Epoch [250/400], train_loss: 0.2924\n",
            "0.001\n",
            "Epoch [251/400], train_loss: 0.2908\n",
            "0.001\n",
            "Epoch [252/400], train_loss: 0.2913\n",
            "0.001\n",
            "Epoch [253/400], train_loss: 0.2927\n",
            "0.001\n",
            "Epoch [254/400], train_loss: 0.2917\n",
            "0.001\n",
            "Epoch [255/400], train_loss: 0.2908\n",
            "0.001\n",
            "Epoch [256/400], train_loss: 0.2926\n",
            "0.001\n",
            "Epoch [257/400], train_loss: 0.2905\n",
            "0.001\n",
            "Epoch [258/400], train_loss: 0.2898\n",
            "0.001\n",
            "Epoch [259/400], train_loss: 0.2902\n",
            "0.001\n",
            "Epoch [260/400], train_loss: 0.2913\n",
            "0.001\n",
            "Epoch [261/400], train_loss: 0.2896\n",
            "0.001\n",
            "Epoch [262/400], train_loss: 0.2875\n",
            "0.001\n",
            "Epoch [263/400], train_loss: 0.2891\n",
            "0.001\n",
            "Epoch [264/400], train_loss: 0.2860\n",
            "0.001\n",
            "Epoch [265/400], train_loss: 0.2894\n",
            "0.001\n",
            "Epoch [266/400], train_loss: 0.2873\n",
            "0.001\n",
            "Epoch [267/400], train_loss: 0.2906\n",
            "0.001\n",
            "Epoch [268/400], train_loss: 0.2878\n",
            "0.001\n",
            "Epoch [269/400], train_loss: 0.2897\n",
            "0.001\n",
            "Epoch [270/400], train_loss: 0.2889\n",
            "0.001\n",
            "Epoch [271/400], train_loss: 0.2868\n",
            "0.001\n",
            "Epoch [272/400], train_loss: 0.2888\n",
            "0.001\n",
            "Epoch [273/400], train_loss: 0.2864\n",
            "0.001\n",
            "Epoch [274/400], train_loss: 0.2873\n",
            "0.001\n",
            "Epoch [275/400], train_loss: 0.2881\n",
            "0.001\n",
            "Epoch [276/400], train_loss: 0.2863\n",
            "0.001\n",
            "Epoch [277/400], train_loss: 0.2874\n",
            "0.001\n",
            "Epoch [278/400], train_loss: 0.2868\n",
            "0.001\n",
            "Epoch [279/400], train_loss: 0.2855\n",
            "0.001\n",
            "Epoch [280/400], train_loss: 0.2847\n",
            "0.001\n",
            "Epoch [281/400], train_loss: 0.2842\n",
            "0.001\n",
            "Epoch [282/400], train_loss: 0.2842\n",
            "0.001\n",
            "Epoch [283/400], train_loss: 0.2830\n",
            "0.001\n",
            "Epoch [284/400], train_loss: 0.2862\n",
            "0.001\n",
            "Epoch [285/400], train_loss: 0.2857\n",
            "0.001\n",
            "Epoch [286/400], train_loss: 0.2856\n",
            "0.001\n",
            "Epoch [287/400], train_loss: 0.2859\n",
            "0.001\n",
            "Epoch [288/400], train_loss: 0.2858\n",
            "0.001\n",
            "Epoch [289/400], train_loss: 0.2838\n",
            "0.001\n",
            "Epoch [290/400], train_loss: 0.2836\n",
            "0.001\n",
            "Epoch [291/400], train_loss: 0.2855\n",
            "0.001\n",
            "Epoch [292/400], train_loss: 0.2837\n",
            "0.001\n",
            "Epoch [293/400], train_loss: 0.2813\n",
            "0.001\n",
            "Epoch [294/400], train_loss: 0.2833\n",
            "0.001\n",
            "Epoch [295/400], train_loss: 0.2843\n",
            "0.001\n",
            "Epoch [296/400], train_loss: 0.2824\n",
            "0.001\n",
            "Epoch [297/400], train_loss: 0.2827\n",
            "0.001\n",
            "Epoch [298/400], train_loss: 0.2831\n",
            "0.001\n",
            "Epoch [299/400], train_loss: 0.2833\n",
            "0.001\n",
            "Epoch [300/400], train_loss: 0.2828\n",
            "0.0001\n",
            "Epoch [301/400], train_loss: 0.2776\n",
            "0.0001\n",
            "Epoch [302/400], train_loss: 0.2792\n",
            "0.0001\n",
            "Epoch [303/400], train_loss: 0.2775\n",
            "0.0001\n",
            "Epoch [304/400], train_loss: 0.2799\n",
            "0.0001\n",
            "Epoch [305/400], train_loss: 0.2785\n",
            "0.0001\n",
            "Epoch [306/400], train_loss: 0.2767\n",
            "0.0001\n",
            "Epoch [307/400], train_loss: 0.2772\n",
            "0.0001\n",
            "Epoch [308/400], train_loss: 0.2787\n",
            "0.0001\n",
            "Epoch [309/400], train_loss: 0.2791\n",
            "0.0001\n",
            "Epoch [310/400], train_loss: 0.2784\n",
            "0.0001\n",
            "Epoch [311/400], train_loss: 0.2792\n",
            "0.0001\n",
            "Epoch [312/400], train_loss: 0.2774\n",
            "0.0001\n",
            "Epoch [313/400], train_loss: 0.2762\n",
            "0.0001\n",
            "Epoch [314/400], train_loss: 0.2777\n",
            "0.0001\n",
            "Epoch [315/400], train_loss: 0.2775\n",
            "0.0001\n",
            "Epoch [316/400], train_loss: 0.2768\n",
            "0.0001\n",
            "Epoch [317/400], train_loss: 0.2767\n",
            "0.0001\n",
            "Epoch [318/400], train_loss: 0.2773\n",
            "0.0001\n",
            "Epoch [319/400], train_loss: 0.2787\n",
            "0.0001\n",
            "Epoch [320/400], train_loss: 0.2766\n",
            "0.0001\n",
            "Epoch [321/400], train_loss: 0.2780\n",
            "0.0001\n",
            "Epoch [322/400], train_loss: 0.2763\n",
            "0.0001\n",
            "Epoch [323/400], train_loss: 0.2769\n",
            "0.0001\n",
            "Epoch [324/400], train_loss: 0.2761\n",
            "0.0001\n",
            "Epoch [325/400], train_loss: 0.2779\n",
            "0.0001\n",
            "Epoch [326/400], train_loss: 0.2768\n",
            "0.0001\n",
            "Epoch [327/400], train_loss: 0.2764\n",
            "0.0001\n",
            "Epoch [328/400], train_loss: 0.2771\n",
            "0.0001\n",
            "Epoch [329/400], train_loss: 0.2772\n",
            "0.0001\n",
            "Epoch [330/400], train_loss: 0.2774\n",
            "0.0001\n",
            "Epoch [331/400], train_loss: 0.2783\n",
            "0.0001\n",
            "Epoch [332/400], train_loss: 0.2757\n",
            "0.0001\n",
            "Epoch [333/400], train_loss: 0.2776\n",
            "0.0001\n",
            "Epoch [334/400], train_loss: 0.2763\n",
            "0.0001\n",
            "Epoch [335/400], train_loss: 0.2769\n",
            "0.0001\n",
            "Epoch [336/400], train_loss: 0.2775\n",
            "0.0001\n",
            "Epoch [337/400], train_loss: 0.2754\n",
            "0.0001\n",
            "Epoch [338/400], train_loss: 0.2766\n",
            "0.0001\n",
            "Epoch [339/400], train_loss: 0.2778\n",
            "0.0001\n",
            "Epoch [340/400], train_loss: 0.2788\n",
            "0.0001\n",
            "Epoch [341/400], train_loss: 0.2763\n",
            "0.0001\n",
            "Epoch [342/400], train_loss: 0.2767\n",
            "0.0001\n",
            "Epoch [343/400], train_loss: 0.2756\n",
            "0.0001\n",
            "Epoch [344/400], train_loss: 0.2757\n",
            "0.0001\n",
            "Epoch [345/400], train_loss: 0.2771\n",
            "0.0001\n",
            "Epoch [346/400], train_loss: 0.2751\n",
            "0.0001\n",
            "Epoch [347/400], train_loss: 0.2746\n",
            "0.0001\n",
            "Epoch [348/400], train_loss: 0.2762\n",
            "0.0001\n",
            "Epoch [349/400], train_loss: 0.2778\n",
            "0.0001\n",
            "Epoch [350/400], train_loss: 0.2752\n",
            "1e-05\n",
            "Epoch [351/400], train_loss: 0.2790\n",
            "1e-05\n",
            "Epoch [352/400], train_loss: 0.2768\n",
            "1e-05\n",
            "Epoch [353/400], train_loss: 0.2762\n",
            "1e-05\n",
            "Epoch [354/400], train_loss: 0.2778\n",
            "1e-05\n",
            "Epoch [355/400], train_loss: 0.2747\n",
            "1e-05\n",
            "Epoch [356/400], train_loss: 0.2755\n",
            "1e-05\n",
            "Epoch [357/400], train_loss: 0.2751\n",
            "1e-05\n",
            "Epoch [358/400], train_loss: 0.2782\n",
            "1e-05\n",
            "Epoch [359/400], train_loss: 0.2751\n",
            "1e-05\n",
            "Epoch [360/400], train_loss: 0.2757\n",
            "1e-05\n",
            "Epoch [361/400], train_loss: 0.2750\n",
            "1e-05\n",
            "Epoch [362/400], train_loss: 0.2753\n",
            "1e-05\n",
            "Epoch [363/400], train_loss: 0.2759\n",
            "1e-05\n",
            "Epoch [364/400], train_loss: 0.2765\n",
            "1e-05\n",
            "Epoch [365/400], train_loss: 0.2749\n",
            "1e-05\n",
            "Epoch [366/400], train_loss: 0.2752\n",
            "1e-05\n",
            "Epoch [367/400], train_loss: 0.2750\n",
            "1e-05\n",
            "Epoch [368/400], train_loss: 0.2748\n",
            "1e-05\n",
            "Epoch [369/400], train_loss: 0.2765\n",
            "1e-05\n",
            "Epoch [370/400], train_loss: 0.2748\n",
            "1e-05\n",
            "Epoch [371/400], train_loss: 0.2759\n",
            "1e-05\n",
            "Epoch [372/400], train_loss: 0.2769\n",
            "1e-05\n",
            "Epoch [373/400], train_loss: 0.2750\n",
            "1e-05\n",
            "Epoch [374/400], train_loss: 0.2760\n",
            "1e-05\n",
            "Epoch [375/400], train_loss: 0.2757\n",
            "1e-05\n",
            "Epoch [376/400], train_loss: 0.2763\n",
            "1e-05\n",
            "Epoch [377/400], train_loss: 0.2767\n",
            "1e-05\n",
            "Epoch [378/400], train_loss: 0.2749\n",
            "1e-05\n",
            "Epoch [379/400], train_loss: 0.2756\n",
            "1e-05\n",
            "Epoch [380/400], train_loss: 0.2765\n",
            "1e-05\n",
            "Epoch [381/400], train_loss: 0.2761\n",
            "1e-05\n",
            "Epoch [382/400], train_loss: 0.2759\n",
            "1e-05\n",
            "Epoch [383/400], train_loss: 0.2752\n",
            "1e-05\n",
            "Epoch [384/400], train_loss: 0.2750\n",
            "1e-05\n",
            "Epoch [385/400], train_loss: 0.2776\n",
            "1e-05\n",
            "Epoch [386/400], train_loss: 0.2790\n",
            "1e-05\n",
            "Epoch [387/400], train_loss: 0.2766\n",
            "1e-05\n",
            "Epoch [388/400], train_loss: 0.2758\n",
            "1e-05\n",
            "Epoch [389/400], train_loss: 0.2744\n",
            "1e-05\n",
            "Epoch [390/400], train_loss: 0.2766\n",
            "1e-05\n",
            "Epoch [391/400], train_loss: 0.2752\n",
            "1e-05\n",
            "Epoch [392/400], train_loss: 0.2765\n",
            "1e-05\n",
            "Epoch [393/400], train_loss: 0.2765\n",
            "1e-05\n",
            "Epoch [394/400], train_loss: 0.2757\n",
            "1e-05\n",
            "Epoch [395/400], train_loss: 0.2764\n",
            "1e-05\n",
            "Epoch [396/400], train_loss: 0.2760\n",
            "1e-05\n",
            "Epoch [397/400], train_loss: 0.2746\n",
            "1e-05\n",
            "Epoch [398/400], train_loss: 0.2726\n",
            "1e-05\n",
            "Epoch [399/400], train_loss: 0.2752\n",
            "1e-05\n",
            "Epoch [400/400], train_loss: 0.2763\n",
            "elapsed_time: 245.697s\n",
            "Finished Training\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate(name):\n",
        "    window_size = 10\n",
        "    hdfs = {} #store the unique sequences and their counts.\n",
        "    length = 0\n",
        "    with open('/content/' + name, 'r') as f:\n",
        "        for ln in f.readlines():\n",
        "            ln = [(int(i)-1) for i in ln.strip().split()]\n",
        "            ln = ln + [-1] * (window_size + 1 - len(ln))     #ensure that all sequences have a fixed length of window_size + 1, even if the original line had fewer elements.\n",
        "            hdfs[tuple(ln)] = hdfs.get(tuple(ln), 0) + 1   #If the tuple is not present in the dictionary, hdfs.get(tuple(ln), 0) returns 0, and the code initializes the count to 1.\n",
        "            length += 1\n",
        "    print('Number of sessions({}): {}'.format(name, len(hdfs)))\n",
        "    return hdfs, length\n"
      ],
      "metadata": {
        "id": "hvq7Ivt3w0EY"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_normal_loader, test_normal_length = generate('hdfs_test_normal')\n",
        "test_abnormal_loader, test_abnormal_length = generate('hdfs_test_abnormal')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dUaWdas_w2Yg",
        "outputId": "fb0f8fc0-610f-493f-a355-7019d51b2df6"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of sessions(hdfs_test_normal): 14177\n",
            "Number of sessions(hdfs_test_abnormal): 4123\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_candidates = 9"
      ],
      "metadata": {
        "id": "W-hWDi2kxXQi"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Test the model\n",
        "model.eval()\n",
        "\n",
        "TP = 0\n",
        "FP = 0\n",
        "\n",
        "start_time = time.time()\n",
        "with torch.no_grad():\n",
        "    for line in tqdm(test_normal_loader.keys()):\n",
        "        for i in range(len(line) - window_size):\n",
        "            seq = line[i:i + window_size]\n",
        "            label = line[i + window_size]\n",
        "            seq = torch.tensor(seq, dtype=torch.float).view(-1, input_size, window_size).to(device)\n",
        "            label = torch.tensor(label).view(-1).to(device)\n",
        "            output = model(seq)\n",
        "            predicted = torch.argsort(output, 1)[0][-num_candidates:]\n",
        "            if label not in predicted:\n",
        "                FP += test_normal_loader[line] # numbers of that set we have\n",
        "                break   #with just one wrong prediction in a line , we assume , abnormal\n",
        "with torch.no_grad():\n",
        "    for line in tqdm(test_abnormal_loader.keys()):\n",
        "        for i in range(len(line) - window_size):\n",
        "            seq = line[i:i + window_size]\n",
        "            label = line[i + window_size]\n",
        "            seq = torch.tensor(seq, dtype=torch.float).view(-1, input_size, window_size).to(device)\n",
        "            label = torch.tensor(label).view(-1).to(device)\n",
        "            output = model(seq)\n",
        "            predicted = torch.argsort(output, 1)[0][-num_candidates:]\n",
        "            if label not in predicted:\n",
        "                TP += test_abnormal_loader[line]\n",
        "                break\n",
        "elapsed_time = time.time() - start_time\n",
        "print('elapsed_time: {:.3f}s'.format(elapsed_time))\n",
        "# Compute precision, recall and F1-measure\n",
        "FN = test_abnormal_length - TP\n",
        "P = 100 * TP / (TP + FP)\n",
        "R = 100 * TP / (TP + FN)\n",
        "F1 = 2 * P * R / (P + R)\n",
        "print('false positive (FP): {}, false negative (FN): {}, Precision: {:.3f}%, Recall: {:.3f}%, F1-measure: {:.3f}%'.format(FP, FN, P, R, F1))\n",
        "print('Finished Predicting')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5SsulX0iw6GI",
        "outputId": "8194893e-252a-4508-91ab-eb2d95962b2e"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 14177/14177 [05:01<00:00, 47.03it/s]\n",
            "100%|██████████| 4123/4123 [00:53<00:00, 77.51it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "elapsed_time: 354.639s\n",
            "false positive (FP): 1134, false negative (FN): 795, Precision: 93.398%, Recall: 95.279%, F1-measure: 94.329%\n",
            "Finished Predicting\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ]
}