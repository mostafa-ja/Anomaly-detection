{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPWux1m2FDSBoqFEmrkdyE+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mostafa-ja/Anomaly-detection/blob/main/LSTM1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Z7U4p6skT_u",
        "outputId": "81e2a394-20e9-4a93-8393-14aa4fb36eb0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-07-19 16:39:14--  https://raw.githubusercontent.com/donglee-afar/logdeep/master/data/hdfs/hdfs_train\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.111.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 257875 (252K) [text/plain]\n",
            "Saving to: ‘hdfs_train’\n",
            "\n",
            "hdfs_train          100%[===================>] 251.83K  --.-KB/s    in 0.03s   \n",
            "\n",
            "2023-07-19 16:39:14 (7.85 MB/s) - ‘hdfs_train’ saved [257875/257875]\n",
            "\n",
            "--2023-07-19 16:39:14--  https://raw.githubusercontent.com/donglee-afar/logdeep/master/data/hdfs/hdfs_test_normal\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.108.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 29284282 (28M) [text/plain]\n",
            "Saving to: ‘hdfs_test_normal’\n",
            "\n",
            "hdfs_test_normal    100%[===================>]  27.93M   175MB/s    in 0.2s    \n",
            "\n",
            "2023-07-19 16:39:15 (175 MB/s) - ‘hdfs_test_normal’ saved [29284282/29284282]\n",
            "\n",
            "--2023-07-19 16:39:15--  https://raw.githubusercontent.com/donglee-afar/logdeep/master/data/hdfs/hdfs_test_abnormal\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 782479 (764K) [text/plain]\n",
            "Saving to: ‘hdfs_test_abnormal’\n",
            "\n",
            "hdfs_test_abnormal  100%[===================>] 764.14K  --.-KB/s    in 0.04s   \n",
            "\n",
            "2023-07-19 16:39:15 (17.2 MB/s) - ‘hdfs_test_abnormal’ saved [782479/782479]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# download datasets\n",
        "!wget 'https://raw.githubusercontent.com/donglee-afar/logdeep/master/data/hdfs/hdfs_train'\n",
        "!wget 'https://raw.githubusercontent.com/donglee-afar/logdeep/master/data/hdfs/hdfs_test_normal'\n",
        "!wget 'https://raw.githubusercontent.com/donglee-afar/logdeep/master/data/hdfs/hdfs_test_abnormal'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "rNI34FVblAMn"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "name = 'hdfs_train'\n",
        "window_size = 10\n",
        "num_sessions = 0\n",
        "inputs = []\n",
        "outputs = []\n",
        "\n",
        "with open('/content/' + name, 'r') as f:\n",
        "        for row in f:\n",
        "            num_sessions += 1\n",
        "            line = [int(i) - 1 for i in row.strip().split()] # we substract by one from templates index for starting from zero\n",
        "            for i in range(len(line) - window_size):\n",
        "                inputs.append(line[i:i + window_size])\n",
        "                outputs.append(line[i + window_size])\n",
        "\n",
        "print('Number of sessions({}): {}'.format(name, num_sessions))\n",
        "print('Number of seqs({}): {}'.format(name, len(inputs)))\n",
        "dataset = TensorDataset(torch.tensor(inputs, dtype=torch.float), torch.tensor(outputs))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yhe9ibmOlHwz",
        "outputId": "6efab5ff-c64f-47ee-a35b-2d98b4ba23a8"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of sessions(hdfs_test_abnormal): 16838\n",
            "Number of seqs(hdfs_test_abnormal): 162697\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(inputs[15:18])\n",
        "print(outputs[15:18])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RlIV5C7OlWKp",
        "outputId": "a66ed90b-3c67-4282-89ba-907747503e46"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[4, 25, 25, 25, 10, 8, 10, 8, 10, 8], [25, 25, 25, 10, 8, 10, 8, 10, 8, 1], [25, 25, 10, 8, 10, 8, 10, 8, 1, 2]]\n",
            "[1, 2, 22]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# unique outputs(templates) in training dataset\n",
        "print(set(outputs))\n",
        "len(set(outputs))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hQTdIVVylxJA",
        "outputId": "eee0e7bf-7436-4b34-ba55-100517c8fbe0"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{1, 2, 3, 4, 5, 8, 10, 15, 17, 19, 20, 21, 22, 24, 25, 26, 27}\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "17"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To represent a single template, we use a “one-hot vector” of\n",
        "size <1 x n_templates>. A one-hot vector is filled with 0s\n",
        "except for a 1 at index of the current template, e.g. \"b\" = <0 1 0 0 0 ...>."
      ],
      "metadata": {
        "id": "fF0QOqUnow5x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "n_templates = 28 #total number of unique templates\n",
        "\n",
        "def template_to_tensor(template):\n",
        "    tensor = [0] * n_templates\n",
        "    if template != -1 :    # we put template= -1 for session less than windows_size in empty part and it will be all zero\n",
        "      tensor[template] = 1\n",
        "    return tensor"
      ],
      "metadata": {
        "id": "kq5LGA7NrsgN"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(template_to_tensor(2))\n",
        "print(len(template_to_tensor(2)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yomezvzvtZSz",
        "outputId": "9bc945cc-d8d3-4673-b434-505dbb7a657f"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "28\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(template_to_tensor(-1))\n",
        "print(len(template_to_tensor(-1)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BiyIDlZ7KQOL",
        "outputId": "a868bf97-da39-4cb8-f545-a3d0b9e4034f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "28\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "name = 'hdfs_train'\n",
        "window_size = 10\n",
        "num_sessions = 0\n",
        "inputs = []\n",
        "outputs = []\n",
        "\n",
        "with open('/content/' + name, 'r') as f:\n",
        "        for row in f:\n",
        "            num_sessions += 1\n",
        "            line = [template_to_tensor(int(i) - 1) for i in row.strip().split()] # we substract by one from templates index for starting from zero\n",
        "            for i in range(len(line) - window_size):\n",
        "                inputs.append(line[i:i + window_size])\n",
        "                outputs.append(line[i + window_size])\n",
        "\n",
        "print('Number of sessions({}): {}'.format(name, num_sessions))\n",
        "print('Number of seqs({}): {}'.format(name, len(inputs)))\n",
        "dataset = TensorDataset(torch.tensor(inputs, dtype=torch.float), torch.tensor(outputs,dtype=torch.float ))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2EMsKC8NthzK",
        "outputId": "e5a5d9fe-9e21-464d-8c2e-c50dd28cc87d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Exception ignored in: <function _xla_gc_callback at 0x7ca8d211c8b0>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/jax/_src/lib/__init__.py\", line 103, in _xla_gc_callback\n",
            "    def _xla_gc_callback(*args):\n",
            "KeyboardInterrupt: \n",
            "Exception ignored in: <function _xla_gc_callback at 0x7ca8d211c8b0>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/jax/_src/lib/__init__.py\", line 103, in _xla_gc_callback\n",
            "    def _xla_gc_callback(*args):\n",
            "KeyboardInterrupt: \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(inputs[15])\n",
        "print(outputs[15])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U1rakG_AtsOW",
        "outputId": "9fda5b11-b506-4cd2-d8ed-8e92f0ef0271"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
            "[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Model(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
        "        super(Model, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
        "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
        "        out, _ = self.lstm(x, (h0, c0))  # out.shape : [batch_size, sequence_length, hidden_size]\n",
        "        out = self.fc(out[:, -1, :]) #The : before , -1, : indicates that we want to include all elements along the first dimension (batch dimension). -1 represents the index of the last element along the second dimension (sequence length). : after , -1 indicates that we want to include all elements along the third dimension (hidden size)\n",
        "        return out"
      ],
      "metadata": {
        "id": "Pck9q_A7uV47"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_size = 28 # means each input has one feature(template's code)\n",
        "num_layers = 2\n",
        "hidden_size = 64\n",
        "num_classes = 28\n",
        "batch_size = 2048\n",
        "num_epochs = 300\n",
        "model_dir = 'model'"
      ],
      "metadata": {
        "id": "fbnCcSOiudxf"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "log = 'Adam_batch_size={}_epoch={}'.format(str(batch_size), str(num_epochs))\n",
        "writer = SummaryWriter(log_dir='log/' + log)\n",
        "\n",
        "\n",
        "model = Model(input_size, hidden_size, num_layers, num_classes).to(device)\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters())"
      ],
      "metadata": {
        "id": "EV1cKoL2uj7I"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for step, (seq, label) in enumerate(dataloader):\n",
        "  print(step)\n",
        "  print(seq.shape)\n",
        "  print(label.shape)\n",
        "  break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NVbYyiS1uqba",
        "outputId": "853ef7de-e9ac-4395-938a-315254118f25"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "torch.Size([2048, 10, 28])\n",
            "torch.Size([2048, 28])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "start_time = time.time()\n",
        "total_step = len(dataloader)\n",
        "for epoch in range(num_epochs):  # Loop over the dataset multiple times\n",
        "    train_loss = 0\n",
        "    for step, (seq, label) in enumerate(dataloader):\n",
        "        # Forward pass\n",
        "        seq = seq.clone().detach().view(-1, window_size, input_size).to(device)\n",
        "        output = model(seq)\n",
        "        loss = criterion(output, label.to(device))\n",
        "\n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        train_loss += loss.item()\n",
        "        optimizer.step()\n",
        "    print('Epoch [{}/{}], train_loss: {:.4f}'.format(epoch + 1, num_epochs, train_loss / total_step))\n",
        "elapsed_time = time.time() - start_time\n",
        "print('elapsed_time: {:.3f}s'.format(elapsed_time))\n",
        "print('Finished Training')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nWAjr1QOvB-n",
        "outputId": "ee897ef1-fe67-4b9b-83da-925a8e5667e5"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/300], train_loss: 3.0062\n",
            "Epoch [2/300], train_loss: 1.9114\n",
            "Epoch [3/300], train_loss: 1.7414\n",
            "Epoch [4/300], train_loss: 1.4911\n",
            "Epoch [5/300], train_loss: 1.1890\n",
            "Epoch [6/300], train_loss: 0.9568\n",
            "Epoch [7/300], train_loss: 0.8476\n",
            "Epoch [8/300], train_loss: 0.7954\n",
            "Epoch [9/300], train_loss: 0.7595\n",
            "Epoch [10/300], train_loss: 0.7283\n",
            "Epoch [11/300], train_loss: 0.6963\n",
            "Epoch [12/300], train_loss: 0.6622\n",
            "Epoch [13/300], train_loss: 0.6288\n",
            "Epoch [14/300], train_loss: 0.5895\n",
            "Epoch [15/300], train_loss: 0.5506\n",
            "Epoch [16/300], train_loss: 0.5248\n",
            "Epoch [17/300], train_loss: 0.5073\n",
            "Epoch [18/300], train_loss: 0.4837\n",
            "Epoch [19/300], train_loss: 0.4607\n",
            "Epoch [20/300], train_loss: 0.4424\n",
            "Epoch [21/300], train_loss: 0.4264\n",
            "Epoch [22/300], train_loss: 0.4147\n",
            "Epoch [23/300], train_loss: 0.4045\n",
            "Epoch [24/300], train_loss: 0.3981\n",
            "Epoch [25/300], train_loss: 0.3923\n",
            "Epoch [26/300], train_loss: 0.3871\n",
            "Epoch [27/300], train_loss: 0.3830\n",
            "Epoch [28/300], train_loss: 0.3755\n",
            "Epoch [29/300], train_loss: 0.3718\n",
            "Epoch [30/300], train_loss: 0.3665\n",
            "Epoch [31/300], train_loss: 0.3615\n",
            "Epoch [32/300], train_loss: 0.3543\n",
            "Epoch [33/300], train_loss: 0.3483\n",
            "Epoch [34/300], train_loss: 0.3433\n",
            "Epoch [35/300], train_loss: 0.3388\n",
            "Epoch [36/300], train_loss: 0.3323\n",
            "Epoch [37/300], train_loss: 0.3307\n",
            "Epoch [38/300], train_loss: 0.3271\n",
            "Epoch [39/300], train_loss: 0.3194\n",
            "Epoch [40/300], train_loss: 0.3159\n",
            "Epoch [41/300], train_loss: 0.3128\n",
            "Epoch [42/300], train_loss: 0.3078\n",
            "Epoch [43/300], train_loss: 0.3035\n",
            "Epoch [44/300], train_loss: 0.2993\n",
            "Epoch [45/300], train_loss: 0.2970\n",
            "Epoch [46/300], train_loss: 0.2950\n",
            "Epoch [47/300], train_loss: 0.2920\n",
            "Epoch [48/300], train_loss: 0.2876\n",
            "Epoch [49/300], train_loss: 0.2848\n",
            "Epoch [50/300], train_loss: 0.2828\n",
            "Epoch [51/300], train_loss: 0.2802\n",
            "Epoch [52/300], train_loss: 0.2791\n",
            "Epoch [53/300], train_loss: 0.2755\n",
            "Epoch [54/300], train_loss: 0.2737\n",
            "Epoch [55/300], train_loss: 0.2718\n",
            "Epoch [56/300], train_loss: 0.2706\n",
            "Epoch [57/300], train_loss: 0.2685\n",
            "Epoch [58/300], train_loss: 0.2650\n",
            "Epoch [59/300], train_loss: 0.2642\n",
            "Epoch [60/300], train_loss: 0.2625\n",
            "Epoch [61/300], train_loss: 0.2598\n",
            "Epoch [62/300], train_loss: 0.2580\n",
            "Epoch [63/300], train_loss: 0.2562\n",
            "Epoch [64/300], train_loss: 0.2546\n",
            "Epoch [65/300], train_loss: 0.2523\n",
            "Epoch [66/300], train_loss: 0.2510\n",
            "Epoch [67/300], train_loss: 0.2487\n",
            "Epoch [68/300], train_loss: 0.2481\n",
            "Epoch [69/300], train_loss: 0.2455\n",
            "Epoch [70/300], train_loss: 0.2459\n",
            "Epoch [71/300], train_loss: 0.2444\n",
            "Epoch [72/300], train_loss: 0.2429\n",
            "Epoch [73/300], train_loss: 0.2408\n",
            "Epoch [74/300], train_loss: 0.2402\n",
            "Epoch [75/300], train_loss: 0.2392\n",
            "Epoch [76/300], train_loss: 0.2384\n",
            "Epoch [77/300], train_loss: 0.2368\n",
            "Epoch [78/300], train_loss: 0.2379\n",
            "Epoch [79/300], train_loss: 0.2355\n",
            "Epoch [80/300], train_loss: 0.2345\n",
            "Epoch [81/300], train_loss: 0.2334\n",
            "Epoch [82/300], train_loss: 0.2335\n",
            "Epoch [83/300], train_loss: 0.2323\n",
            "Epoch [84/300], train_loss: 0.2318\n",
            "Epoch [85/300], train_loss: 0.2298\n",
            "Epoch [86/300], train_loss: 0.2308\n",
            "Epoch [87/300], train_loss: 0.2296\n",
            "Epoch [88/300], train_loss: 0.2298\n",
            "Epoch [89/300], train_loss: 0.2283\n",
            "Epoch [90/300], train_loss: 0.2274\n",
            "Epoch [91/300], train_loss: 0.2271\n",
            "Epoch [92/300], train_loss: 0.2254\n",
            "Epoch [93/300], train_loss: 0.2261\n",
            "Epoch [94/300], train_loss: 0.2255\n",
            "Epoch [95/300], train_loss: 0.2257\n",
            "Epoch [96/300], train_loss: 0.2232\n",
            "Epoch [97/300], train_loss: 0.2227\n",
            "Epoch [98/300], train_loss: 0.2218\n",
            "Epoch [99/300], train_loss: 0.2222\n",
            "Epoch [100/300], train_loss: 0.2219\n",
            "Epoch [101/300], train_loss: 0.2211\n",
            "Epoch [102/300], train_loss: 0.2199\n",
            "Epoch [103/300], train_loss: 0.2186\n",
            "Epoch [104/300], train_loss: 0.2185\n",
            "Epoch [105/300], train_loss: 0.2177\n",
            "Epoch [106/300], train_loss: 0.2173\n",
            "Epoch [107/300], train_loss: 0.2175\n",
            "Epoch [108/300], train_loss: 0.2166\n",
            "Epoch [109/300], train_loss: 0.2171\n",
            "Epoch [110/300], train_loss: 0.2155\n",
            "Epoch [111/300], train_loss: 0.2156\n",
            "Epoch [112/300], train_loss: 0.2159\n",
            "Epoch [113/300], train_loss: 0.2150\n",
            "Epoch [114/300], train_loss: 0.2147\n",
            "Epoch [115/300], train_loss: 0.2142\n",
            "Epoch [116/300], train_loss: 0.2135\n",
            "Epoch [117/300], train_loss: 0.2137\n",
            "Epoch [118/300], train_loss: 0.2136\n",
            "Epoch [119/300], train_loss: 0.2117\n",
            "Epoch [120/300], train_loss: 0.2106\n",
            "Epoch [121/300], train_loss: 0.2118\n",
            "Epoch [122/300], train_loss: 0.2110\n",
            "Epoch [123/300], train_loss: 0.2105\n",
            "Epoch [124/300], train_loss: 0.2099\n",
            "Epoch [125/300], train_loss: 0.2095\n",
            "Epoch [126/300], train_loss: 0.2092\n",
            "Epoch [127/300], train_loss: 0.2094\n",
            "Epoch [128/300], train_loss: 0.2084\n",
            "Epoch [129/300], train_loss: 0.2079\n",
            "Epoch [130/300], train_loss: 0.2082\n",
            "Epoch [131/300], train_loss: 0.2086\n",
            "Epoch [132/300], train_loss: 0.2077\n",
            "Epoch [133/300], train_loss: 0.2069\n",
            "Epoch [134/300], train_loss: 0.2071\n",
            "Epoch [135/300], train_loss: 0.2063\n",
            "Epoch [136/300], train_loss: 0.2068\n",
            "Epoch [137/300], train_loss: 0.2064\n",
            "Epoch [138/300], train_loss: 0.2056\n",
            "Epoch [139/300], train_loss: 0.2055\n",
            "Epoch [140/300], train_loss: 0.2058\n",
            "Epoch [141/300], train_loss: 0.2053\n",
            "Epoch [142/300], train_loss: 0.2052\n",
            "Epoch [143/300], train_loss: 0.2056\n",
            "Epoch [144/300], train_loss: 0.2055\n",
            "Epoch [145/300], train_loss: 0.2062\n",
            "Epoch [146/300], train_loss: 0.2051\n",
            "Epoch [147/300], train_loss: 0.2035\n",
            "Epoch [148/300], train_loss: 0.2039\n",
            "Epoch [149/300], train_loss: 0.2045\n",
            "Epoch [150/300], train_loss: 0.2038\n",
            "Epoch [151/300], train_loss: 0.2038\n",
            "Epoch [152/300], train_loss: 0.2031\n",
            "Epoch [153/300], train_loss: 0.2031\n",
            "Epoch [154/300], train_loss: 0.2025\n",
            "Epoch [155/300], train_loss: 0.2026\n",
            "Epoch [156/300], train_loss: 0.2024\n",
            "Epoch [157/300], train_loss: 0.2027\n",
            "Epoch [158/300], train_loss: 0.2026\n",
            "Epoch [159/300], train_loss: 0.2024\n",
            "Epoch [160/300], train_loss: 0.2033\n",
            "Epoch [161/300], train_loss: 0.2022\n",
            "Epoch [162/300], train_loss: 0.2035\n",
            "Epoch [163/300], train_loss: 0.2024\n",
            "Epoch [164/300], train_loss: 0.2017\n",
            "Epoch [165/300], train_loss: 0.2021\n",
            "Epoch [166/300], train_loss: 0.2019\n",
            "Epoch [167/300], train_loss: 0.2019\n",
            "Epoch [168/300], train_loss: 0.2013\n",
            "Epoch [169/300], train_loss: 0.2008\n",
            "Epoch [170/300], train_loss: 0.2018\n",
            "Epoch [171/300], train_loss: 0.2010\n",
            "Epoch [172/300], train_loss: 0.2001\n",
            "Epoch [173/300], train_loss: 0.2003\n",
            "Epoch [174/300], train_loss: 0.2008\n",
            "Epoch [175/300], train_loss: 0.2004\n",
            "Epoch [176/300], train_loss: 0.1999\n",
            "Epoch [177/300], train_loss: 0.2000\n",
            "Epoch [178/300], train_loss: 0.1999\n",
            "Epoch [179/300], train_loss: 0.2002\n",
            "Epoch [180/300], train_loss: 0.1993\n",
            "Epoch [181/300], train_loss: 0.1996\n",
            "Epoch [182/300], train_loss: 0.1995\n",
            "Epoch [183/300], train_loss: 0.1995\n",
            "Epoch [184/300], train_loss: 0.1999\n",
            "Epoch [185/300], train_loss: 0.1996\n",
            "Epoch [186/300], train_loss: 0.1987\n",
            "Epoch [187/300], train_loss: 0.1993\n",
            "Epoch [188/300], train_loss: 0.1979\n",
            "Epoch [189/300], train_loss: 0.1993\n",
            "Epoch [190/300], train_loss: 0.1988\n",
            "Epoch [191/300], train_loss: 0.1980\n",
            "Epoch [192/300], train_loss: 0.1980\n",
            "Epoch [193/300], train_loss: 0.1990\n",
            "Epoch [194/300], train_loss: 0.1985\n",
            "Epoch [195/300], train_loss: 0.1985\n",
            "Epoch [196/300], train_loss: 0.1978\n",
            "Epoch [197/300], train_loss: 0.1977\n",
            "Epoch [198/300], train_loss: 0.1972\n",
            "Epoch [199/300], train_loss: 0.1978\n",
            "Epoch [200/300], train_loss: 0.1977\n",
            "Epoch [201/300], train_loss: 0.1969\n",
            "Epoch [202/300], train_loss: 0.1971\n",
            "Epoch [203/300], train_loss: 0.1976\n",
            "Epoch [204/300], train_loss: 0.1978\n",
            "Epoch [205/300], train_loss: 0.1981\n",
            "Epoch [206/300], train_loss: 0.1963\n",
            "Epoch [207/300], train_loss: 0.1965\n",
            "Epoch [208/300], train_loss: 0.1963\n",
            "Epoch [209/300], train_loss: 0.1972\n",
            "Epoch [210/300], train_loss: 0.1965\n",
            "Epoch [211/300], train_loss: 0.1960\n",
            "Epoch [212/300], train_loss: 0.1962\n",
            "Epoch [213/300], train_loss: 0.1960\n",
            "Epoch [214/300], train_loss: 0.1964\n",
            "Epoch [215/300], train_loss: 0.1959\n",
            "Epoch [216/300], train_loss: 0.1954\n",
            "Epoch [217/300], train_loss: 0.1955\n",
            "Epoch [218/300], train_loss: 0.1955\n",
            "Epoch [219/300], train_loss: 0.1951\n",
            "Epoch [220/300], train_loss: 0.1964\n",
            "Epoch [221/300], train_loss: 0.1965\n",
            "Epoch [222/300], train_loss: 0.1951\n",
            "Epoch [223/300], train_loss: 0.1946\n",
            "Epoch [224/300], train_loss: 0.1955\n",
            "Epoch [225/300], train_loss: 0.1951\n",
            "Epoch [226/300], train_loss: 0.1950\n",
            "Epoch [227/300], train_loss: 0.1946\n",
            "Epoch [228/300], train_loss: 0.1948\n",
            "Epoch [229/300], train_loss: 0.1952\n",
            "Epoch [230/300], train_loss: 0.1941\n",
            "Epoch [231/300], train_loss: 0.1948\n",
            "Epoch [232/300], train_loss: 0.1947\n",
            "Epoch [233/300], train_loss: 0.1945\n",
            "Epoch [234/300], train_loss: 0.1952\n",
            "Epoch [235/300], train_loss: 0.1949\n",
            "Epoch [236/300], train_loss: 0.1942\n",
            "Epoch [237/300], train_loss: 0.1943\n",
            "Epoch [238/300], train_loss: 0.1936\n",
            "Epoch [239/300], train_loss: 0.1940\n",
            "Epoch [240/300], train_loss: 0.1943\n",
            "Epoch [241/300], train_loss: 0.1930\n",
            "Epoch [242/300], train_loss: 0.1936\n",
            "Epoch [243/300], train_loss: 0.1933\n",
            "Epoch [244/300], train_loss: 0.1935\n",
            "Epoch [245/300], train_loss: 0.1933\n",
            "Epoch [246/300], train_loss: 0.1934\n",
            "Epoch [247/300], train_loss: 0.1928\n",
            "Epoch [248/300], train_loss: 0.1934\n",
            "Epoch [249/300], train_loss: 0.1932\n",
            "Epoch [250/300], train_loss: 0.1930\n",
            "Epoch [251/300], train_loss: 0.1924\n",
            "Epoch [252/300], train_loss: 0.1935\n",
            "Epoch [253/300], train_loss: 0.1925\n",
            "Epoch [254/300], train_loss: 0.1927\n",
            "Epoch [255/300], train_loss: 0.1930\n",
            "Epoch [256/300], train_loss: 0.1926\n",
            "Epoch [257/300], train_loss: 0.1922\n",
            "Epoch [258/300], train_loss: 0.1923\n",
            "Epoch [259/300], train_loss: 0.1924\n",
            "Epoch [260/300], train_loss: 0.1918\n",
            "Epoch [261/300], train_loss: 0.1925\n",
            "Epoch [262/300], train_loss: 0.1923\n",
            "Epoch [263/300], train_loss: 0.1921\n",
            "Epoch [264/300], train_loss: 0.1927\n",
            "Epoch [265/300], train_loss: 0.1924\n",
            "Epoch [266/300], train_loss: 0.1920\n",
            "Epoch [267/300], train_loss: 0.1916\n",
            "Epoch [268/300], train_loss: 0.1915\n",
            "Epoch [269/300], train_loss: 0.1914\n",
            "Epoch [270/300], train_loss: 0.1921\n",
            "Epoch [271/300], train_loss: 0.1911\n",
            "Epoch [272/300], train_loss: 0.1914\n",
            "Epoch [273/300], train_loss: 0.1916\n",
            "Epoch [274/300], train_loss: 0.1908\n",
            "Epoch [275/300], train_loss: 0.1909\n",
            "Epoch [276/300], train_loss: 0.1908\n",
            "Epoch [277/300], train_loss: 0.1915\n",
            "Epoch [278/300], train_loss: 0.1917\n",
            "Epoch [279/300], train_loss: 0.1917\n",
            "Epoch [280/300], train_loss: 0.1909\n",
            "Epoch [281/300], train_loss: 0.1902\n",
            "Epoch [282/300], train_loss: 0.1898\n",
            "Epoch [283/300], train_loss: 0.1908\n",
            "Epoch [284/300], train_loss: 0.1907\n",
            "Epoch [285/300], train_loss: 0.1907\n",
            "Epoch [286/300], train_loss: 0.1911\n",
            "Epoch [287/300], train_loss: 0.1905\n",
            "Epoch [288/300], train_loss: 0.1908\n",
            "Epoch [289/300], train_loss: 0.1908\n",
            "Epoch [290/300], train_loss: 0.1906\n",
            "Epoch [291/300], train_loss: 0.1907\n",
            "Epoch [292/300], train_loss: 0.1908\n",
            "Epoch [293/300], train_loss: 0.1908\n",
            "Epoch [294/300], train_loss: 0.1903\n",
            "Epoch [295/300], train_loss: 0.1897\n",
            "Epoch [296/300], train_loss: 0.1897\n",
            "Epoch [297/300], train_loss: 0.1898\n",
            "Epoch [298/300], train_loss: 0.1888\n",
            "Epoch [299/300], train_loss: 0.1896\n",
            "Epoch [300/300], train_loss: 0.1898\n",
            "elapsed_time: 245.259s\n",
            "Finished Training\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#save the model\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "torch.save(model.state_dict(), '/content/drive/MyDrive/LSTM_model_parameter')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 227
        },
        "id": "qOTRSFSizLLG",
        "outputId": "a6d63749-a326-4c80-aed7-30ee73223142"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-18ba56ea5405>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'/content/drive/MyDrive/LSTM_model_parameter'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print model's state_dict\n",
        "print(\"Model's state_dict:\")\n",
        "for param_tensor in model.state_dict():\n",
        "    print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TkujEkDKB0iN",
        "outputId": "8c9d40d4-b663-4969-ffcd-584b98376ea3"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model's state_dict:\n",
            "lstm.weight_ih_l0 \t torch.Size([256, 28])\n",
            "lstm.weight_hh_l0 \t torch.Size([256, 64])\n",
            "lstm.bias_ih_l0 \t torch.Size([256])\n",
            "lstm.bias_hh_l0 \t torch.Size([256])\n",
            "lstm.weight_ih_l1 \t torch.Size([256, 64])\n",
            "lstm.weight_hh_l1 \t torch.Size([256, 64])\n",
            "lstm.bias_ih_l1 \t torch.Size([256])\n",
            "lstm.bias_hh_l1 \t torch.Size([256])\n",
            "fc.weight \t torch.Size([28, 64])\n",
            "fc.bias \t torch.Size([28])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x,y = dataset[0]\n",
        "print(x)\n",
        "print(y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qZ4YJq2FytWK",
        "outputId": "9756cfdd-ec0a-4443-cb17-12e0a0b5757f"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
            "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "  output = model(x.view(-1, 10, 28).to(device))\n",
        "\n",
        "print(output)\n",
        "print(np.argmax(output.cpu()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yXW9TfOPyyfX",
        "outputId": "9167f87c-08f7-4d05-fcbd-755445cff2f8"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-4.2119, -0.9358, -0.4673, -2.2237, -8.2818, -3.1104, -4.5761, -4.5197,\n",
            "          0.3511, -4.4162,  1.6622, -4.6405, -4.5563, -4.3275, -4.1238, -7.0442,\n",
            "         -4.7698, -2.1374, -4.2913, -4.2192, -0.0477, -2.4882, -0.7021, -4.1183,\n",
            "         -1.7215, 12.4460, -4.2609, -4.2010]], device='cuda:0')\n",
            "tensor(25)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x,y = dataset[5]\n",
        "print(x)\n",
        "print(y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "71BGXoiezD4h",
        "outputId": "7507c025-ef1c-42cc-f259-88448847472a"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]])\n",
            "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "with torch.no_grad():\n",
        "  output = model(x.view(-1, 10, 28).to(device))\n",
        "\n",
        "print(output)\n",
        "print(np.argmax(output.cpu()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MAqx1iDozGrH",
        "outputId": "a1faf437-aac4-46f4-f681-739b8bc8953b"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ -6.3857,   1.7995,   1.1668,  -0.4470,  -0.6188,  -3.4613,  -6.9188,\n",
            "          -6.1911, -10.7469,  -6.1307,  -3.6821,  -5.9067,  -5.9824,  -6.3436,\n",
            "          -5.5927,  -8.6709,  -7.1039,   0.2851,  -5.9360,  -6.4386,   1.8577,\n",
            "          -3.6044,  12.1136,  -5.7297,   0.6888,  -0.5130,  -6.1667,  -5.8830]],\n",
            "       device='cuda:0')\n",
            "tensor(22)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HLHCD2VNBsk-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PART 2"
      ],
      "metadata": {
        "id": "Znevp-JWB9i8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "n_templates = 28 #total number of unique templates\n",
        "\n",
        "def template_to_tensor(template):\n",
        "    tensor = [0] * n_templates\n",
        "    if template != -1 :    # we put template= -1 for session less than windows_size in empty part and it will be all zero\n",
        "      tensor[template] = 1\n",
        "    return tensor"
      ],
      "metadata": {
        "id": "_H3wad8qKmQw"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Model(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
        "        super(Model, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
        "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
        "        out, _ = self.lstm(x, (h0, c0))  # out.shape : [batch_size, sequence_length, hidden_size]\n",
        "        out = self.fc(out[:, -1, :]) #The : before , -1, : indicates that we want to include all elements along the first dimension (batch dimension). -1 represents the index of the last element along the second dimension (sequence length). : after , -1 indicates that we want to include all elements along the third dimension (hidden size)\n",
        "        return out"
      ],
      "metadata": {
        "id": "m0XBTfjdCEiB"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "window_size = 10\n",
        "num_classes = 28\n",
        "input_size = 28\n",
        "num_layers = 2\n",
        "hidden_size = 64\n",
        "num_candidates = 9 # on paper is g , top-g(here top 9) probabilities to appear next are considered normal\n",
        "model = Model(input_size, hidden_size, num_layers, num_classes).to(device)"
      ],
      "metadata": {
        "id": "egIle7ZyCJ8t"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# upload the model\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "model_path = '/content/drive/MyDrive/LSTM_model_parameter'\n",
        "model.load_state_dict(torch.load(model_path))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NpEL9Hj7B_iI",
        "outputId": "7469f9b5-08b2-4c3a-a168-c542f7cdaf16"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print model's state_dict\n",
        "print(\"Model's state_dict:\")\n",
        "for param_tensor in model.state_dict():\n",
        "    print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sdwMgzHVDNHq",
        "outputId": "d2d1ef0b-277e-42f9-bb17-96252a180912"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model's state_dict:\n",
            "lstm.weight_ih_l0 \t torch.Size([256, 28])\n",
            "lstm.weight_hh_l0 \t torch.Size([256, 64])\n",
            "lstm.bias_ih_l0 \t torch.Size([256])\n",
            "lstm.bias_hh_l0 \t torch.Size([256])\n",
            "lstm.weight_ih_l1 \t torch.Size([256, 64])\n",
            "lstm.weight_hh_l1 \t torch.Size([256, 64])\n",
            "lstm.bias_ih_l1 \t torch.Size([256])\n",
            "lstm.bias_hh_l1 \t torch.Size([256])\n",
            "fc.weight \t torch.Size([28, 64])\n",
            "fc.bias \t torch.Size([28])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "row = '2 3 4 5 6 7'\n",
        "line = [template_to_tensor(int(i) - 1) for i in row.strip().split()]\n",
        "line = line + [[0]*28] * (window_size + 1 - len(line))\n",
        "len(line)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_hTQqljUDwIH",
        "outputId": "e62e4825-95f4-477f-ac7f-e7ff5c7c61bb"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "11"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "line = [int(i) - 1 for i in row.strip().split()]\n",
        "line = line + [-1] * (window_size + 1 - len(line))\n",
        "line = [template_to_tensor(template) for template in line]\n",
        "line\n"
      ],
      "metadata": {
        "id": "cJPbICUCGhqL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate(name):\n",
        "    # If you what to replicate the DeepLog paper results(Actually, I have a better result than DeepLog paper results),\n",
        "    # you should use the 'list' not 'set' to obtain the full dataset, I use 'set' just for test and acceleration.\n",
        "    hdfs = set()\n",
        "    # hdfs = []\n",
        "    with open('/content/' + name, 'r') as f:\n",
        "        for row in f:\n",
        "            line = [int(i) - 1 for i in row.strip().split()]\n",
        "            line = line + [-1] * (window_size + 1 - len(line)) #if the length of the line is less than windows size, it covers by -1\n",
        "            hdfs.add(tuple(line))\n",
        "            # hdfs.append(tuple(line))\n",
        "    print('Number of sessions({}): {}'.format(name, len(hdfs)))\n",
        "    return hdfs"
      ],
      "metadata": {
        "id": "uhxl140ML2EW"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_normal_loader = generate('hdfs_test_normal')\n",
        "test_abnormal_loader = generate('hdfs_test_abnormal')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JSMY2N3nMD-u",
        "outputId": "5b852b87-5efd-4da6-c22f-86305f9bc025"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of sessions(hdfs_test_normal): 14177\n",
            "Number of sessions(hdfs_test_abnormal): 4123\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the model\n",
        "\n",
        "model.eval()\n",
        "\n",
        "TP = 0\n",
        "FP = 0\n",
        "start_time = time.time()\n",
        "with torch.no_grad():\n",
        "    for line in test_normal_loader:\n",
        "        for i in range(len(line) - window_size):\n",
        "            session = line[i:i + window_size]\n",
        "            seq = [template_to_tensor(temp) for temp in session]\n",
        "            print(seq)\n",
        "\n",
        "            label = template_to_tensor(line[i + window_size])\n",
        "            print(label)\n",
        "            seq = torch.tensor(seq, dtype=torch.float).view(-1, window_size, input_size).to(device)\n",
        "            print(seq.shape)\n",
        "            label = torch.tensor(label).view(-1).to(device)\n",
        "            print(label.shape)\n",
        "            output = model(seq)\n",
        "            print(output)\n",
        "            predicted = torch.argsort(output, 1)[0][num_candidates:]\n",
        "            break\n",
        "        break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CRY-XMHAIE8R",
        "outputId": "c8f6a3b6-0614-4052-fec8-c56a4c58d6ce"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "torch.Size([1, 10, 28])\n",
            "torch.Size([28])\n",
            "tensor([[-4.3896, -1.8478,  1.3329, -1.0443, -5.1919, -2.7708, -4.2849, -4.0560,\n",
            "          3.3327, -4.4825, 11.7076, -4.0216, -4.3207, -4.0244, -3.5000, -6.8778,\n",
            "         -4.9745, -5.5103, -4.1834, -4.1093, -3.4329, -2.5519,  1.2227, -3.7865,\n",
            "         -4.9176,  5.0063, -3.9862, -4.1436]], device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(np.argmax(label.cpu()))\n",
        "print(np.argmax(output.cpu()))\n",
        "print(output[0,10])\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sLYqmjfSNtyQ",
        "outputId": "ec8d89a5-bcf1-486c-a642-61bb37083eca"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(10)\n",
            "tensor(10)\n",
            "tensor(11.7076, device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "session = line[i:i + window_size]\n",
        "            seq = [template_to_tensor(temp) for temp in session]\n",
        "            label = template_to_tensor(line[i + window_size])"
      ],
      "metadata": {
        "id": "25hq5aJmOecN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the model\n",
        "TP = 0\n",
        "FP = 0\n",
        "\n",
        "start_time = time.time()\n",
        "with torch.no_grad():\n",
        "    for line in test_normal_loader:\n",
        "        for i in range(len(line) - window_size):\n",
        "            session = line[i:i + window_size]\n",
        "            seq = [template_to_tensor(temp) for temp in session]\n",
        "            label = template_to_tensor(line[i + window_size])\n",
        "            seq = torch.tensor(seq, dtype=torch.float).view(-1, window_size, input_size).to(device)\n",
        "            label = torch.tensor(label).view(-1).to(device)\n",
        "            output = model(seq)\n",
        "            predicted = torch.argsort(output, 1)[0][-num_candidates:]\n",
        "            if torch.argmax(label) not in predicted:\n",
        "                FP += 1\n",
        "                break   #with just one wrong prediction in a line , we assume , abnormal\n",
        "with torch.no_grad():\n",
        "    for line in test_abnormal_loader:\n",
        "        for i in range(len(line) - window_size):\n",
        "            session = line[i:i + window_size]\n",
        "            seq = [template_to_tensor(temp) for temp in session]\n",
        "            label = template_to_tensor(line[i + window_size])\n",
        "            seq = torch.tensor(seq, dtype=torch.float).view(-1, window_size, input_size).to(device)\n",
        "            label = torch.tensor(label).view(-1).to(device)\n",
        "            output = model(seq)\n",
        "            predicted = torch.argsort(output, 1)[0][-num_candidates:]\n",
        "            if torch.argmax(label) not in predicted:\n",
        "                TP += 1\n",
        "                break\n",
        "elapsed_time = time.time() - start_time\n",
        "print('elapsed_time: {:.3f}s'.format(elapsed_time))\n",
        "# Compute precision, recall and F1-measure\n",
        "FN = len(test_abnormal_loader) - TP\n",
        "P = 100 * TP / (TP + FP)\n",
        "R = 100 * TP / (TP + FN)\n",
        "F1 = 2 * P * R / (P + R)\n",
        "print('false positive (FP): {}, false negative (FN): {}, Precision: {:.3f}%, Recall: {:.3f}%, F1-measure: {:.3f}%'.format(FP, FN, P, R, F1))\n",
        "print('Finished Predicting')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QT3vZzUbOFUr",
        "outputId": "1a09db45-c767-4ef8-ffa8-bd52218ed8a3"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "elapsed_time: 180.065s\n",
            "false positive (FP): 350, false negative (FN): 744, Precision: 90.614%, Recall: 81.955%, F1-measure: 86.067%\n",
            "Finished Predicting\n"
          ]
        }
      ]
    }
  ]
}