{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNsSi/bGxcSpdW9ubD62rAg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mostafa-ja/Anomaly-detection/blob/main/C(loganomaly_model).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
      ],
      "metadata": {
        "id": "-cgjPvYjFbKo"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# download datasets\n",
        "!wget 'https://raw.githubusercontent.com/donglee-afar/logdeep/master/data/hdfs/hdfs_train'\n",
        "!wget 'https://raw.githubusercontent.com/donglee-afar/logdeep/master/data/hdfs/hdfs_test_normal'\n",
        "!wget 'https://raw.githubusercontent.com/donglee-afar/logdeep/master/data/hdfs/hdfs_test_abnormal'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qZYNqcwLIPLF",
        "outputId": "91f54dee-9af1-438a-cc5b-3eed685c1893"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-07-26 16:53:20--  https://raw.githubusercontent.com/donglee-afar/logdeep/master/data/hdfs/hdfs_train\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.111.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 257875 (252K) [text/plain]\n",
            "Saving to: ‘hdfs_train.1’\n",
            "\n",
            "hdfs_train.1        100%[===================>] 251.83K  --.-KB/s    in 0.004s  \n",
            "\n",
            "2023-07-26 16:53:20 (57.5 MB/s) - ‘hdfs_train.1’ saved [257875/257875]\n",
            "\n",
            "--2023-07-26 16:53:21--  https://raw.githubusercontent.com/donglee-afar/logdeep/master/data/hdfs/hdfs_test_normal\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 29284282 (28M) [text/plain]\n",
            "Saving to: ‘hdfs_test_normal.1’\n",
            "\n",
            "hdfs_test_normal.1  100%[===================>]  27.93M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2023-07-26 16:53:23 (291 MB/s) - ‘hdfs_test_normal.1’ saved [29284282/29284282]\n",
            "\n",
            "--2023-07-26 16:53:23--  https://raw.githubusercontent.com/donglee-afar/logdeep/master/data/hdfs/hdfs_test_abnormal\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 782479 (764K) [text/plain]\n",
            "Saving to: ‘hdfs_test_abnormal.1’\n",
            "\n",
            "hdfs_test_abnormal. 100%[===================>] 764.14K  --.-KB/s    in 0.01s   \n",
            "\n",
            "2023-07-26 16:53:23 (61.0 MB/s) - ‘hdfs_test_abnormal.1’ saved [782479/782479]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# count session for each dataset\n",
        "\n",
        "def count_sessions(dataset):\n",
        "    num_sessions = 0\n",
        "    with open('/content/'+ dataset, 'r') as f:\n",
        "        for row in f:\n",
        "            num_sessions += 1\n",
        "    print('Number of sessions({}): {}'.format(dataset, num_sessions))\n",
        "\n",
        "datasets = ['hdfs_train','hdfs_test_normal','hdfs_test_abnormal']\n",
        "\n",
        "for dataset in datasets:\n",
        "  count_sessions(dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BaCB9o7erHFe",
        "outputId": "255f48bc-1cae-40f6-eeab-cef42f7720c9"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of sessions(hdfs_train): 4855\n",
            "Number of sessions(hdfs_test_normal): 553366\n",
            "Number of sessions(hdfs_test_abnormal): 16838\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# all templates in our datasets\n",
        "\n",
        "datasets = ['hdfs_train','hdfs_test_normal','hdfs_test_abnormal']\n",
        "templates = set()\n",
        "\n",
        "for dataset in datasets:\n",
        "  with open('/content/' + dataset, 'r') as f:\n",
        "          for row in f:\n",
        "            for temp in row.split():\n",
        "              templates.add(temp)\n",
        "\n",
        "print(sorted(templates))\n",
        "print('nember of templates : ',len(templates))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pNDAhBmpwKSs",
        "outputId": "b2bfcb06-f0da-486e-df7c-58f851697aef"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['1', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '2', '20', '21', '22', '23', '24', '25', '26', '27', '28', '3', '4', '5', '6', '7', '8', '9']\n",
            "nember of templates :  28\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "name = 'hdfs_train'\n",
        "window_size = 10\n",
        "num_sessions = 0\n",
        "inputs = []\n",
        "outputs = []\n",
        "\n",
        "with open('/content/' + name, 'r') as f:\n",
        "        for row in f:\n",
        "            num_sessions += 1\n",
        "            line = [ (int(i)-1) for i in row.strip().split()]\n",
        "            for i in range(len(line) - window_size):\n",
        "                inputs.append(line[i:i + window_size])\n",
        "                outputs.append(line[i + window_size])\n",
        "\n",
        "print('Number of sessions({}): {}'.format(name, num_sessions))\n",
        "print('Number of seqs({}): {}'.format(name, len(inputs)))\n",
        "dataset = TensorDataset(torch.tensor(inputs, dtype=torch.float), torch.tensor(outputs))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w1P4keFzwKDn",
        "outputId": "dd5bd00f-3f94-46f6-cfe6-32935b33071c"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of sessions(hdfs_train): 4855\n",
            "Number of seqs(hdfs_train): 46575\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#import json\n",
        "#def read_json(filename):\n",
        "    #with open(filename, 'r') as load_f:\n",
        "        #file_dict = json.load(load_f)\n",
        "    #return file_dict\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "def sliding_window(data_dir, window_size):\n",
        "    '''\n",
        "    dataset structure\n",
        "        result_logs(dict):\n",
        "            result_logs['feature0'] = list()\n",
        "            result_logs['feature1'] = list()\n",
        "            ...\n",
        "        labels(list)\n",
        "    '''\n",
        "    #event2semantic_vec = read_json(data_dir + 'hdfs/event2semantic_vec.json')\n",
        "    num_sessions = 0\n",
        "    result_logs = {}\n",
        "    result_logs['Sequentials'] = []\n",
        "    result_logs['Quantitatives'] = []\n",
        "    #result_logs['Semantics'] = []\n",
        "    labels = []\n",
        "\n",
        "    with open(data_dir, 'r') as f:\n",
        "        for line in f.readlines():\n",
        "            num_sessions += 1\n",
        "            line = [(int(i)-1) for i in line.strip().split()]\n",
        "            for i in range(len(line) - window_size):\n",
        "\n",
        "                Sequential_pattern = list(line[i:i + window_size])\n",
        "\n",
        "                Quantitative_pattern = [0] * 28  # 28 templates we have\n",
        "                log_counter = Counter(Sequential_pattern)\n",
        "                for key in log_counter:\n",
        "                  if key > 0:  # avoid considering -1 in templates , but we just see in test dataset\n",
        "                    Quantitative_pattern[key] = log_counter[key]\n",
        "\n",
        "                #Semantic_pattern = []\n",
        "                #for event in Sequential_pattern:\n",
        "                    #if event == 0:\n",
        "                        #Semantic_pattern.append([-1] * 300)\n",
        "                    #else:\n",
        "                        #Semantic_pattern.append(event2semantic_vec[str(event - 1)])\n",
        "\n",
        "                #Sequential_pattern = np.array(Sequential_pattern)[:, np.newaxis]\n",
        "                #Quantitative_pattern = np.array(Quantitative_pattern)[:, np.newaxis]\n",
        "                result_logs['Sequentials'].append(Sequential_pattern)\n",
        "                result_logs['Quantitatives'].append(Quantitative_pattern)\n",
        "                #result_logs['Semantics'].append(Semantic_pattern)\n",
        "\n",
        "                labels.append(line[i + window_size])\n",
        "\n",
        "    print('File {}, number of sessions {}'.format(data_dir, num_sessions))\n",
        "    print('File {}, number of seqs {}'.format(data_dir, len(result_logs['Sequentials'])))\n",
        "\n",
        "    return result_logs, labels\n"
      ],
      "metadata": {
        "id": "eZfqkQCoBqMg"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result_logs, labels = sliding_window('/content/hdfs_train', 10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dpio7sLuIQzV",
        "outputId": "fef9bb04-f6f4-4ee0-d481-bdd35b047b3f"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File /content/hdfs_train, number of sessions 4855\n",
            "File /content/hdfs_train, number of seqs 46575\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = TensorDataset(torch.tensor(result_logs['Sequentials'], dtype=torch.float), torch.tensor(result_logs['Quantitatives'], dtype=torch.float), torch.tensor(labels))\n",
        "dataloader = DataLoader(dataset, batch_size=1)\n"
      ],
      "metadata": {
        "id": "lHJ6J37wNEPV"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.autograd import Variable\n",
        "\n",
        "class loganomaly(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, num_keys):\n",
        "        super(loganomaly, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.lstm0 = nn.LSTM(input_size,\n",
        "                             hidden_size,\n",
        "                             num_layers,\n",
        "                             batch_first=True)\n",
        "        self.lstm1 = nn.LSTM(input_size,\n",
        "                             hidden_size,\n",
        "                             num_layers,\n",
        "                             batch_first=True)\n",
        "        self.fc = nn.Linear(2 * hidden_size, num_keys)\n",
        "        self.attention_size = self.hidden_size\n",
        "\n",
        "        self.w_omega = Variable(\n",
        "            torch.zeros(self.hidden_size, self.attention_size))\n",
        "        self.u_omega = Variable(torch.zeros(self.attention_size))\n",
        "\n",
        "        self.sequence_length = 28\n",
        "\n",
        "    def attention_net(self, lstm_output):\n",
        "        output_reshape = torch.Tensor.reshape(lstm_output,\n",
        "                                              [-1, self.hidden_size])\n",
        "        attn_tanh = torch.tanh(torch.mm(output_reshape, self.w_omega))\n",
        "        attn_hidden_layer = torch.mm(\n",
        "            attn_tanh, torch.Tensor.reshape(self.u_omega, [-1, 1]))\n",
        "        exps = torch.Tensor.reshape(torch.exp(attn_hidden_layer),\n",
        "                                    [-1, self.sequence_length])\n",
        "        alphas = exps / torch.Tensor.reshape(torch.sum(exps, 1), [-1, 1])\n",
        "        alphas_reshape = torch.Tensor.reshape(alphas,\n",
        "                                              [-1, self.sequence_length, 1])\n",
        "        state = lstm_output\n",
        "        attn_output = torch.sum(state * alphas_reshape, 1)\n",
        "        return attn_output\n",
        "\n",
        "    def forward(self, features, device):\n",
        "        input0, input1 = features[0], features[1]\n",
        "\n",
        "        h0_0 = torch.zeros(self.num_layers, input0.size(0),\n",
        "                           self.hidden_size).to(device)\n",
        "        c0_0 = torch.zeros(self.num_layers, input0.size(0),\n",
        "                           self.hidden_size).to(device)\n",
        "\n",
        "        out0, _ = self.lstm0(input0, (h0_0, c0_0))\n",
        "\n",
        "        h0_1 = torch.zeros(self.num_layers, input1.size(0),\n",
        "                           self.hidden_size).to(device)\n",
        "        c0_1 = torch.zeros(self.num_layers, input1.size(0),\n",
        "                           self.hidden_size).to(device)\n",
        "\n",
        "        out1, _ = self.lstm1(input1, (h0_1, c0_1))\n",
        "        multi_out = torch.cat((out0[:, -1, :], out1[:, -1, :]), -1)\n",
        "        out = self.fc(multi_out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "v3DBDvgg7P7U"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "input_size = 1\n",
        "num_layers = 2\n",
        "hidden_size = 64\n",
        "num_classes = 28  # templates\n",
        "batch_size = 2048\n",
        "num_epochs = 370\n",
        "window_size = 10"
      ],
      "metadata": {
        "id": "R0ht4IKd2QFf"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = loganomaly(input_size, hidden_size, num_layers, num_classes).to(device)\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n"
      ],
      "metadata": {
        "id": "Ve9GNCW-2zDL"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def adjust_learning_rate(optimizer, epoch, lr_step=(300, 350), lr_decay_ratio=0.1):\n",
        "    \"\"\"Adjust the learning rate based on the epoch number.\"\"\"\n",
        "    if epoch == 0:\n",
        "        optimizer.param_groups[0]['lr'] /= 32\n",
        "    elif epoch in [2, 4, 6, 8, 10]:  # in step 10 , we finish warm up ,and start normal learning rate\n",
        "        optimizer.param_groups[0]['lr'] *= 2\n",
        "    if epoch in lr_step: # in these steps , we are geting close to optimal point so we need to have shorter step\n",
        "        optimizer.param_groups[0]['lr'] *= lr_decay_ratio\n",
        "    return optimizer\n",
        "\n",
        "# Define options here\n",
        "options = {\n",
        "    'lr': 0.001,\n",
        "    'lr_step': (300, 350), #steps(epoch) for updating learning rate\n",
        "    'lr_decay_ratio': 0.1,\n",
        "    # Add other options here\n",
        "}\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=options['lr'], betas=(0.9, 0.999))\n"
      ],
      "metadata": {
        "id": "54g6LFd-IkwN"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Train the model\n",
        "start_time = time.time()\n",
        "total_step = len(dataloader)\n",
        "for epoch in range(num_epochs):  # Loop over the dataset multiple times\n",
        "    optimizer = adjust_learning_rate(optimizer, epoch, options['lr_step'], options['lr_decay_ratio'])\n",
        "    print(optimizer.param_groups[0]['lr'])\n",
        "    train_loss = 0\n",
        "    for step, (seq,quan,label) in enumerate(dataloader):\n",
        "        # Move data to the device\n",
        "        seq = seq.view(-1, window_size, input_size).to(device)\n",
        "        quan = quan.view(-1, 28, input_size).to(device)\n",
        "        label = label.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        output = model(features=(seq, quan), device=device)\n",
        "        loss = criterion(output, label)\n",
        "\n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        train_loss += loss.item()\n",
        "        optimizer.step()\n",
        "    print('Epoch [{}/{}], train_loss: {:.4f}'.format(epoch + 1, num_epochs, train_loss / total_step))\n",
        "elapsed_time = time.time() - start_time\n",
        "print('elapsed_time: {:.3f}s'.format(elapsed_time))\n",
        "print('Finished Training')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A5NxknEJTzNi",
        "outputId": "b648fe1c-d6c6-4803-e872-cad85ce15ffb"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.125e-05\n",
            "Epoch [1/370], train_loss: 3.3239\n",
            "3.125e-05\n",
            "Epoch [2/370], train_loss: 3.2961\n",
            "6.25e-05\n",
            "Epoch [3/370], train_loss: 3.2530\n",
            "6.25e-05\n",
            "Epoch [4/370], train_loss: 3.1835\n",
            "0.000125\n",
            "Epoch [5/370], train_loss: 3.0139\n",
            "0.000125\n",
            "Epoch [6/370], train_loss: 2.4787\n",
            "0.00025\n",
            "Epoch [7/370], train_loss: 1.8865\n",
            "0.00025\n",
            "Epoch [8/370], train_loss: 1.7778\n",
            "0.0005\n",
            "Epoch [9/370], train_loss: 1.6388\n",
            "0.0005\n",
            "Epoch [10/370], train_loss: 1.3952\n",
            "0.001\n",
            "Epoch [11/370], train_loss: 1.0811\n",
            "0.001\n",
            "Epoch [12/370], train_loss: 0.8493\n",
            "0.001\n",
            "Epoch [13/370], train_loss: 0.7217\n",
            "0.001\n",
            "Epoch [14/370], train_loss: 0.6399\n",
            "0.001\n",
            "Epoch [15/370], train_loss: 0.5818\n",
            "0.001\n",
            "Epoch [16/370], train_loss: 0.5439\n",
            "0.001\n",
            "Epoch [17/370], train_loss: 0.5187\n",
            "0.001\n",
            "Epoch [18/370], train_loss: 0.4846\n",
            "0.001\n",
            "Epoch [19/370], train_loss: 0.4644\n",
            "0.001\n",
            "Epoch [20/370], train_loss: 0.4494\n",
            "0.001\n",
            "Epoch [21/370], train_loss: 0.4550\n",
            "0.001\n",
            "Epoch [22/370], train_loss: 0.4410\n",
            "0.001\n",
            "Epoch [23/370], train_loss: 0.4266\n",
            "0.001\n",
            "Epoch [24/370], train_loss: 0.4053\n",
            "0.001\n",
            "Epoch [25/370], train_loss: 0.3945\n",
            "0.001\n",
            "Epoch [26/370], train_loss: 0.3919\n",
            "0.001\n",
            "Epoch [27/370], train_loss: 0.3798\n",
            "0.001\n",
            "Epoch [28/370], train_loss: 0.3720\n",
            "0.001\n",
            "Epoch [29/370], train_loss: 0.3622\n",
            "0.001\n",
            "Epoch [30/370], train_loss: 0.3561\n",
            "0.001\n",
            "Epoch [31/370], train_loss: 0.3504\n",
            "0.001\n",
            "Epoch [32/370], train_loss: 0.3441\n",
            "0.001\n",
            "Epoch [33/370], train_loss: 0.3428\n",
            "0.001\n",
            "Epoch [34/370], train_loss: 0.3379\n",
            "0.001\n",
            "Epoch [35/370], train_loss: 0.3364\n",
            "0.001\n",
            "Epoch [36/370], train_loss: 0.3298\n",
            "0.001\n",
            "Epoch [37/370], train_loss: 0.3247\n",
            "0.001\n",
            "Epoch [38/370], train_loss: 0.3246\n",
            "0.001\n",
            "Epoch [39/370], train_loss: 0.3226\n",
            "0.001\n",
            "Epoch [40/370], train_loss: 0.3171\n",
            "0.001\n",
            "Epoch [41/370], train_loss: 0.3153\n",
            "0.001\n",
            "Epoch [42/370], train_loss: 0.3115\n",
            "0.001\n",
            "Epoch [43/370], train_loss: 0.3082\n",
            "0.001\n",
            "Epoch [44/370], train_loss: 0.3078\n",
            "0.001\n",
            "Epoch [45/370], train_loss: 0.3059\n",
            "0.001\n",
            "Epoch [46/370], train_loss: 0.3032\n",
            "0.001\n",
            "Epoch [47/370], train_loss: 0.2993\n",
            "0.001\n",
            "Epoch [48/370], train_loss: 0.2974\n",
            "0.001\n",
            "Epoch [49/370], train_loss: 0.3007\n",
            "0.001\n",
            "Epoch [50/370], train_loss: 0.3029\n",
            "0.001\n",
            "Epoch [51/370], train_loss: 0.2903\n",
            "0.001\n",
            "Epoch [52/370], train_loss: 0.2869\n",
            "0.001\n",
            "Epoch [53/370], train_loss: 0.2905\n",
            "0.001\n",
            "Epoch [54/370], train_loss: 0.3020\n",
            "0.001\n",
            "Epoch [55/370], train_loss: 0.2987\n",
            "0.001\n",
            "Epoch [56/370], train_loss: 0.2864\n",
            "0.001\n",
            "Epoch [57/370], train_loss: 0.2819\n",
            "0.001\n",
            "Epoch [58/370], train_loss: 0.2782\n",
            "0.001\n",
            "Epoch [59/370], train_loss: 0.2770\n",
            "0.001\n",
            "Epoch [60/370], train_loss: 0.2745\n",
            "0.001\n",
            "Epoch [61/370], train_loss: 0.2741\n",
            "0.001\n",
            "Epoch [62/370], train_loss: 0.2757\n",
            "0.001\n",
            "Epoch [63/370], train_loss: 0.2775\n",
            "0.001\n",
            "Epoch [64/370], train_loss: 0.2753\n",
            "0.001\n",
            "Epoch [65/370], train_loss: 0.2715\n",
            "0.001\n",
            "Epoch [66/370], train_loss: 0.2696\n",
            "0.001\n",
            "Epoch [67/370], train_loss: 0.2726\n",
            "0.001\n",
            "Epoch [68/370], train_loss: 0.2668\n",
            "0.001\n",
            "Epoch [69/370], train_loss: 0.2667\n",
            "0.001\n",
            "Epoch [70/370], train_loss: 0.2619\n",
            "0.001\n",
            "Epoch [71/370], train_loss: 0.2593\n",
            "0.001\n",
            "Epoch [72/370], train_loss: 0.2661\n",
            "0.001\n",
            "Epoch [73/370], train_loss: 0.3234\n",
            "0.001\n",
            "Epoch [74/370], train_loss: 0.2763\n",
            "0.001\n",
            "Epoch [75/370], train_loss: 0.2638\n",
            "0.001\n",
            "Epoch [76/370], train_loss: 0.2597\n",
            "0.001\n",
            "Epoch [77/370], train_loss: 0.2561\n",
            "0.001\n",
            "Epoch [78/370], train_loss: 0.2561\n",
            "0.001\n",
            "Epoch [79/370], train_loss: 0.2549\n",
            "0.001\n",
            "Epoch [80/370], train_loss: 0.2543\n",
            "0.001\n",
            "Epoch [81/370], train_loss: 0.2519\n",
            "0.001\n",
            "Epoch [82/370], train_loss: 0.2517\n",
            "0.001\n",
            "Epoch [83/370], train_loss: 0.2524\n",
            "0.001\n",
            "Epoch [84/370], train_loss: 0.2544\n",
            "0.001\n",
            "Epoch [85/370], train_loss: 0.2479\n",
            "0.001\n",
            "Epoch [86/370], train_loss: 0.2495\n",
            "0.001\n",
            "Epoch [87/370], train_loss: 0.2671\n",
            "0.001\n",
            "Epoch [88/370], train_loss: 0.2556\n",
            "0.001\n",
            "Epoch [89/370], train_loss: 0.2529\n",
            "0.001\n",
            "Epoch [90/370], train_loss: 0.2464\n",
            "0.001\n",
            "Epoch [91/370], train_loss: 0.2441\n",
            "0.001\n",
            "Epoch [92/370], train_loss: 0.2442\n",
            "0.001\n",
            "Epoch [93/370], train_loss: 0.2430\n",
            "0.001\n",
            "Epoch [94/370], train_loss: 0.2410\n",
            "0.001\n",
            "Epoch [95/370], train_loss: 0.2418\n",
            "0.001\n",
            "Epoch [96/370], train_loss: 0.2500\n",
            "0.001\n",
            "Epoch [97/370], train_loss: 0.2431\n",
            "0.001\n",
            "Epoch [98/370], train_loss: 0.2395\n",
            "0.001\n",
            "Epoch [99/370], train_loss: 0.2389\n",
            "0.001\n",
            "Epoch [100/370], train_loss: 0.2391\n",
            "0.001\n",
            "Epoch [101/370], train_loss: 0.2384\n",
            "0.001\n",
            "Epoch [102/370], train_loss: 0.2361\n",
            "0.001\n",
            "Epoch [103/370], train_loss: 0.2375\n",
            "0.001\n",
            "Epoch [104/370], train_loss: 0.2354\n",
            "0.001\n",
            "Epoch [105/370], train_loss: 0.2354\n",
            "0.001\n",
            "Epoch [106/370], train_loss: 0.2339\n",
            "0.001\n",
            "Epoch [107/370], train_loss: 0.2348\n",
            "0.001\n",
            "Epoch [108/370], train_loss: 0.2348\n",
            "0.001\n",
            "Epoch [109/370], train_loss: 0.2324\n",
            "0.001\n",
            "Epoch [110/370], train_loss: 0.2368\n",
            "0.001\n",
            "Epoch [111/370], train_loss: 0.2324\n",
            "0.001\n",
            "Epoch [112/370], train_loss: 0.2317\n",
            "0.001\n",
            "Epoch [113/370], train_loss: 0.2301\n",
            "0.001\n",
            "Epoch [114/370], train_loss: 0.2293\n",
            "0.001\n",
            "Epoch [115/370], train_loss: 0.2351\n",
            "0.001\n",
            "Epoch [116/370], train_loss: 0.2365\n",
            "0.001\n",
            "Epoch [117/370], train_loss: 0.2315\n",
            "0.001\n",
            "Epoch [118/370], train_loss: 0.2323\n",
            "0.001\n",
            "Epoch [119/370], train_loss: 0.2319\n",
            "0.001\n",
            "Epoch [120/370], train_loss: 0.2276\n",
            "0.001\n",
            "Epoch [121/370], train_loss: 0.2272\n",
            "0.001\n",
            "Epoch [122/370], train_loss: 0.2264\n",
            "0.001\n",
            "Epoch [123/370], train_loss: 0.2258\n",
            "0.001\n",
            "Epoch [124/370], train_loss: 0.2257\n",
            "0.001\n",
            "Epoch [125/370], train_loss: 0.2245\n",
            "0.001\n",
            "Epoch [126/370], train_loss: 0.2239\n",
            "0.001\n",
            "Epoch [127/370], train_loss: 0.2243\n",
            "0.001\n",
            "Epoch [128/370], train_loss: 0.2257\n",
            "0.001\n",
            "Epoch [129/370], train_loss: 0.2239\n",
            "0.001\n",
            "Epoch [130/370], train_loss: 0.2283\n",
            "0.001\n",
            "Epoch [131/370], train_loss: 0.2220\n",
            "0.001\n",
            "Epoch [132/370], train_loss: 0.2213\n",
            "0.001\n",
            "Epoch [133/370], train_loss: 0.2211\n",
            "0.001\n",
            "Epoch [134/370], train_loss: 0.2241\n",
            "0.001\n",
            "Epoch [135/370], train_loss: 0.2301\n",
            "0.001\n",
            "Epoch [136/370], train_loss: 0.2228\n",
            "0.001\n",
            "Epoch [137/370], train_loss: 0.2195\n",
            "0.001\n",
            "Epoch [138/370], train_loss: 0.2189\n",
            "0.001\n",
            "Epoch [139/370], train_loss: 0.2199\n",
            "0.001\n",
            "Epoch [140/370], train_loss: 0.2195\n",
            "0.001\n",
            "Epoch [141/370], train_loss: 0.2185\n",
            "0.001\n",
            "Epoch [142/370], train_loss: 0.2184\n",
            "0.001\n",
            "Epoch [143/370], train_loss: 0.2178\n",
            "0.001\n",
            "Epoch [144/370], train_loss: 0.2163\n",
            "0.001\n",
            "Epoch [145/370], train_loss: 0.2174\n",
            "0.001\n",
            "Epoch [146/370], train_loss: 0.2176\n",
            "0.001\n",
            "Epoch [147/370], train_loss: 0.2171\n",
            "0.001\n",
            "Epoch [148/370], train_loss: 0.2195\n",
            "0.001\n",
            "Epoch [149/370], train_loss: 0.2161\n",
            "0.001\n",
            "Epoch [150/370], train_loss: 0.2142\n",
            "0.001\n",
            "Epoch [151/370], train_loss: 0.2143\n",
            "0.001\n",
            "Epoch [152/370], train_loss: 0.2150\n",
            "0.001\n",
            "Epoch [153/370], train_loss: 0.2144\n",
            "0.001\n",
            "Epoch [154/370], train_loss: 0.2149\n",
            "0.001\n",
            "Epoch [155/370], train_loss: 0.2128\n",
            "0.001\n",
            "Epoch [156/370], train_loss: 0.2127\n",
            "0.001\n",
            "Epoch [157/370], train_loss: 0.2137\n",
            "0.001\n",
            "Epoch [158/370], train_loss: 0.2130\n",
            "0.001\n",
            "Epoch [159/370], train_loss: 0.2138\n",
            "0.001\n",
            "Epoch [160/370], train_loss: 0.2126\n",
            "0.001\n",
            "Epoch [161/370], train_loss: 0.2118\n",
            "0.001\n",
            "Epoch [162/370], train_loss: 0.2114\n",
            "0.001\n",
            "Epoch [163/370], train_loss: 0.2116\n",
            "0.001\n",
            "Epoch [164/370], train_loss: 0.2130\n",
            "0.001\n",
            "Epoch [165/370], train_loss: 0.2106\n",
            "0.001\n",
            "Epoch [166/370], train_loss: 0.2115\n",
            "0.001\n",
            "Epoch [167/370], train_loss: 0.2111\n",
            "0.001\n",
            "Epoch [168/370], train_loss: 0.2170\n",
            "0.001\n",
            "Epoch [169/370], train_loss: 0.2165\n",
            "0.001\n",
            "Epoch [170/370], train_loss: 0.2117\n",
            "0.001\n",
            "Epoch [171/370], train_loss: 0.2103\n",
            "0.001\n",
            "Epoch [172/370], train_loss: 0.2084\n",
            "0.001\n",
            "Epoch [173/370], train_loss: 0.2078\n",
            "0.001\n",
            "Epoch [174/370], train_loss: 0.2072\n",
            "0.001\n",
            "Epoch [175/370], train_loss: 0.2074\n",
            "0.001\n",
            "Epoch [176/370], train_loss: 0.2091\n",
            "0.001\n",
            "Epoch [177/370], train_loss: 0.2080\n",
            "0.001\n",
            "Epoch [178/370], train_loss: 0.2073\n",
            "0.001\n",
            "Epoch [179/370], train_loss: 0.2075\n",
            "0.001\n",
            "Epoch [180/370], train_loss: 0.2072\n",
            "0.001\n",
            "Epoch [181/370], train_loss: 0.2066\n",
            "0.001\n",
            "Epoch [182/370], train_loss: 0.2066\n",
            "0.001\n",
            "Epoch [183/370], train_loss: 0.2064\n",
            "0.001\n",
            "Epoch [184/370], train_loss: 0.2082\n",
            "0.001\n",
            "Epoch [185/370], train_loss: 0.2073\n",
            "0.001\n",
            "Epoch [186/370], train_loss: 0.2062\n",
            "0.001\n",
            "Epoch [187/370], train_loss: 0.2062\n",
            "0.001\n",
            "Epoch [188/370], train_loss: 0.2062\n",
            "0.001\n",
            "Epoch [189/370], train_loss: 0.2052\n",
            "0.001\n",
            "Epoch [190/370], train_loss: 0.2151\n",
            "0.001\n",
            "Epoch [191/370], train_loss: 0.2120\n",
            "0.001\n",
            "Epoch [192/370], train_loss: 0.2081\n",
            "0.001\n",
            "Epoch [193/370], train_loss: 0.2061\n",
            "0.001\n",
            "Epoch [194/370], train_loss: 0.2042\n",
            "0.001\n",
            "Epoch [195/370], train_loss: 0.2041\n",
            "0.001\n",
            "Epoch [196/370], train_loss: 0.2039\n",
            "0.001\n",
            "Epoch [197/370], train_loss: 0.2034\n",
            "0.001\n",
            "Epoch [198/370], train_loss: 0.2042\n",
            "0.001\n",
            "Epoch [199/370], train_loss: 0.2044\n",
            "0.001\n",
            "Epoch [200/370], train_loss: 0.2041\n",
            "0.001\n",
            "Epoch [201/370], train_loss: 0.2097\n",
            "0.001\n",
            "Epoch [202/370], train_loss: 0.2161\n",
            "0.001\n",
            "Epoch [203/370], train_loss: 0.2060\n",
            "0.001\n",
            "Epoch [204/370], train_loss: 0.2041\n",
            "0.001\n",
            "Epoch [205/370], train_loss: 0.2477\n",
            "0.001\n",
            "Epoch [206/370], train_loss: 0.2309\n",
            "0.001\n",
            "Epoch [207/370], train_loss: 0.2139\n",
            "0.001\n",
            "Epoch [208/370], train_loss: 0.2086\n",
            "0.001\n",
            "Epoch [209/370], train_loss: 0.2063\n",
            "0.001\n",
            "Epoch [210/370], train_loss: 0.2060\n",
            "0.001\n",
            "Epoch [211/370], train_loss: 0.2056\n",
            "0.001\n",
            "Epoch [212/370], train_loss: 0.2048\n",
            "0.001\n",
            "Epoch [213/370], train_loss: 0.2042\n",
            "0.001\n",
            "Epoch [214/370], train_loss: 0.2035\n",
            "0.001\n",
            "Epoch [215/370], train_loss: 0.2027\n",
            "0.001\n",
            "Epoch [216/370], train_loss: 0.2025\n",
            "0.001\n",
            "Epoch [217/370], train_loss: 0.2022\n",
            "0.001\n",
            "Epoch [218/370], train_loss: 0.2032\n",
            "0.001\n",
            "Epoch [219/370], train_loss: 0.2020\n",
            "0.001\n",
            "Epoch [220/370], train_loss: 0.2014\n",
            "0.001\n",
            "Epoch [221/370], train_loss: 0.2006\n",
            "0.001\n",
            "Epoch [222/370], train_loss: 0.2009\n",
            "0.001\n",
            "Epoch [223/370], train_loss: 0.2013\n",
            "0.001\n",
            "Epoch [224/370], train_loss: 0.2009\n",
            "0.001\n",
            "Epoch [225/370], train_loss: 0.2002\n",
            "0.001\n",
            "Epoch [226/370], train_loss: 0.2000\n",
            "0.001\n",
            "Epoch [227/370], train_loss: 0.1993\n",
            "0.001\n",
            "Epoch [228/370], train_loss: 0.1998\n",
            "0.001\n",
            "Epoch [229/370], train_loss: 0.1998\n",
            "0.001\n",
            "Epoch [230/370], train_loss: 0.1994\n",
            "0.001\n",
            "Epoch [231/370], train_loss: 0.1992\n",
            "0.001\n",
            "Epoch [232/370], train_loss: 0.1990\n",
            "0.001\n",
            "Epoch [233/370], train_loss: 0.1986\n",
            "0.001\n",
            "Epoch [234/370], train_loss: 0.1989\n",
            "0.001\n",
            "Epoch [235/370], train_loss: 0.1980\n",
            "0.001\n",
            "Epoch [236/370], train_loss: 0.1985\n",
            "0.001\n",
            "Epoch [237/370], train_loss: 0.1986\n",
            "0.001\n",
            "Epoch [238/370], train_loss: 0.1996\n",
            "0.001\n",
            "Epoch [239/370], train_loss: 0.1978\n",
            "0.001\n",
            "Epoch [240/370], train_loss: 0.1973\n",
            "0.001\n",
            "Epoch [241/370], train_loss: 0.1989\n",
            "0.001\n",
            "Epoch [242/370], train_loss: 0.1972\n",
            "0.001\n",
            "Epoch [243/370], train_loss: 0.3324\n",
            "0.001\n",
            "Epoch [244/370], train_loss: 0.2481\n",
            "0.001\n",
            "Epoch [245/370], train_loss: 0.2219\n",
            "0.001\n",
            "Epoch [246/370], train_loss: 0.2149\n",
            "0.001\n",
            "Epoch [247/370], train_loss: 0.2113\n",
            "0.001\n",
            "Epoch [248/370], train_loss: 0.2078\n",
            "0.001\n",
            "Epoch [249/370], train_loss: 0.2050\n",
            "0.001\n",
            "Epoch [250/370], train_loss: 0.2041\n",
            "0.001\n",
            "Epoch [251/370], train_loss: 0.2042\n",
            "0.001\n",
            "Epoch [252/370], train_loss: 0.2025\n",
            "0.001\n",
            "Epoch [253/370], train_loss: 0.2016\n",
            "0.001\n",
            "Epoch [254/370], train_loss: 0.1997\n",
            "0.001\n",
            "Epoch [255/370], train_loss: 0.1995\n",
            "0.001\n",
            "Epoch [256/370], train_loss: 0.1981\n",
            "0.001\n",
            "Epoch [257/370], train_loss: 0.1990\n",
            "0.001\n",
            "Epoch [258/370], train_loss: 0.1982\n",
            "0.001\n",
            "Epoch [259/370], train_loss: 0.1977\n",
            "0.001\n",
            "Epoch [260/370], train_loss: 0.1976\n",
            "0.001\n",
            "Epoch [261/370], train_loss: 0.1975\n",
            "0.001\n",
            "Epoch [262/370], train_loss: 0.1974\n",
            "0.001\n",
            "Epoch [263/370], train_loss: 0.1988\n",
            "0.001\n",
            "Epoch [264/370], train_loss: 0.1972\n",
            "0.001\n",
            "Epoch [265/370], train_loss: 0.1973\n",
            "0.001\n",
            "Epoch [266/370], train_loss: 0.1958\n",
            "0.001\n",
            "Epoch [267/370], train_loss: 0.1956\n",
            "0.001\n",
            "Epoch [268/370], train_loss: 0.1972\n",
            "0.001\n",
            "Epoch [269/370], train_loss: 0.1969\n",
            "0.001\n",
            "Epoch [270/370], train_loss: 0.1959\n",
            "0.001\n",
            "Epoch [271/370], train_loss: 0.1954\n",
            "0.001\n",
            "Epoch [272/370], train_loss: 0.1954\n",
            "0.001\n",
            "Epoch [273/370], train_loss: 0.1958\n",
            "0.001\n",
            "Epoch [274/370], train_loss: 0.1955\n",
            "0.001\n",
            "Epoch [275/370], train_loss: 0.1956\n",
            "0.001\n",
            "Epoch [276/370], train_loss: 0.1949\n",
            "0.001\n",
            "Epoch [277/370], train_loss: 0.1946\n",
            "0.001\n",
            "Epoch [278/370], train_loss: 0.1941\n",
            "0.001\n",
            "Epoch [279/370], train_loss: 0.1939\n",
            "0.001\n",
            "Epoch [280/370], train_loss: 0.1937\n",
            "0.001\n",
            "Epoch [281/370], train_loss: 0.1943\n",
            "0.001\n",
            "Epoch [282/370], train_loss: 0.1949\n",
            "0.001\n",
            "Epoch [283/370], train_loss: 0.1934\n",
            "0.001\n",
            "Epoch [284/370], train_loss: 0.1935\n",
            "0.001\n",
            "Epoch [285/370], train_loss: 0.1939\n",
            "0.001\n",
            "Epoch [286/370], train_loss: 0.1940\n",
            "0.001\n",
            "Epoch [287/370], train_loss: 0.1944\n",
            "0.001\n",
            "Epoch [288/370], train_loss: 0.1951\n",
            "0.001\n",
            "Epoch [289/370], train_loss: 0.1943\n",
            "0.001\n",
            "Epoch [290/370], train_loss: 0.1932\n",
            "0.001\n",
            "Epoch [291/370], train_loss: 0.1939\n",
            "0.001\n",
            "Epoch [292/370], train_loss: 0.1949\n",
            "0.001\n",
            "Epoch [293/370], train_loss: 0.1933\n",
            "0.001\n",
            "Epoch [294/370], train_loss: 0.1949\n",
            "0.001\n",
            "Epoch [295/370], train_loss: 0.1934\n",
            "0.001\n",
            "Epoch [296/370], train_loss: 0.1931\n",
            "0.001\n",
            "Epoch [297/370], train_loss: 0.1934\n",
            "0.001\n",
            "Epoch [298/370], train_loss: 0.1930\n",
            "0.001\n",
            "Epoch [299/370], train_loss: 0.1929\n",
            "0.001\n",
            "Epoch [300/370], train_loss: 0.1929\n",
            "0.0001\n",
            "Epoch [301/370], train_loss: 0.1903\n",
            "0.0001\n",
            "Epoch [302/370], train_loss: 0.1886\n",
            "0.0001\n",
            "Epoch [303/370], train_loss: 0.1886\n",
            "0.0001\n",
            "Epoch [304/370], train_loss: 0.1882\n",
            "0.0001\n",
            "Epoch [305/370], train_loss: 0.1885\n",
            "0.0001\n",
            "Epoch [306/370], train_loss: 0.1880\n",
            "0.0001\n",
            "Epoch [307/370], train_loss: 0.1881\n",
            "0.0001\n",
            "Epoch [308/370], train_loss: 0.1880\n",
            "0.0001\n",
            "Epoch [309/370], train_loss: 0.1879\n",
            "0.0001\n",
            "Epoch [310/370], train_loss: 0.1881\n",
            "0.0001\n",
            "Epoch [311/370], train_loss: 0.1884\n",
            "0.0001\n",
            "Epoch [312/370], train_loss: 0.1881\n",
            "0.0001\n",
            "Epoch [313/370], train_loss: 0.1880\n",
            "0.0001\n",
            "Epoch [314/370], train_loss: 0.1882\n",
            "0.0001\n",
            "Epoch [315/370], train_loss: 0.1879\n",
            "0.0001\n",
            "Epoch [316/370], train_loss: 0.1881\n",
            "0.0001\n",
            "Epoch [317/370], train_loss: 0.1880\n",
            "0.0001\n",
            "Epoch [318/370], train_loss: 0.1881\n",
            "0.0001\n",
            "Epoch [319/370], train_loss: 0.1877\n",
            "0.0001\n",
            "Epoch [320/370], train_loss: 0.1880\n",
            "0.0001\n",
            "Epoch [321/370], train_loss: 0.1878\n",
            "0.0001\n",
            "Epoch [322/370], train_loss: 0.1883\n",
            "0.0001\n",
            "Epoch [323/370], train_loss: 0.1883\n",
            "0.0001\n",
            "Epoch [324/370], train_loss: 0.1877\n",
            "0.0001\n",
            "Epoch [325/370], train_loss: 0.1874\n",
            "0.0001\n",
            "Epoch [326/370], train_loss: 0.1879\n",
            "0.0001\n",
            "Epoch [327/370], train_loss: 0.1883\n",
            "0.0001\n",
            "Epoch [328/370], train_loss: 0.1877\n",
            "0.0001\n",
            "Epoch [329/370], train_loss: 0.1876\n",
            "0.0001\n",
            "Epoch [330/370], train_loss: 0.1880\n",
            "0.0001\n",
            "Epoch [331/370], train_loss: 0.1876\n",
            "0.0001\n",
            "Epoch [332/370], train_loss: 0.1878\n",
            "0.0001\n",
            "Epoch [333/370], train_loss: 0.1874\n",
            "0.0001\n",
            "Epoch [334/370], train_loss: 0.1876\n",
            "0.0001\n",
            "Epoch [335/370], train_loss: 0.1874\n",
            "0.0001\n",
            "Epoch [336/370], train_loss: 0.1876\n",
            "0.0001\n",
            "Epoch [337/370], train_loss: 0.1875\n",
            "0.0001\n",
            "Epoch [338/370], train_loss: 0.1873\n",
            "0.0001\n",
            "Epoch [339/370], train_loss: 0.1879\n",
            "0.0001\n",
            "Epoch [340/370], train_loss: 0.1874\n",
            "0.0001\n",
            "Epoch [341/370], train_loss: 0.1877\n",
            "0.0001\n",
            "Epoch [342/370], train_loss: 0.1875\n",
            "0.0001\n",
            "Epoch [343/370], train_loss: 0.1874\n",
            "0.0001\n",
            "Epoch [344/370], train_loss: 0.1874\n",
            "0.0001\n",
            "Epoch [345/370], train_loss: 0.1874\n",
            "0.0001\n",
            "Epoch [346/370], train_loss: 0.1876\n",
            "0.0001\n",
            "Epoch [347/370], train_loss: 0.1873\n",
            "0.0001\n",
            "Epoch [348/370], train_loss: 0.1873\n",
            "0.0001\n",
            "Epoch [349/370], train_loss: 0.1872\n",
            "0.0001\n",
            "Epoch [350/370], train_loss: 0.1873\n",
            "1e-05\n",
            "Epoch [351/370], train_loss: 0.1868\n",
            "1e-05\n",
            "Epoch [352/370], train_loss: 0.1869\n",
            "1e-05\n",
            "Epoch [353/370], train_loss: 0.1867\n",
            "1e-05\n",
            "Epoch [354/370], train_loss: 0.1868\n",
            "1e-05\n",
            "Epoch [355/370], train_loss: 0.1867\n",
            "1e-05\n",
            "Epoch [356/370], train_loss: 0.1868\n",
            "1e-05\n",
            "Epoch [357/370], train_loss: 0.1867\n",
            "1e-05\n",
            "Epoch [358/370], train_loss: 0.1867\n",
            "1e-05\n",
            "Epoch [359/370], train_loss: 0.1870\n",
            "1e-05\n",
            "Epoch [360/370], train_loss: 0.1867\n",
            "1e-05\n",
            "Epoch [361/370], train_loss: 0.1867\n",
            "1e-05\n",
            "Epoch [362/370], train_loss: 0.1869\n",
            "1e-05\n",
            "Epoch [363/370], train_loss: 0.1871\n",
            "1e-05\n",
            "Epoch [364/370], train_loss: 0.1870\n",
            "1e-05\n",
            "Epoch [365/370], train_loss: 0.1869\n",
            "1e-05\n",
            "Epoch [366/370], train_loss: 0.1868\n",
            "1e-05\n",
            "Epoch [367/370], train_loss: 0.1869\n",
            "1e-05\n",
            "Epoch [368/370], train_loss: 0.1870\n",
            "1e-05\n",
            "Epoch [369/370], train_loss: 0.1867\n",
            "1e-05\n",
            "Epoch [370/370], train_loss: 0.1867\n",
            "elapsed_time: 378.216s\n",
            "Finished Training\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lsa9Wiv-OUfq",
        "outputId": "a4e33bd6-04b9-4f95-fd71-582259326bf1"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#save the model\n",
        "torch.save(model.state_dict(), '/content/drive/MyDrive/HDFS/loganomaly_model_parameter')"
      ],
      "metadata": {
        "id": "qVHEFEB4NqVS"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# upload the model\n",
        "model_path = '/content/drive/MyDrive/HDFS/loganomaly_model_parameter'\n",
        "model.load_state_dict(torch.load(model_path))"
      ],
      "metadata": {
        "id": "pA_FNHo7ORnT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c5457aa-7a6a-4a19-9fe8-c2de3550d071"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate(name):\n",
        "    window_size = 10\n",
        "    hdfs = {} #store the unique sequences and their counts.\n",
        "    length = 0\n",
        "    with open('/content/' + name, 'r') as f:\n",
        "        for ln in f.readlines():\n",
        "            ln = [(int(i)-1) for i in ln.strip().split()]\n",
        "            ln = ln + [-1] * (window_size + 1 - len(ln))     #ensure that all sequences have a fixed length of window_size + 1, even if the original line had fewer elements.\n",
        "            hdfs[tuple(ln)] = hdfs.get(tuple(ln), 0) + 1   #If the tuple is not present in the dictionary, hdfs.get(tuple(ln), 0) returns 0, and the code initializes the count to 1.\n",
        "            length += 1\n",
        "    print('Number of sessions({}): {}'.format(name, len(hdfs)))\n",
        "    return hdfs, length"
      ],
      "metadata": {
        "id": "ntfFr0QmwD6x"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "test_normal_loader, test_normal_length = generate('hdfs_test_normal')\n",
        "test_abnormal_loader, test_abnormal_length = generate('hdfs_test_abnormal')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6NnZp1dT75jI",
        "outputId": "3b94e1c1-abb1-43e8-c4a0-2f6fefae1105"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of sessions(hdfs_test_normal): 14177\n",
            "Number of sessions(hdfs_test_abnormal): 4123\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_candidates = 9 # on paper is g , top-g(here top 9) probabilities to appear next are considered normal"
      ],
      "metadata": {
        "id": "dSDO2VmH8dRF"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_data(model, data_loader, num_candidates, device):\n",
        "  model.eval()\n",
        "\n",
        "  S = 0\n",
        "\n",
        "  start_time = time.time()\n",
        "  with torch.no_grad():\n",
        "      for line in tqdm(data_loader.keys()):\n",
        "          for i in range(len(line) - window_size):\n",
        "              seq0 = line[i:i + window_size]\n",
        "              label = line[i + window_size]\n",
        "              seq1 = [0] * 28\n",
        "              log_conuter = Counter(seq0)\n",
        "              for key in log_conuter:\n",
        "                if key > 0:  # avoid considering -1 in templates\n",
        "                  seq1[key] = log_conuter[key]\n",
        "\n",
        "              seq0 = torch.tensor(seq0, dtype=torch.float).view(-1, window_size, input_size).to(device)\n",
        "              seq1 = torch.tensor(seq1, dtype=torch.float).view(-1, num_classes, input_size).to(device)\n",
        "              label = torch.tensor(label).view(-1).to(device)\n",
        "              output = model(features=[seq0, seq1], device=device)\n",
        "              predicted = torch.argsort(output,1)[0][-num_candidates:]\n",
        "              if label not in predicted:\n",
        "                  S += data_loader[line]\n",
        "                  break\n",
        "\n",
        "  return S"
      ],
      "metadata": {
        "id": "MenhoSebeWoE"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the model on normal data\n",
        "FP_normal = test_data(model, test_normal_loader, num_candidates, device)\n",
        "\n",
        "# Test the model on abnormal data\n",
        "TP_abnormal = test_data(model, test_abnormal_loader, num_candidates, device)\n",
        "\n",
        "# Compute precision, recall, and F1-measure\n",
        "FN = test_abnormal_length - TP_abnormal\n",
        "P = 100 * TP_abnormal / (TP_abnormal + FP_normal)\n",
        "R = 100 * TP_abnormal / (TP_abnormal + FN)\n",
        "F1 = 2 * P * R / (P + R)\n",
        "\n",
        "print('------------------------------------------------')\n",
        "print('false positive (FP): {}, false negative (FN): {}, Precision: {:.3f}%, Recall: {:.3f}%, F1-measure: {:.3f}%'.format(FP_normal, FN, P, R, F1))\n",
        "print('Finished Predicting')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oHIZ6nb4fLFc",
        "outputId": "6f07d553-a8f3-4199-de67-5bb9289fc9e2"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 14177/14177 [03:46<00:00, 62.62it/s]\n",
            "100%|██████████| 4123/4123 [00:50<00:00, 81.31it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------\n",
            "false positive (FP): 654, false negative (FN): 1259, Precision: 95.971%, Recall: 92.523%, F1-measure: 94.215%\n",
            "Finished Predicting\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ]
}